{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ClientAI","text":"<p> A unified client for AI providers with built-in agent support. </p> <p> </p> <p> ClientAI is a Python package that provides a unified framework for building AI applications, from direct provider interactions to transparent LLM-powered agents, with seamless support for OpenAI, Replicate, Groq and Ollama. </p>"},{"location":"#features","title":"Features","text":"<ul> <li>Unified Interface: Consistent methods across multiple AI providers (OpenAI, Replicate, Groq, Ollama).</li> <li>Streaming Support: Real-time response streaming and chat capabilities.</li> <li>Intelligent Agents: Framework for building transparent, multi-step LLM workflows with tool integration.</li> <li>Modular Design: Use components independently, from simple provider wrappers to complete agent systems.</li> <li>Type Safety: Comprehensive type hints for better development experience.</li> </ul>"},{"location":"#installing","title":"Installing","text":"<p>To install ClientAI with all providers, run:</p> <pre><code>pip install clientai[all]\n</code></pre> <p>Or, if you prefer to install only specific providers:</p> <pre><code>pip install clientai[openai]  # For OpenAI support\npip install clientai[replicate]  # For Replicate support\npip install clientai[ollama]  # For Ollama support\npip install clientai[groq]  # For Groq support\n</code></pre>"},{"location":"#design-philosophy","title":"Design Philosophy","text":"<p>The ClientAI Agent module is built on four core principles:</p> <ol> <li> <p>Prompt-Centric Design: Prompts are the key interface between you and the LLM. They should be explicit, debuggable, and easy to understand. No hidden or obscured prompts - what you see is what is sent to the model.</p> </li> <li> <p>Customization First: Every component is designed to be extended or overridden. Create custom steps, tool selectors, or entirely new workflow patterns. The architecture embraces your modifications.</p> </li> <li> <p>Zero Lock-In: Start with high-level components and drop down to lower levels as needed. You can:</p> <ul> <li>Extend <code>Agent</code> for custom behavior</li> <li>Use individual components directly</li> <li>Gradually replace parts with your own implementation</li> <li>Or gradually migrate away entirely - no lock-in</li> </ul> </li> </ol>"},{"location":"#examples","title":"Examples","text":""},{"location":"#1-basic-client-usage","title":"1. Basic Client Usage","text":"<pre><code>from clientai import ClientAI\n\n# Initialize with OpenAI\nclient = ClientAI('openai', api_key=\"your-openai-key\")\n\n# Generate text\nresponse = client.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n)\n\nprint(response)\n\n# Chat functionality\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"Paris.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\n\nresponse = client.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n)\n\nprint(response)\n</code></pre>"},{"location":"#2-quick-start-agent","title":"2. Quick-Start Agent","text":"<p>The fastest way to create a simple agent:</p> <pre><code>from clientai import client\nfrom clientai.agent import create_agent, tool\n\n# creating a tool with an explicit description\n@tool(name=\"add\", description=\"Add two numbers together\")\ndef add(x: int, y: int) -&gt; int:\n    return x + y\n\n# creating a tool that uses the docstring as description\n@tool(name=\"multiply\")\ndef multiply(x: int, y: int) -&gt; int:\n    \"\"\"Multiply two numbers and return their product.\"\"\"\n    return x * y\n\ncalculator = create_agent(\n    client=client(\"groq\", api_key=\"your-groq-key\"),\n    role=\"calculator\", \n    system_prompt=\"You are a helpful calculator assistant.\",\n    model=\"llama-3.2-3b-preview\",\n    tools=[add, multiply]\n)\n\nresult = calculator.run(\"What is 5 plus 3, then multiplied by 2?\")\nprint(result)\n</code></pre>"},{"location":"#3-custom-agent-with-workflow","title":"3. Custom Agent with Workflow","text":"<p>For more control, create a custom agent with defined steps:</p> <pre><code>from clientai import Agent, think, act, Tool\n\n# Create a standalone tool\n@tool(name=\"calculator\")\ndef calculate_average(numbers: list[float]) -&gt; float:\n    \"\"\"Calculate the arithmetic mean of a list of numbers.\"\"\"\n    return sum(numbers) / len(numbers)\n\nclass DataAnalyzer(Agent):\n    # add an analyze step\n    @think(\"analyze\")\n    def analyze_data(self, input_data: str) -&gt; str:\n        \"\"\"Analyze sales data by calculating key metrics.\"\"\"\n        return f\"\"\"\n            Please analyze these sales figures:\n\n            {input_data}\n\n            Calculate the average using the calculator tool\n            and identify the trend.\n            \"\"\"\n\n    # and also an act step\n    @act\n    def summarize(self, analysis: str) -&gt; str:\n        \"\"\"Create a brief summary of the analysis.\"\"\"\n        return \"\"\"\n            Create a brief summary that includes:\n            1. The average sales figure\n            2. Whether sales are trending up or down\n            3. One key recommendation\n            \"\"\"\n\n# Initialize with the tool\nanalyzer = DataAnalyzer(\n    client=client(\"replicate\", api_key=\"your-replicate-key\"),\n    default_model=\"meta/meta-llama-3-70b-instruct\",\n    tool_confidence=0.8,\n    tools=[calculate_average]\n)\n\nresult = analyzer.run(\"Monthly sales: [1000, 1200, 950, 1100]\")\nprint(result)\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<p>Before installing ClientAI, ensure you have the following prerequisites:</p> <ul> <li>Python: Version 3.9 or newer.</li> <li>Dependencies: The core ClientAI package has minimal dependencies. Provider-specific packages (e.g., <code>openai</code>, <code>replicate</code>, <code>ollama</code>, <code>groq</code>) are optional and can be installed separately.</li> </ul>"},{"location":"#usage","title":"Usage","text":"<p>ClientAI offers three main ways to interact with AI providers:</p> <ol> <li>Text Generation: Use the <code>generate_text</code> method for text generation tasks.</li> <li>Chat: Use the <code>chat</code> method for conversational interactions.</li> <li>Agents: Create intelligent agents with automated tool selection and workflow management.</li> </ol> <p>All methods support streaming responses and returning full response objects.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ol> <li>Check out the Usage Guide for detailed functionality and advanced features</li> <li>See the API Reference for complete API documentation</li> <li>For agent development, see the Agent Guide</li> <li>Explore our Examples for practical applications and real-world usage patterns</li> </ol> <p>Remember to handle API keys securely and never expose them in your code or version control systems.</p>"},{"location":"#license","title":"License","text":"<p><code>MIT</code></p>"},{"location":"showcase/","title":"Showcase","text":"<p>Categories</p> <p>Browse by type: Applications \u00b7 Tutorials</p>"},{"location":"showcase/#applications","title":"Applications","text":"<p>Be the First!</p> <p>No applications yet. Have you built something with ClientAI? We'd love to feature it here!</p> <p>Submit Your Project</p>"},{"location":"showcase/#tutorials","title":"Tutorials","text":"<p>Simple Q&amp;A Bot</p> <p>By ClientAI Team \u00b7 View Tutorial</p> <p>Learn the basics of ClientAI by building a Q&amp;A bot with chat functionality, context management, and real-time streaming responses.</p> <p><code>OpenAI</code> <code>Chat</code> <code>Beginner</code></p> <p>Multi-Provider Translator</p> <p>By ClientAI Team \u00b7 View Tutorial</p> <p>Build a translator that compares outputs from different AI providers, with performance metrics and parallel processing.</p> <p><code>OpenAI</code> <code>Groq</code> <code>Replicate</code> <code>Intermediate</code></p> <p>AI Dungeon Master</p> <p>By ClientAI Team \u00b7 View Tutorial</p> <p>Create an AI-powered text adventure game using multiple providers, with dynamic storytelling and game state management.</p> <p><code>OpenAI</code> <code>Replicate</code> <code>Ollama</code> <code>Advanced</code></p> <p>Simple Q&amp;A Bot (Agent)</p> <p>By ClientAI Team \u00b7 View Tutorial</p> <p>Build a Q&amp;A bot using ClientAI's agent framework, demonstrating core agent features and context management.</p> <p><code>OpenAI</code> <code>Agent</code> <code>Beginner</code></p> <p>Task Planner</p> <p>By ClientAI Team \u00b7 View Tutorial</p> <p>Develop a local task planning system that breaks down goals into actionable steps with realistic timelines.</p> <p><code>Ollama</code> <code>Planning</code> <code>Intermediate</code></p> <p>Writing Assistant</p> <p>By ClientAI Team \u00b7 View Tutorial</p> <p>Create a sophisticated writing assistant that analyzes text, suggests improvements, and rewrites content while maintaining context.</p> <p><code>Groq</code> <code>Writing</code> <code>Intermediate</code></p> <p>Code Analyzer</p> <p>By ClientAI Team \u00b7 View Tutorial</p> <p>Build a code analysis assistant that examines code structure, identifies potential issues, and suggests improvements.</p> <p><code>Ollama</code> <code>Development</code> <code>Advanced</code></p> <p>Add Your Project</p> <p>Built something with ClientAI? We'd love to showcase it!</p> <p>Submit Your Project</p>"},{"location":"advanced/error_handling/","title":"Error Handling and Retry Strategies","text":"<p>This guide covers best practices for handling errors and implementing retry strategies when working with ClientAI. Learn how to gracefully handle API errors, implement effective retry mechanisms, and build robust AI applications.</p>"},{"location":"advanced/error_handling/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Common Error Types</li> <li>Client Error Handling</li> <li>Agent Error Handling</li> <li>Retry Strategies</li> <li>Advanced Error Handling Patterns</li> </ol>"},{"location":"advanced/error_handling/#common-error-types","title":"Common Error Types","text":"<p>ClientAI provides a unified error hierarchy that normalizes errors across all providers. This means you can handle errors consistently regardless of which AI provider you're using. </p>"},{"location":"advanced/error_handling/#client-level-exceptions","title":"Client-Level Exceptions","text":"<pre><code>from clientai.exceptions import (\n    ClientAIError,          # Base exception for all client errors\n    AuthenticationError,    # API key or auth issues\n    APIError,              # General API errors\n    RateLimitError,        # Rate limits exceeded\n    InvalidRequestError,    # Malformed requests\n    ModelError,            # Model-related issues\n    ProviderNotInstalledError,  # Missing provider package\n    TimeoutError           # Request timeouts\n)\n</code></pre> <p>Each exception includes: - A descriptive message - Optional HTTP status code - Optional reference to the original error</p>"},{"location":"advanced/error_handling/#agent-level-exceptions","title":"Agent-Level Exceptions","text":"<pre><code>from clientai.agent.exceptions import (\n    AgentError,    # Base exception for all agent errors\n    StepError,     # Step execution/validation errors\n    ToolError      # Tool execution/validation errors\n)\n</code></pre>"},{"location":"advanced/error_handling/#client-error-handling","title":"Client Error Handling","text":"<p>At the client level, error handling focuses on direct interactions with AI providers. The most common pattern is to handle specific exceptions first, followed by more general ones:</p> <pre><code>from clientai import ClientAI\nfrom clientai.exceptions import (\n    ClientAIError, \n    RateLimitError,\n    AuthenticationError,\n    ModelError\n)\n\nclient = ClientAI(\"openai\", api_key=\"your-api-key\")\n\ntry:\n    response = client.generate_text(\n        prompt=\"Write a story\",\n        model=\"gpt-3.5-turbo\"\n    )\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e}\")\n    print(f\"Status code: {e.status_code}\")\nexcept RateLimitError as e:\n    print(f\"Rate limit exceeded: {e}\")\n    print(f\"Original error: {e.original_error}\")\nexcept ModelError as e:\n    print(f\"Model error: {e}\")\nexcept ClientAIError as e:\n    print(f\"Generation failed: {e}\")\n</code></pre>"},{"location":"advanced/error_handling/#agent-error-handling","title":"Agent Error Handling","text":"<p>Agent error handling builds on client error handling but adds considerations for workflow steps and tools. Here's how to handle agent-specific errors:</p> <pre><code>from clientai.agent import Agent, think\nfrom clientai.agent.exceptions import StepError, ToolError, AgentError\n\nclass ErrorAwareAgent(Agent):\n    @think(\"analyze\")\n    def analyze_data(self, data: str) -&gt; str:\n        try:\n            # Attempt to use tool\n            result = self.use_tool(\"analyzer\", data=data)\n            return f\"Analysis result: {result}\"\n        except ToolError:\n            # Fallback to direct prompt if tool fails\n            return f\"Please analyze this data: {data}\"\n\n# Using the agent\nagent = ErrorAwareAgent(client=client, default_model=\"gpt-4\")\n\ntry:\n    result = agent.run(\"Analyze this data\")\nexcept StepError as e:\n    print(f\"Step execution failed: {e}\")\nexcept ToolError as e:\n    print(f\"Tool execution failed: {e}\")\nexcept AgentError as e:\n    print(f\"Agent error: {e}\")\n</code></pre>"},{"location":"advanced/error_handling/#retry-strategies","title":"Retry Strategies","text":"<p>ClientAI supports several approaches to implementing retries:</p>"},{"location":"advanced/error_handling/#1-internal-agent-retries","title":"1. Internal Agent Retries","text":"<p>The agent system provides built-in retry capabilities through step configuration:</p> <pre><code>from clientai.agent.config import StepConfig\n\nclass RetryAgent(Agent):\n    @think(\n        \"analyze\",\n        step_config=StepConfig(\n            retry_count=3,\n            timeout=30.0\n            # use_internal_retry is True by default\n        )\n    )\n    def analyze_data(self, data: str) -&gt; str:\n        return f\"Please analyze this data: {data}\"\n</code></pre>"},{"location":"advanced/error_handling/#2-external-retry-libraries","title":"2. External Retry Libraries","text":"<p>For more complex retry patterns, you can use external retry libraries like Stamina. When using external retry mechanisms, disable internal retries:</p> <pre><code>import stamina\n\nclass StaminaAgent(Agent):\n    @think(\n        \"analyze\",\n        step_config=StepConfig(\n            use_internal_retry=False  # Disable internal retry\n        )\n    )\n    @stamina.retry(\n        on=(RateLimitError, TimeoutError),\n        attempts=3\n    )\n    def analyze_data(self, data: str) -&gt; str:\n        return f\"Please analyze this data: {data}\"\n</code></pre>"},{"location":"advanced/error_handling/#agent-error-handling-and-step-configuration","title":"Agent Error Handling and Step Configuration","text":"<p>The agent system provides fine-grained control over error handling through step configuration. Each step can be configured to handle failures differently, enabling robust workflows that gracefully handle errors.</p>"},{"location":"advanced/error_handling/#step-configuration-and-error-flow","title":"Step Configuration and Error Flow","text":"<p>Steps can be marked as required or optional, affecting how errors propagate through the workflow:</p> <pre><code>from clientai.agent import Agent, think, act\nfrom clientai.agent.config import StepConfig\nfrom clientai.exceptions import ModelError, ToolError\n\nclass AnalysisAgent(Agent):\n    # Required step - failure stops workflow\n    @think(\n        \"analyze\",\n        step_config=StepConfig(\n            required=True,         # Workflow fails if this step fails\n            retry_count=3,         # Retry up to 3 times\n            timeout=30.0          # 30 second timeout\n        )\n    )\n    def analyze_data(self, data: str) -&gt; str:\n        return f\"Please perform critical analysis of: {data}\"\n\n    # Optional step - workflow continues if it fails\n    @think(\n        \"enhance\",\n        step_config=StepConfig(\n            required=False,        # Workflow continues if this fails\n            retry_count=1,         # Try once more on failure\n            pass_result=False      # Don't update context on failure\n        )\n    )\n    def enhance_analysis(self, analysis: str) -&gt; str:\n        return f\"Please enhance this analysis: {analysis}\"\n\n    # Final required step\n    @act(\n        \"summarize\",\n        step_config=StepConfig(\n            required=True,\n            pass_result=True      # Pass result to next step\n        )\n    )\n    def summarize_results(self, enhanced: str) -&gt; str:\n        return f\"Please summarize these results: {enhanced}\"\n\n# Usage\nagent = AnalysisAgent(client=client, default_model=\"gpt-4\")\ntry:\n    result = agent.run(\"Sample data\")\nexcept StepError as e:\n    if \"analyze\" in str(e):\n        print(\"Critical analysis failed\")\n    elif \"summarize\" in str(e):\n        print(\"Summary generation failed\")\n    else:\n        print(f\"Step failed: {e}\")\n</code></pre>"},{"location":"advanced/error_handling/#result-passing-and-error-state","title":"Result Passing and Error State","text":"<p>The <code>pass_result</code> parameter controls how results flow between steps, especially important during error handling:</p> <pre><code>class DataProcessingAgent(Agent):\n    @think(\n        \"analyze\",\n        step_config=StepConfig(\n            required=True,\n            pass_result=True      # Success: result passed to next step\n        )\n    )\n    def analyze_data(self, data: str) -&gt; str:\n        return f\"Analyze: {data}\"\n\n    @think(\n        \"validate\",\n        step_config=StepConfig(\n            required=False,        # Optional validation\n            pass_result=False,     # Don't pass failed validation results\n            retry_count=2         # Retry validation twice\n        )\n    )\n    def validate_analysis(self, analysis: str) -&gt; str:\n        try:\n            result = self.use_tool(\"validator\", data=analysis)\n            return f\"Validation result: {result}\"\n        except ToolError:\n            # Let it fail but don't pass result\n            raise\n\n    @act(\n        \"process\",\n        step_config=StepConfig(\n            required=True,\n            pass_result=True\n        )\n    )\n    def process_results(self, validated_or_original: str) -&gt; str:\n        # Gets original analysis if validation failed\n        return f\"Process: {validated_or_original}\"\n</code></pre>"},{"location":"advanced/error_handling/#retry-configuration-patterns","title":"Retry Configuration Patterns","text":"<p>Here's how to combine retry settings with required and optional steps:</p> <pre><code>class ResilientAgent(Agent):\n    # Critical step with extensive retries\n    @think(\n        \"critical_analysis\",\n        step_config=StepConfig(\n            required=True,\n            retry_count=5,\n            timeout=45.0,\n            use_internal_retry=True\n        )\n    )\n    def analyze_critical(self, data: str) -&gt; str:\n        return f\"Critical analysis: {data}\"\n\n    # Optional enhancement with limited retries\n    @think(\n        \"enhance\",\n        step_config=StepConfig(\n            required=False,\n            retry_count=2,\n            timeout=15.0,\n            use_internal_retry=True\n        )\n    )\n    def enhance_results(self, analysis: str) -&gt; str:\n        return f\"Enhance: {analysis}\"\n\n    # Final step with external retry (e.g., Stamina)\n    @think(\n        \"summarize\",\n        step_config=StepConfig(\n            required=True,\n            use_internal_retry=False  # Using external retry\n        )\n    )\n    @stamina.retry(\n        on=ModelError,\n        attempts=3\n    )\n    def summarize(self, results: str) -&gt; str:\n        return f\"Summarize: {results}\"\n</code></pre>"},{"location":"advanced/error_handling/#graceful-degradation-with-optional-steps","title":"Graceful Degradation with Optional Steps","text":"<p>Here's a complete example of implementing graceful degradation using optional steps:</p> <pre><code>class DegradingAnalysisAgent(Agent):\n    @think(\n        \"detailed_analysis\",\n        step_config=StepConfig(\n            required=False,        # Optional - can fail\n            retry_count=2,\n            pass_result=False      # Don't pass failed results\n        )\n    )\n    def analyze_detailed(self, data: str) -&gt; str:\n        try:\n            # Try complex analysis first\n            result = self.use_tool(\"complex_analyzer\", data=data)\n            return f\"Detailed analysis: {result}\"\n        except ToolError:\n            raise\n\n    @think(\n        \"basic_analysis\",\n        step_config=StepConfig(\n            required=False,        # Optional fallback\n            retry_count=1,\n            pass_result=True      # Pass results if successful\n        )\n    )\n    def analyze_basic(self, data: str) -&gt; str:\n        try:\n            # Simpler analysis as fallback\n            result = self.use_tool(\"basic_analyzer\", data=data)\n            return f\"Basic analysis: {result}\"\n        except ToolError:\n            raise\n\n    @think(\n        \"minimal_analysis\",\n        step_config=StepConfig(\n            required=True,         # Must succeed\n            pass_result=True,\n            retry_count=3\n        )\n    )\n    def analyze_minimal(self, data: str) -&gt; str:\n        # Minimal analysis - just use LLM\n        return f\"Please provide a minimal analysis of: {data}\"\n\n# Usage showing graceful degradation\nagent = DegradingAnalysisAgent(client=client, default_model=\"gpt-4\")\n\ntry:\n    result = agent.run(\"Complex dataset\")\n    # Will try detailed -&gt; basic -&gt; minimal,\n    # using the best successful analysis\nexcept StepError as e:\n    if \"minimal_analysis\" in str(e):\n        print(\"Even minimal analysis failed\")\n    else:\n        print(f\"Unexpected failure: {e}\")\n</code></pre>"},{"location":"advanced/error_handling/#advanced-error-handling-patterns","title":"Advanced Error Handling Patterns","text":""},{"location":"advanced/error_handling/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<p>The circuit breaker pattern prevents system overload by temporarily stopping operations after a series of failures:</p> <pre><code>from typing import Optional\nfrom datetime import datetime, timedelta\n\nclass CircuitBreaker:\n    def __init__(\n        self,\n        failure_threshold: int = 5,\n        reset_timeout: int = 60\n    ):\n        self.failure_threshold = failure_threshold\n        self.reset_timeout = reset_timeout\n        self.failures = 0\n        self.last_failure_time: Optional[datetime] = None\n        self.is_open = False\n\n    def record_failure(self) -&gt; None:\n        self.failures += 1\n        self.last_failure_time = datetime.now()\n        if self.failures &gt;= self.failure_threshold:\n            self.is_open = True\n\n    def can_proceed(self) -&gt; bool:\n        if not self.is_open:\n            return True\n\n        if self.last_failure_time and \\\n           datetime.now() - self.last_failure_time &gt; timedelta(seconds=self.reset_timeout):\n            self.reset()\n            return True\n\n        return False\n\n    def reset(self) -&gt; None:\n        self.failures = 0\n        self.is_open = False\n        self.last_failure_time = None\n\n# Usage with agent\nclass CircuitBreakerAgent(Agent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.circuit_breaker = CircuitBreaker()\n\n    @think(\"analyze\")\n    def analyze_data(self, data: str) -&gt; str:\n        if not self.circuit_breaker.can_proceed():\n            return \"Service temporarily unavailable\"\n\n        try:\n            result = self.use_tool(\"analyzer\", data=data)\n            return f\"Analysis result: {result}\"\n        except ToolError as e:\n            self.circuit_breaker.record_failure()\n            raise\n</code></pre>"},{"location":"advanced/error_handling/#fallback-chain-pattern","title":"Fallback Chain Pattern","text":"<pre><code>from typing import Optional, List, Tuple\n\nclass FallbackChain:\n    def __init__(self, default_response: Optional[str] = None):\n        self.default_response = default_response\n        self.handlers: List[Tuple[ClientAI, str, Optional[CircuitBreaker]]] = []\n\n    def add_handler(\n        self,\n        client: ClientAI,\n        model: str,\n        circuit_breaker: Optional[CircuitBreaker] = None\n    ):\n        self.handlers.append((client, model, circuit_breaker))\n        return self\n\n    def execute(self, prompt: str) -&gt; str:\n        last_error = None\n\n        for client, model, circuit_breaker in self.handlers:\n            if circuit_breaker and not circuit_breaker.can_proceed():\n                continue\n\n            try:\n                return client.generate_text(prompt, model=model)\n            except ClientAIError as e:\n                if circuit_breaker:\n                    circuit_breaker.record_failure()\n                last_error = e\n\n        if self.default_response:\n            return self.default_response\n\n        raise last_error or ClientAIError(\"All handlers failed\")\n\n# Usage with agent\nclass FallbackAgent(Agent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.fallback_chain = FallbackChain(\n            default_response=\"Unable to process request\"\n        )\n        self.fallback_chain.add_handler(\n            ClientAI(\"openai\", api_key=OPENAI_API_KEY), \"gpt-4\", CircuitBreaker()\n        ).add_handler(\n            ClientAI(\"groq\", api_key=GROQ_API_KEY), \"llama-3.1-70b-versatile\", CircuitBreaker()\n        )\n\n    @think(\"analyze\")\n    def analyze_data(self, data: str) -&gt; str:\n        return self.fallback_chain.execute(\n            f\"Please analyze this data: {data}\"\n        )\n</code></pre>"},{"location":"advanced/error_handling/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Specific Exception Types <pre><code>try:\n    response = client.generate_text(prompt, model)\nexcept RateLimitError:\n    # Handle rate limits\nexcept ModelError:\n    # Handle model issues\nexcept ClientAIError:\n    # Handle other errors\n</code></pre></p> </li> <li> <p>Implement Graceful Degradation <pre><code>def generate_with_fallback(prompt: str) -&gt; str:\n    try:\n        return client.generate_text(\n            prompt, model=\"gpt-4\"\n        )\n    except (RateLimitError, ModelError):\n        return client.generate_text(\n            prompt, model=\"gpt-3.5-turbo\"\n        )\n    except ClientAIError:\n        return \"Service temporarily unavailable\"\n</code></pre></p> </li> <li> <p>Log Errors Appropriately <pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    response = client.generate_text(prompt, model)\nexcept ClientAIError as e:\n    logger.error(\n        \"Generation failed\",\n        extra={\n            \"status_code\": e.status_code,\n            \"error_type\": type(e).__name__,\n            \"original_error\": str(e.original_error)\n        }\n    )\n</code></pre></p> </li> <li> <p>Choose the Right Retry Strategy</p> <ul> <li>Use internal retries for simple cases</li> <li>Use external retry libraries for complex patterns</li> <li>Never mix internal and external retries</li> <li>Consider provider-specific characteristics</li> </ul> </li> </ol> <p>By following these error handling and retry strategies, you can build robust applications that handle failures gracefully and provide reliable service to your users.</p>"},{"location":"advanced/overview/","title":"Advanced Overview","text":"<p>This section provides in-depth guides on leveraging specific features of ClientAI and provider-specific functionalities. Each topic delves into a particular aspect of usage or focuses on a specific provider's unique capabilities.</p>"},{"location":"advanced/overview/#provider-specific-parameters","title":"Provider-Specific Parameters","text":"<p>Different AI providers offer unique parameters and features. Understanding these can help you fine-tune your AI interactions for optimal results.</p> <ol> <li> <p>Ollama Specific Guide: Learn about Ollama's unique parameters, including context handling, streaming options, and custom templates.</p> <ul> <li>Ollama Specific Guide</li> </ul> </li> <li> <p>OpenAI Specific Guide: Explore OpenAI's advanced features, such as logit bias and model-specific parameters.</p> <ul> <li>OpenAI Specific Guide</li> </ul> </li> <li> <p>Replicate Specific Guide: Discover Replicate's distinctive offerings, including model versioning and custom deployment options.</p> <ul> <li>Replicate Specific Guide</li> </ul> </li> <li> <p>Groq Specific Guide: Also check Groq's specific settings and parameters.</p> <ul> <li>Groq Specific Guide</li> </ul> </li> </ol>"},{"location":"advanced/overview/#custom-run-workflows","title":"Custom Run Workflows","text":"<ol> <li> <p>Creating Custom Run Methods: Learn how to implement custom run methods for complete workflow control.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> <li> <p>Managing State in Custom Runs: Master state management techniques in custom run implementations.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> <li> <p>Tool Usage in Custom Runs: Effectively leverage tools within custom run methods.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> <li> <p>Error Handling in Custom Runs: Implement robust error handling in custom workflows.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> </ol>"},{"location":"advanced/overview/#complex-workflows","title":"Complex Workflows","text":"<ol> <li> <p>Multi-Step Decision Making: Advanced patterns for complex decision workflows.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> <li> <p>Parallel Step Execution: Implementing concurrent step execution patterns.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> <li> <p>Conditional Workflows: Creating dynamic, condition-based workflows.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> </ol>"},{"location":"advanced/overview/#advanced-tool-patterns","title":"Advanced Tool Patterns","text":"<ol> <li> <p>Tool Chaining: Techniques for combining multiple tools effectively.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> <li> <p>Tool Result Caching: Optimizing tool execution with intelligent caching.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> <li> <p>Tool Fallback Strategies: Implementing robust tool execution patterns.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> </ol>"},{"location":"advanced/overview/#advanced-integration-topics","title":"Advanced Integration Topics","text":"<ol> <li> <p>External Service Integration: Patterns for integrating with external services.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> <li> <p>Database Integration: Implementing persistent storage in agent workflows.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> <li> <p>Event Systems: Building event-driven agent architectures.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> </ol>"},{"location":"advanced/overview/#testing-and-monitoring","title":"Testing and Monitoring","text":"<ol> <li> <p>Testing Strategies: Comprehensive testing approaches for agent systems.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> <li> <p>Performance Monitoring: Monitoring and optimizing agent performance.</p> <ul> <li><code>\ud83d\udea7 Coming Soon</code></li> </ul> </li> <li> <p>Error Handling and Retry Strategies: Best practices for error handling.</p> <ul> <li>Error Handling and Retry Strategies</li> </ul> </li> </ol> <p>Each guide in this section is designed to provide you with a deeper understanding of ClientAI's capabilities and how to leverage them effectively in your projects.</p>"},{"location":"advanced/agent/creating_run/","title":"Creating Custom Run Methods in ClientAI","text":"<p>Custom run methods provide granular control over your agent's workflow execution. While ClientAI's default sequential execution works well for most cases, custom run methods let you implement sophisticated logic, handle errors gracefully, and maintain complex state across your workflow.</p>"},{"location":"advanced/agent/creating_run/#understanding-custom-run-methods","title":"Understanding Custom Run Methods","text":"<p>A custom run method replaces the default workflow execution in ClientAI. This gives you direct control over:</p> <ul> <li>When and how steps execute</li> <li>How data flows between steps </li> <li>How results are processed</li> <li>How state is maintained</li> <li>How errors are handled</li> </ul> <p>Let's build a code review assistant that demonstrates these capabilities by implementing it step by step.</p>"},{"location":"advanced/agent/creating_run/#step-1-core-analysis-steps","title":"Step 1: Core Analysis Steps","text":"<p>First, let's define our main analysis steps:</p> <pre><code>from clientai.agent import Agent, run, think, act, synthesize\nfrom typing import Dict, Optional\nimport logging\nimport time\n\nclass CodeReviewAssistant(Agent):\n    @think\n    def analyze_structure(self, code: str) -&gt; str:\n        \"\"\"Analyze code structure and organization.\"\"\"\n        return f\"\"\"\n        Analyze this code's structure. Consider:\n        - Code organization and flow\n        - Function and variable naming\n        - Module structure\n\n        Code: {code}\n        \"\"\"\n\n    @think\n    def analyze_complexity(self, code: str) -&gt; str:\n        \"\"\"Assess code complexity and maintainability.\"\"\"\n        return f\"\"\"\n        Evaluate code complexity focusing on:\n        - Cyclomatic complexity\n        - Cognitive complexity\n        - Maintainability concerns\n        - Resource usage\n\n        Code: {code}\n        \"\"\"\n</code></pre> <p>These initial steps use the <code>@think</code> decorator because they involve analytical processing. Each step has a clear, focused purpose and provides specific guidance to the LLM.</p>"},{"location":"advanced/agent/creating_run/#step-2-security-and-improvement-steps","title":"Step 2: Security and Improvement Steps","text":"<p>Next, we'll add steps for security analysis and suggesting improvements:</p> <pre><code>    @act\n    def run_security_check(self, code: str) -&gt; str:\n        \"\"\"Scan code for potential security issues.\"\"\"\n        return f\"\"\"\n        Scan this code for security vulnerabilities, focusing on:\n        - Input validation\n        - Data sanitization\n        - Authentication checks\n        - Resource management\n\n        Code: {code}\n        \"\"\"\n\n    @act\n    def suggest_refactoring(self, analysis: Dict[str, str]) -&gt; str:\n        \"\"\"Suggest code refactoring improvements.\"\"\"\n        return f\"\"\"\n        Based on the provided analyses, suggest specific refactoring improvements:\n\n        Structural Analysis: {analysis['structure']}\n        Complexity Analysis: {analysis['complexity']}\n        Security Analysis: {analysis.get('security', 'No security analysis available')}\n\n        Provide practical, actionable improvements prioritized by impact.\n        \"\"\"\n</code></pre> <p>These steps use the <code>@act</code> decorator because they involve taking action based on analysis. Note how <code>suggest_refactoring</code> accepts a dictionary containing all previous analyses.</p>"},{"location":"advanced/agent/creating_run/#step-3-report-generation","title":"Step 3: Report Generation","text":"<p>The final step synthesizes all our findings into a report:</p> <pre><code>    @synthesize\n    def generate_report(self, data: Dict[str, str]) -&gt; str:\n        \"\"\"Create final code review report.\"\"\"\n        return f\"\"\"\n        Generate a comprehensive code review report with these sections:\n\n        1. Executive Summary\n        2. Structural Analysis\n        3. Complexity Assessment\n        4. Security Review\n        5. Recommended Improvements\n        6. Action Items\n\n        Using this data:\n        {data}\n\n        Format the report in a professional, easy-to-read style.\n        \"\"\"\n</code></pre> <p>The <code>@synthesize</code> decorator indicates this step combines and formats results from previous steps.</p>"},{"location":"advanced/agent/creating_run/#step-4-custom-run-implementation","title":"Step 4: Custom Run Implementation","text":"<p>Now let's implement our custom run method that orchestrates these steps:</p> <pre><code>    @run\n    def custom_run(self, input_data: str) -&gt; str:\n        \"\"\"Execute comprehensive code review workflow.\"\"\"\n        try:\n            logging.info(\"Starting code review workflow\")\n            self.context.state[\"start_time\"] = time.time()\n            self.context.state[\"original_code\"] = input_data\n\n            # Initialize results dictionary\n            results = {}\n\n            # Step 1: Structural Analysis\n            logging.info(\"Analyzing code structure\")\n            structure_result = self.analyze_structure(input_data)\n            if \"invalid syntax\" in structure_result.lower():\n                return \"Error: Code contains invalid syntax. Please fix syntax errors before review.\"\n            results[\"structure\"] = structure_result\n</code></pre> <p>This first part of our custom run method sets up logging, initializes state, and performs the initial structural analysis. Notice how we exit early if we detect syntax errors.</p> <p>Let's continue with security and complexity analysis:</p> <pre><code>            # Step 2: Security Analysis\n            logging.info(\"Performing security scan\")\n            try:\n                security_result = self.run_security_check(input_data)\n                results[\"security\"] = security_result\n\n                if \"critical vulnerability\" in security_result.lower():\n                    logging.warning(\"Critical security vulnerabilities detected\")\n                    self.context.state[\"has_security_issues\"] = True\n            except Exception as e:\n                logging.error(f\"Security analysis failed: {e}\")\n                results[\"security\"] = \"Security analysis failed - skipping\"\n\n            # Step 3: Complexity Analysis\n            logging.info(\"Analyzing code complexity\")\n            complexity_result = self.analyze_complexity(input_data)\n            results[\"complexity\"] = complexity_result\n</code></pre> <p>This section shows proper error handling and state tracking. Note how security analysis failures don't stop the entire workflow.</p> <p>Finally, let's handle improvements and report generation:</p> <pre><code>            # Determine if code requires extensive refactoring\n            needs_refactoring = (\n                \"high complexity\" in complexity_result.lower() or\n                \"maintainability issues\" in structure_result.lower() or\n                self.context.state.get(\"has_security_issues\", False)\n            )\n\n            # Generate improvements and final report\n            logging.info(\"Generating improvement suggestions\")\n            self.context.state[\"refactoring_priority\"] = \"high\" if needs_refactoring else \"low\"\n            improvements = self.suggest_refactoring(results)\n            results[\"improvements\"] = improvements\n\n            # Generate final report\n            logging.info(\"Generating final report\")\n            final_report = self.generate_report(results)\n\n            # Update completion time\n            self.context.state[\"end_time\"] = time.time()\n            self.context.state[\"processing_time\"] = (\n                self.context.state[\"end_time\"] - self.context.state[\"start_time\"]\n            )\n\n            return final_report\n\n        except Exception as e:\n            logging.error(f\"Workflow failed: {e}\")\n            return f\"Error: Code review workflow failed - {str(e)}\"\n</code></pre>"},{"location":"advanced/agent/creating_run/#using-the-code-review-assistant","title":"Using the Code Review Assistant","text":"<p>Here's how to use our custom code review assistant:</p> <pre><code>import logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\n\n# Initialize the agent\nagent = CodeReviewAssistant(\n    client=client,\n    default_model=\"gpt-4\"\n)\n\n# Sample code to review\ncode = \"\"\"\ndef calculate_total(items):\n    total = 0\n    for item in items:\n        total += item.price * item.quantity\n    return total\n\"\"\"\n\n# Run the review\nresult = agent.run(code)\nprint(result)\n\n# Access workflow metrics\nprint(f\"Processing time: {agent.context.state['processing_time']:.2f} seconds\")\nprint(f\"Refactoring priority: {agent.context.state['refactoring_priority']}\")\n</code></pre>"},{"location":"advanced/agent/creating_run/#key-features-demonstrated","title":"Key Features Demonstrated","text":"<p>This implementation shows several important aspects of creating effective custom run methods:</p> <p>Error Handling: Each major step includes error handling that can gracefully recover from failures without stopping the entire workflow.</p> <p>State Management: The context system maintains useful state like processing time, security status, and refactoring priorities.</p> <p>Conditional Processing: The workflow makes intelligent decisions about processing based on results, such as exiting early for syntax errors or adjusting refactoring detail based on code quality.</p> <p>Logging: Comprehensive logging provides visibility into the workflow's progress and helps with debugging issues.</p> <p>Custom run methods let you create sophisticated workflows that adapt to different situations while maintaining clean, maintainable code structure. They're particularly valuable for complex tasks that need dynamic behavior or sophisticated state management.</p>"},{"location":"advanced/client/groq_specific/","title":"Groq-Specific Parameters in ClientAI","text":"<p>This guide covers the Groq-specific parameters that can be passed to ClientAI's <code>generate_text</code> and <code>chat</code> methods. These parameters are passed as additional keyword arguments to customize Groq's behavior.</p>"},{"location":"advanced/client/groq_specific/#generate_text-method","title":"generate_text Method","text":""},{"location":"advanced/client/groq_specific/#basic-structure","title":"Basic Structure","text":"<pre><code>from clientai import ClientAI\n\nclient = ClientAI('groq', api_key=\"your-groq-api-key\")\nresponse = client.generate_text(\n    prompt=\"Your prompt here\",          # Required\n    model=\"llama3-8b-8192\",            # Required\n    frequency_penalty=0.5,              # Groq-specific\n    presence_penalty=0.2,               # Groq-specific\n    max_tokens=100,                     # Groq-specific\n    response_format={\"type\": \"json\"},   # Groq-specific\n    seed=12345,                        # Groq-specific\n    temperature=0.7,                    # Groq-specific\n    top_p=0.9,                         # Groq-specific\n    n=1,                               # Groq-specific\n    stop=[\"END\"],                      # Groq-specific\n    stream=False,                      # Groq-specific\n    stream_options=None,               # Groq-specific\n    functions=None,                    # Groq-specific (Deprecated)\n    function_call=None,                # Groq-specific (Deprecated)\n    tools=None,                        # Groq-specific\n    tool_choice=None,                  # Groq-specific\n    parallel_tool_calls=True,          # Groq-specific\n    user=\"user_123\"                    # Groq-specific\n)\n</code></pre>"},{"location":"advanced/client/groq_specific/#groq-specific-parameters","title":"Groq-Specific Parameters","text":""},{"location":"advanced/client/groq_specific/#frequency_penalty-optionalfloat","title":"<code>frequency_penalty: Optional[float]</code>","text":"<ul> <li>Range: -2.0 to 2.0</li> <li>Default: 0</li> <li>Penalizes tokens based on their frequency in the text <pre><code>response = client.generate_text(\n    prompt=\"Write a creative story\",\n    model=\"llama3-8b-8192\",\n    frequency_penalty=0.7  # Reduces repetition\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#presence_penalty-optionalfloat","title":"<code>presence_penalty: Optional[float]</code>","text":"<ul> <li>Range: -2.0 to 2.0</li> <li>Default: 0</li> <li>Penalizes tokens based on their presence in prior text <pre><code>response = client.generate_text(\n    prompt=\"Write a varied story\",\n    model=\"llama3-8b-8192\",\n    presence_penalty=0.6  # Encourages topic diversity\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#max_tokens-optionalint","title":"<code>max_tokens: Optional[int]</code>","text":"<ul> <li>Maximum tokens for completion</li> <li>Limited by model's context length <pre><code>response = client.generate_text(\n    prompt=\"Write a summary\",\n    model=\"llama3-8b-8192\",\n    max_tokens=100\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#response_format-optionaldict","title":"<code>response_format: Optional[Dict]</code>","text":"<ul> <li>Controls output structure</li> <li>Requires explicit JSON instruction in prompt <pre><code>response = client.generate_text(\n    prompt=\"List three colors in JSON\",\n    model=\"llama3-8b-8192\",\n    response_format={\"type\": \"json_object\"}\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#seed-optionalint","title":"<code>seed: Optional[int]</code>","text":"<ul> <li>For deterministic generation <pre><code>response = client.generate_text(\n    prompt=\"Generate a random number\",\n    model=\"llama3-8b-8192\",\n    seed=12345\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#temperature-optionalfloat","title":"<code>temperature: Optional[float]</code>","text":"<ul> <li>Range: 0 to 2</li> <li>Default: 1</li> <li>Controls randomness in output <pre><code>response = client.generate_text(\n    prompt=\"Write creatively\",\n    model=\"llama3-8b-8192\",\n    temperature=0.7  # More creative output\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#top_p-optionalfloat","title":"<code>top_p: Optional[float]</code>","text":"<ul> <li>Range: 0 to 1</li> <li>Default: 1</li> <li>Alternative to temperature, called nucleus sampling <pre><code>response = client.generate_text(\n    prompt=\"Generate text\",\n    model=\"llama3-8b-8192\",\n    top_p=0.1  # Only consider top 10% probability tokens\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#n-optionalint","title":"<code>n: Optional[int]</code>","text":"<ul> <li>Default: 1</li> <li>Number of completions to generate</li> <li>Note: Currently only n=1 is supported <pre><code>response = client.generate_text(\n    prompt=\"Generate a story\",\n    model=\"llama3-8b-8192\",\n    n=1\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#stop-optionalunionstr-liststr","title":"<code>stop: Optional[Union[str, List[str]]]</code>","text":"<ul> <li>Up to 4 sequences where generation stops <pre><code>response = client.generate_text(\n    prompt=\"Write until you see END\",\n    model=\"llama3-8b-8192\",\n    stop=[\"END\", \"STOP\"]  # Stops at either sequence\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#stream-optionalbool","title":"<code>stream: Optional[bool]</code>","text":"<ul> <li>Default: False</li> <li>Enable token streaming <pre><code>for chunk in client.generate_text(\n    prompt=\"Tell a story\",\n    model=\"llama3-8b-8192\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#stream_options-optionaldict","title":"<code>stream_options: Optional[Dict]</code>","text":"<ul> <li>Options for streaming responses</li> <li>Only used when stream=True <pre><code>response = client.generate_text(\n    prompt=\"Long story\",\n    model=\"llama3-8b-8192\",\n    stream=True,\n    stream_options={\"chunk_size\": 1024}\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#user-optionalstr","title":"<code>user: Optional[str]</code>","text":"<ul> <li>Unique identifier for end-user tracking <pre><code>response = client.generate_text(\n    prompt=\"Hello\",\n    model=\"llama3-8b-8192\",\n    user=\"user_123\"\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#chat-method","title":"chat Method","text":""},{"location":"advanced/client/groq_specific/#basic-structure_1","title":"Basic Structure","text":"<pre><code>response = client.chat(\n    model=\"llama3-8b-8192\",            # Required\n    messages=[...],                    # Required\n    tools=[...],                      # Groq-specific\n    tool_choice=\"auto\",               # Groq-specific\n    parallel_tool_calls=True,         # Groq-specific\n    response_format={\"type\": \"json\"}, # Groq-specific\n    temperature=0.7,                  # Groq-specific\n    frequency_penalty=0.5,            # Groq-specific\n    presence_penalty=0.2,             # Groq-specific\n    max_tokens=100,                   # Groq-specific\n    seed=12345,                      # Groq-specific\n    stop=[\"END\"],                    # Groq-specific\n    stream=False,                    # Groq-specific\n    stream_options=None,             # Groq-specific\n    top_p=0.9,                       # Groq-specific\n    n=1,                             # Groq-specific\n    user=\"user_123\"                  # Groq-specific\n)\n</code></pre>"},{"location":"advanced/client/groq_specific/#groq-specific-parameters_1","title":"Groq-Specific Parameters","text":""},{"location":"advanced/client/groq_specific/#tools-optionallistdict","title":"<code>tools: Optional[List[Dict]]</code>","text":"<ul> <li>List of available tools (max 128) <pre><code>response = client.chat(\n    model=\"llama3-70b-8192\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather data\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                }\n            }\n        }\n    }]\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#tool_choice-optionalunionstr-dict","title":"<code>tool_choice: Optional[Union[str, Dict]]</code>","text":"<ul> <li>Controls tool selection behavior</li> <li>Values: \"none\", \"auto\", \"required\" <pre><code>response = client.chat(\n    model=\"llama3-70b-8192\",\n    messages=[{\"role\": \"user\", \"content\": \"Calculate something\"}],\n    tool_choice=\"auto\"  # or \"none\" or \"required\"\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#parallel_tool_calls-optionalbool","title":"<code>parallel_tool_calls: Optional[bool]</code>","text":"<ul> <li>Default: True</li> <li>Enable parallel function calling <pre><code>response = client.chat(\n    model=\"llama3-70b-8192\",\n    messages=[{\"role\": \"user\", \"content\": \"Multiple tasks\"}],\n    parallel_tool_calls=True\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/groq_specific/#complete-examples","title":"Complete Examples","text":""},{"location":"advanced/client/groq_specific/#example-1-structured-output-with-tools","title":"Example 1: Structured Output with Tools","text":"<pre><code>response = client.chat(\n    model=\"llama3-70b-8192\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a data assistant\"},\n        {\"role\": \"user\", \"content\": \"Get weather for Paris\"}\n    ],\n    response_format={\"type\": \"json_object\"},\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather data\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                }\n            }\n        }\n    }],\n    tool_choice=\"auto\",\n    temperature=0.7,\n    max_tokens=200,\n    seed=42\n)\n</code></pre>"},{"location":"advanced/client/groq_specific/#example-2-advanced-text-generation","title":"Example 2: Advanced Text Generation","text":"<pre><code>response = client.generate_text(\n    prompt=\"Write a technical analysis\",\n    model=\"mixtral-8x7b-32768\",\n    max_tokens=500,\n    frequency_penalty=0.7,\n    presence_penalty=0.6,\n    temperature=0.4,\n    top_p=0.9,\n    stop=[\"END\", \"CONCLUSION\"],\n    user=\"analyst_1\",\n    seed=42\n)\n</code></pre>"},{"location":"advanced/client/groq_specific/#example-3-streaming-generation","title":"Example 3: Streaming Generation","text":"<pre><code>for chunk in client.chat(\n    model=\"llama3-8b-8192\",\n    messages=[{\"role\": \"user\", \"content\": \"Explain quantum physics\"}],\n    stream=True,\n    temperature=0.7,\n    max_tokens=1000,\n    stream_options={\"chunk_size\": 1024}\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"advanced/client/groq_specific/#parameter-validation-notes","title":"Parameter Validation Notes","text":"<ol> <li>Both <code>model</code> and <code>prompt</code>/<code>messages</code> are required</li> <li>Model must be one of: \"gemma-7b-it\", \"llama3-70b-8192\", \"llama3-8b-8192\", \"mixtral-8x7b-32768\"</li> <li><code>n</code> parameter only supports value of 1</li> <li><code>stop</code> sequences limited to 4 maximum</li> <li>Tool usage limited to 128 functions</li> <li><code>response_format</code> requires explicit JSON instruction in prompt</li> <li>Parameters like <code>logprobs</code>, <code>logit_bias</code>, and <code>top_logprobs</code> are not yet supported</li> <li>Deterministic generation with <code>seed</code> is best-effort</li> <li><code>functions</code> and <code>function_call</code> are deprecated in favor of <code>tools</code> and <code>tool_choice</code></li> </ol> <p>These parameters allow you to fully customize Groq's behavior while working with ClientAI's abstraction layer.</p>"},{"location":"advanced/client/ollama_specific/","title":"Ollama-Specific Parameters in ClientAI","text":"<p>This guide covers the Ollama-specific parameters that can be passed to ClientAI's <code>generate_text</code> and <code>chat</code> methods. These parameters are passed as additional keyword arguments to customize Ollama's behavior.</p>"},{"location":"advanced/client/ollama_specific/#generate_text-method","title":"generate_text Method","text":""},{"location":"advanced/client/ollama_specific/#basic-structure","title":"Basic Structure","text":"<pre><code>from clientai import ClientAI\n\nclient = ClientAI('ollama')\nresponse = client.generate_text(\n    prompt=\"Your prompt here\",    # Required\n    model=\"llama2\",              # Required\n    suffix=\"Optional suffix\",     # Ollama-specific\n    system=\"System message\",      # Ollama-specific\n    template=\"Custom template\",   # Ollama-specific\n    context=[1, 2, 3],           # Ollama-specific\n    format=\"json\",               # Ollama-specific\n    options={\"temperature\": 0.7}, # Ollama-specific\n    keep_alive=\"5m\"              # Ollama-specific\n)\n</code></pre>"},{"location":"advanced/client/ollama_specific/#ollama-specific-parameters","title":"Ollama-Specific Parameters","text":""},{"location":"advanced/client/ollama_specific/#suffix-str","title":"<code>suffix: str</code>","text":"<ul> <li>Text to append to the generated output <pre><code>response = client.generate_text(\n    prompt=\"Write a story about a robot\",\n    model=\"llama2\",\n    suffix=\"\\n\\nThe End.\"\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/ollama_specific/#system-str","title":"<code>system: str</code>","text":"<ul> <li>System message to guide the model's behavior <pre><code>response = client.generate_text(\n    prompt=\"Explain quantum computing\",\n    model=\"llama2\",\n    system=\"You are a quantum physics professor explaining concepts to beginners\"\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/ollama_specific/#template-str","title":"<code>template: str</code>","text":"<ul> <li>Custom prompt template <pre><code>response = client.generate_text(\n    prompt=\"What is Python?\",\n    model=\"llama2\",\n    template=\"Question: {{.Prompt}}\\n\\nDetailed answer:\"\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/ollama_specific/#context-listint","title":"<code>context: List[int]</code>","text":"<ul> <li>Context from previous interactions <pre><code># First request\nfirst_response = client.generate_text(\n    prompt=\"Tell me a story about space\",\n    model=\"llama2\"\n)\n\n# Continue the story using context\ncontinued_response = client.generate_text(\n    prompt=\"What happened next?\",\n    model=\"llama2\",\n    context=first_response.context  # Context from previous response\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/ollama_specific/#format-literal-json","title":"<code>format: Literal['', 'json']</code>","text":"<ul> <li>Controls response format <pre><code>response = client.generate_text(\n    prompt=\"List three fruits with their colors\",\n    model=\"llama2\",\n    format=\"json\"\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/ollama_specific/#options-optionaloptions","title":"<code>options: Optional[Options]</code>","text":"<ul> <li>Model-specific parameters <pre><code>response = client.generate_text(\n    prompt=\"Write a creative story\",\n    model=\"llama2\",\n    options={\n        \"temperature\": 0.9,\n        \"top_p\": 0.8,\n        \"top_k\": 40\n    }\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/ollama_specific/#keep_alive-optionalunionfloat-str","title":"<code>keep_alive: Optional[Union[float, str]]</code>","text":"<ul> <li>Model memory retention duration <pre><code>response = client.generate_text(\n    prompt=\"Quick calculation\",\n    model=\"llama2\",\n    keep_alive=\"10m\"  # Keep model loaded for 10 minutes\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/ollama_specific/#chat-method","title":"chat Method","text":""},{"location":"advanced/client/ollama_specific/#basic-structure_1","title":"Basic Structure","text":"<pre><code>response = client.chat(\n    model=\"llama2\",              # Required\n    messages=[...],              # Required\n    tools=[...],                 # Ollama-specific\n    format=\"json\",               # Ollama-specific\n    options={\"temperature\": 0.7}, # Ollama-specific\n    keep_alive=\"5m\"              # Ollama-specific\n)\n</code></pre>"},{"location":"advanced/client/ollama_specific/#ollama-specific-parameters_1","title":"Ollama-Specific Parameters","text":""},{"location":"advanced/client/ollama_specific/#tools-optionallistdict","title":"<code>tools: Optional[List[Dict]]</code>","text":"<ul> <li>Tools available for the model (requires stream=False) <pre><code>response = client.chat(\n    model=\"llama2\",\n    messages=[{\"role\": \"user\", \"content\": \"What's 2+2?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"calculate\",\n            \"description\": \"Perform basic math\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"expression\": {\"type\": \"string\"}\n                }\n            }\n        }\n    }],\n    stream=False\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/ollama_specific/#format-literal-json_1","title":"<code>format: Literal['', 'json']</code>","text":"<ul> <li>Controls response format <pre><code>response = client.chat(\n    model=\"llama2\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"List three countries with their capitals\"}\n    ],\n    format=\"json\"\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/ollama_specific/#options-optionaloptions_1","title":"<code>options: Optional[Options]</code>","text":"<ul> <li>Model-specific parameters <pre><code>response = client.chat(\n    model=\"llama2\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke\"}],\n    options={\n        \"temperature\": 0.8,\n        \"top_p\": 0.9,\n        \"presence_penalty\": 0.5\n    }\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/ollama_specific/#keep_alive-optionalunionfloat-str_1","title":"<code>keep_alive: Optional[Union[float, str]]</code>","text":"<ul> <li>Model memory retention duration <pre><code>response = client.chat(\n    model=\"llama2\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    keep_alive=300.0  # 5 minutes in seconds\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/ollama_specific/#complete-examples","title":"Complete Examples","text":""},{"location":"advanced/client/ollama_specific/#example-1-creative-writing-with-generate_text","title":"Example 1: Creative Writing with generate_text","text":"<pre><code>response = client.generate_text(\n    prompt=\"Write a short story about AI\",\n    model=\"llama2\",\n    system=\"You are a creative writer specializing in science fiction\",\n    template=\"Story prompt: {{.Prompt}}\\n\\nCreative story:\",\n    options={\n        \"temperature\": 0.9,\n        \"top_p\": 0.95\n    },\n    suffix=\"\\n\\nThe End.\",\n    keep_alive=\"10m\"\n)\n</code></pre>"},{"location":"advanced/client/ollama_specific/#example-2-json-response-with-chat","title":"Example 2: JSON Response with chat","text":"<pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides structured data\"},\n    {\"role\": \"user\", \"content\": \"List 3 programming languages with their key features\"}\n]\n\nresponse = client.chat(\n    model=\"llama2\",\n    messages=messages,\n    format=\"json\",\n    options={\n        \"temperature\": 0.3,  # Lower temperature for more structured output\n        \"top_p\": 0.9\n    }\n)\n</code></pre>"},{"location":"advanced/client/ollama_specific/#example-3-multimodal-chat-with-image","title":"Example 3: Multimodal Chat with Image","text":"<pre><code>messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What's in this image?\",\n        \"images\": [\"encoded_image_data_or_path\"]\n    }\n]\n\nresponse = client.chat(\n    model=\"llava\",\n    messages=messages,\n    format=\"json\",\n    keep_alive=\"5m\"\n)\n</code></pre>"},{"location":"advanced/client/ollama_specific/#example-4-contextual-generation","title":"Example 4: Contextual Generation","text":"<pre><code># First generation\nfirst_response = client.generate_text(\n    prompt=\"Write the beginning of a mystery story\",\n    model=\"llama2\",\n    options={\"temperature\": 0.8}\n)\n\n# Continue the story using context\ncontinued_response = client.generate_text(\n    prompt=\"Continue the story with a plot twist\",\n    model=\"llama2\",\n    context=first_response.context,\n    options={\"temperature\": 0.8}\n)\n</code></pre>"},{"location":"advanced/client/ollama_specific/#parameter-validation-notes","title":"Parameter Validation Notes","text":"<ol> <li>Both <code>model</code> and <code>prompt</code>/<code>messages</code> are required</li> <li>When using <code>tools</code>, <code>stream</code> must be <code>False</code></li> <li><code>format</code> only accepts <code>''</code> or <code>'json'</code></li> <li>Image support requires multimodal models (e.g., llava)</li> <li>Context preservation works only with <code>generate_text</code></li> <li>Keep alive duration can be string (e.g., \"5m\") or float (seconds)</li> </ol> <p>These parameters allow you to fully customize Ollama's behavior while working with ClientAI's abstraction layer.</p>"},{"location":"advanced/client/openai_specific/","title":"OpenAI-Specific Parameters in ClientAI","text":"<p>This guide covers the OpenAI-specific parameters that can be passed to ClientAI's <code>generate_text</code> and <code>chat</code> methods. These parameters are passed as additional keyword arguments to customize OpenAI's behavior.</p>"},{"location":"advanced/client/openai_specific/#generate_text-method","title":"generate_text Method","text":""},{"location":"advanced/client/openai_specific/#basic-structure","title":"Basic Structure","text":"<pre><code>from clientai import ClientAI\n\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\nresponse = client.generate_text(\n    prompt=\"Your prompt here\",          # Required\n    model=\"gpt-3.5-turbo\",             # Required\n    frequency_penalty=0.5,              # OpenAI-specific\n    presence_penalty=0.2,               # OpenAI-specific\n    logit_bias={123: 100},             # OpenAI-specific\n    max_completion_tokens=100,          # OpenAI-specific\n    response_format={\"type\": \"json\"},   # OpenAI-specific\n    seed=12345                         # OpenAI-specific\n)\n</code></pre>"},{"location":"advanced/client/openai_specific/#openai-specific-parameters","title":"OpenAI-Specific Parameters","text":""},{"location":"advanced/client/openai_specific/#frequency_penalty-optionalfloat","title":"<code>frequency_penalty: Optional[float]</code>","text":"<ul> <li>Range: -2.0 to 2.0</li> <li>Penalizes tokens based on their frequency in the text <pre><code>response = client.generate_text(\n    prompt=\"Write a creative story\",\n    model=\"gpt-3.5-turbo\",\n    frequency_penalty=0.7  # Reduces repetition\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/openai_specific/#presence_penalty-optionalfloat","title":"<code>presence_penalty: Optional[float]</code>","text":"<ul> <li>Range: -2.0 to 2.0</li> <li>Penalizes tokens based on their presence in prior text <pre><code>response = client.generate_text(\n    prompt=\"Write a varied story\",\n    model=\"gpt-3.5-turbo\",\n    presence_penalty=0.6  # Encourages topic diversity\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/openai_specific/#logit_bias-optionaldictstr-int","title":"<code>logit_bias: Optional[Dict[str, int]]</code>","text":"<ul> <li>Maps token IDs to bias values (-100 to 100) <pre><code>response = client.generate_text(\n    prompt=\"Write about technology\",\n    model=\"gpt-3.5-turbo\",\n    logit_bias={\n        123: 100,  # Increases likelihood of token 123\n        456: -100  # Decreases likelihood of token 456\n    }\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/openai_specific/#max_completion_tokens-optionalint","title":"<code>max_completion_tokens: Optional[int]</code>","text":"<ul> <li>Maximum tokens for completion <pre><code>response = client.generate_text(\n    prompt=\"Write a summary\",\n    model=\"gpt-3.5-turbo\",\n    max_completion_tokens=100\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/openai_specific/#response_format-responseformat","title":"<code>response_format: ResponseFormat</code>","text":"<ul> <li>Controls output structure <pre><code>response = client.generate_text(\n    prompt=\"List three colors\",\n    model=\"gpt-4\",\n    response_format={\"type\": \"json_object\"}\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/openai_specific/#seed-optionalint","title":"<code>seed: Optional[int]</code>","text":"<ul> <li>For deterministic generation (Beta) <pre><code>response = client.generate_text(\n    prompt=\"Generate a random number\",\n    model=\"gpt-3.5-turbo\",\n    seed=12345\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/openai_specific/#user-str","title":"<code>user: str</code>","text":"<ul> <li>Unique identifier for end-user tracking <pre><code>response = client.generate_text(\n    prompt=\"Hello\",\n    model=\"gpt-3.5-turbo\",\n    user=\"user_123\"\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/openai_specific/#chat-method","title":"chat Method","text":""},{"location":"advanced/client/openai_specific/#basic-structure_1","title":"Basic Structure","text":"<pre><code>response = client.chat(\n    model=\"gpt-3.5-turbo\",            # Required\n    messages=[...],                    # Required\n    tools=[...],                      # OpenAI-specific\n    tool_choice=\"auto\",               # OpenAI-specific\n    response_format={\"type\": \"json\"}, # OpenAI-specific\n    logprobs=True,                    # OpenAI-specific\n    top_logprobs=5                    # OpenAI-specific\n)\n</code></pre>"},{"location":"advanced/client/openai_specific/#openai-specific-parameters_1","title":"OpenAI-Specific Parameters","text":""},{"location":"advanced/client/openai_specific/#tools-iterablechatcompletiontoolparam","title":"<code>tools: Iterable[ChatCompletionToolParam]</code>","text":"<ul> <li>List of available tools (max 128) <pre><code>response = client.chat(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather data\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                }\n            }\n        }\n    }]\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/openai_specific/#tool_choice-chatcompletiontoolchoiceoptionparam","title":"<code>tool_choice: ChatCompletionToolChoiceOptionParam</code>","text":"<ul> <li>Controls tool selection behavior <pre><code>response = client.chat(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Calculate something\"}],\n    tool_choice=\"auto\"  # or \"none\" or \"required\"\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/openai_specific/#modalities-optionallistchatcompletionmodality","title":"<code>modalities: Optional[List[ChatCompletionModality]]</code>","text":"<ul> <li>Output types for generation <pre><code>response = client.chat(\n    model=\"gpt-4o-audio-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Generate audio\"}],\n    modalities=[\"text\", \"audio\"]\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/openai_specific/#audio-optionalchatcompletionaudioparam","title":"<code>audio: Optional[ChatCompletionAudioParam]</code>","text":"<ul> <li>Audio output parameters <pre><code>response = client.chat(\n    model=\"gpt-4o-audio-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Speak this\"}],\n    modalities=[\"audio\"],\n    audio={\"model\": \"tts-1\", \"voice\": \"alloy\"}\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/openai_specific/#metadata-optionaldictstr-str","title":"<code>metadata: Optional[Dict[str, str]]</code>","text":"<ul> <li>Custom tags for filtering <pre><code>response = client.chat(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    metadata={\"purpose\": \"greeting\", \"user_type\": \"new\"}\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/openai_specific/#complete-examples","title":"Complete Examples","text":""},{"location":"advanced/client/openai_specific/#example-1-structured-output-with-tools","title":"Example 1: Structured Output with Tools","text":"<pre><code>response = client.chat(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a data assistant\"},\n        {\"role\": \"user\", \"content\": \"Get weather for Paris\"}\n    ],\n    response_format={\"type\": \"json_object\"},\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather data\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                }\n            }\n        }\n    }],\n    tool_choice=\"auto\"\n)\n</code></pre>"},{"location":"advanced/client/openai_specific/#example-2-advanced-text-generation","title":"Example 2: Advanced Text Generation","text":"<pre><code>response = client.generate_text(\n    prompt=\"Write a technical analysis\",\n    model=\"gpt-4\",\n    max_completion_tokens=500,\n    frequency_penalty=0.7,\n    presence_penalty=0.6,\n    logit_bias={123: 50},\n    user=\"analyst_1\",\n    seed=42\n)\n</code></pre>"},{"location":"advanced/client/openai_specific/#example-3-audio-generation","title":"Example 3: Audio Generation","text":"<pre><code>response = client.chat(\n    model=\"gpt-4o-audio-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Explain quantum physics\"}],\n    modalities=[\"text\", \"audio\"],\n    audio={\n        \"model\": \"tts-1\",\n        \"voice\": \"nova\",\n        \"speed\": 1.0\n    },\n    metadata={\"type\": \"educational\"}\n)\n</code></pre>"},{"location":"advanced/client/openai_specific/#parameter-validation-notes","title":"Parameter Validation Notes","text":"<ol> <li>Both <code>model</code> and <code>prompt</code>/<code>messages</code> are required</li> <li><code>response_format</code> requires compatible models</li> <li>Tool usage limited to 128 functions</li> <li>Audio generation requires specific models</li> <li><code>logprobs</code> must be True when using <code>top_logprobs</code></li> <li><code>seed</code> feature is in Beta and not guaranteed</li> </ol> <p>These parameters allow you to fully customize OpenAI's behavior while working with ClientAI's abstraction layer.</p>"},{"location":"advanced/client/replicate_specific/","title":"Replicate-Specific Parameters in ClientAI","text":"<p>This guide covers the Replicate-specific parameters that can be passed to ClientAI's <code>generate_text</code> and <code>chat</code> methods. These parameters are passed as additional keyword arguments to customize Replicate's behavior.</p>"},{"location":"advanced/client/replicate_specific/#generate_text-method","title":"generate_text Method","text":""},{"location":"advanced/client/replicate_specific/#basic-structure","title":"Basic Structure","text":"<pre><code>from clientai import ClientAI\n\nclient = ClientAI('replicate', api_key=\"your-replicate-api-key\")\nresponse = client.generate_text(\n    prompt=\"Your prompt here\",     # Required\n    model=\"owner/name:version\",    # Required\n    webhook=\"https://...\",         # Replicate-specific\n    webhook_completed=\"https://...\",# Replicate-specific\n    webhook_events_filter=[...],   # Replicate-specific\n    stream=False,                  # Optional\n    wait=True                      # Replicate-specific\n)\n</code></pre>"},{"location":"advanced/client/replicate_specific/#replicate-specific-parameters","title":"Replicate-Specific Parameters","text":""},{"location":"advanced/client/replicate_specific/#webhook-optionalstr","title":"<code>webhook: Optional[str]</code>","text":"<ul> <li>URL to receive POST requests with prediction updates <pre><code>response = client.generate_text(\n    prompt=\"Write a story\",\n    model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\",\n    webhook=\"https://your-server.com/webhook\"\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/replicate_specific/#webhook_completed-optionalstr","title":"<code>webhook_completed: Optional[str]</code>","text":"<ul> <li>URL for receiving completion notifications <pre><code>response = client.generate_text(\n    prompt=\"Generate text\",\n    model=\"meta/llama-2-70b:latest\",\n    webhook_completed=\"https://your-server.com/completed\"\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/replicate_specific/#webhook_events_filter-optionalliststr","title":"<code>webhook_events_filter: Optional[List[str]]</code>","text":"<ul> <li>List of events that trigger webhooks</li> <li>Common events: <code>\"completed\"</code>, <code>\"output\"</code> <pre><code>response = client.generate_text(\n    prompt=\"Analyze text\",\n    model=\"meta/llama-2-70b:latest\",\n    webhook_events_filter=[\"completed\", \"output\"]\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/replicate_specific/#wait-optionalunionint-bool","title":"<code>wait: Optional[Union[int, bool]]</code>","text":"<ul> <li>Controls request blocking behavior</li> <li>True: keeps request open up to 60 seconds</li> <li>int: specifies seconds to hold request (1-60)</li> <li>False: doesn't wait (default) <pre><code>response = client.generate_text(\n    prompt=\"Complex analysis\",\n    model=\"meta/llama-2-70b:latest\",\n    wait=30  # Wait for 30 seconds\n)\n</code></pre></li> </ul>"},{"location":"advanced/client/replicate_specific/#stream-bool","title":"<code>stream: bool</code>","text":"<ul> <li>Enables token streaming for supported models <pre><code>for chunk in client.generate_text(\n    prompt=\"Write a story\",\n    model=\"meta/llama-2-70b:latest\",\n    stream=True\n):\n    print(chunk, end=\"\")\n</code></pre></li> </ul>"},{"location":"advanced/client/replicate_specific/#chat-method","title":"chat Method","text":""},{"location":"advanced/client/replicate_specific/#basic-structure_1","title":"Basic Structure","text":"<pre><code>response = client.chat(\n    model=\"meta/llama-2-70b:latest\",  # Required\n    messages=[...],                    # Required\n    webhook=\"https://...\",             # Replicate-specific\n    webhook_completed=\"https://...\",   # Replicate-specific\n    webhook_events_filter=[...],       # Replicate-specific\n    wait=True                          # Replicate-specific\n)\n</code></pre>"},{"location":"advanced/client/replicate_specific/#message-formatting","title":"Message Formatting","text":"<p>Replicate formats chat messages into a single prompt: <pre><code>prompt = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\nprompt += \"\\nassistant: \"\n</code></pre></p>"},{"location":"advanced/client/replicate_specific/#training-parameters","title":"Training Parameters","text":"<p>When using Replicate's training capabilities:</p> <pre><code>response = client.train(\n    model=\"stability-ai/sdxl\",\n    version=\"39ed52f2a78e934b3ba6e2a89f5b1c712de7dfea535525255b1aa35c5565e08b\",\n    input={\n        \"input_images\": \"https://domain/images.zip\",\n        \"token_string\": \"TOK\",\n        \"caption_prefix\": \"a photo of TOK\",\n        \"max_train_steps\": 1000,\n        \"use_face_detection_instead\": False\n    },\n    destination=\"username/model-name\"\n)\n</code></pre>"},{"location":"advanced/client/replicate_specific/#complete-examples","title":"Complete Examples","text":""},{"location":"advanced/client/replicate_specific/#example-1-generation-with-webhooks","title":"Example 1: Generation with Webhooks","text":"<pre><code>response = client.generate_text(\n    prompt=\"Write a scientific paper summary\",\n    model=\"meta/llama-2-70b:latest\",\n    webhook=\"https://your-server.com/updates\",\n    webhook_completed=\"https://your-server.com/completed\",\n    webhook_events_filter=[\"completed\"],\n    wait=True\n)\n</code></pre>"},{"location":"advanced/client/replicate_specific/#example-2-chat-with-streaming","title":"Example 2: Chat with Streaming","text":"<pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"Write a haiku about coding\"}\n]\n\nfor chunk in client.chat(\n    messages=messages,\n    model=\"meta/llama-2-70b:latest\",\n    stream=True\n):\n    print(chunk, end=\"\")\n</code></pre>"},{"location":"advanced/client/replicate_specific/#example-3-image-generation","title":"Example 3: Image Generation","text":"<pre><code>response = client.generate_text(\n    prompt=\"A portrait of a wombat gentleman\",\n    model=\"stability-ai/stable-diffusion:27b93a2413e7f36cd83da926f3656280b2931564ff050bf9575f1fdf9bcd7478\",\n    wait=60\n)\n</code></pre>"},{"location":"advanced/client/replicate_specific/#error-handling","title":"Error Handling","text":"<p>ClientAI maps Replicate's exceptions to its own error types: <pre><code>try:\n    response = client.generate_text(\n        prompt=\"Test prompt\",\n        model=\"meta/llama-2-70b:latest\",\n        wait=True\n    )\nexcept ClientAIError as e:\n    print(f\"Error: {e}\")\n</code></pre></p> <p>Error mappings: - <code>AuthenticationError</code>: API key issues - <code>RateLimitError</code>: Rate limit exceeded - <code>ModelError</code>: Model not found or failed - <code>InvalidRequestError</code>: Invalid parameters - <code>TimeoutError</code>: Request timeout (default 300s) - <code>APIError</code>: Other server errors</p>"},{"location":"advanced/client/replicate_specific/#parameter-validation-notes","title":"Parameter Validation Notes","text":"<ol> <li>Both <code>model</code> and <code>prompt</code>/<code>messages</code> are required</li> <li>Model string format: <code>\"owner/name:version\"</code> or <code>\"owner/name\"</code> for latest version</li> <li><code>wait</code> must be boolean or integer 1-60</li> <li>Webhook URLs must be valid HTTP/HTTPS URLs</li> <li><code>webhook_events_filter</code> must contain valid event types</li> <li>Some models may not support streaming</li> <li>File inputs can be URLs or local file paths</li> </ol> <p>These parameters allow you to leverage Replicate's features through ClientAI, including model management, webhook notifications, and streaming capabilities.</p>"},{"location":"api/overview/","title":"API Reference Overview","text":"<p>Welcome to the API Reference section of ClientAI documentation. This section provides detailed information about the various classes, functions, and modules that make up ClientAI. Whether you're looking to integrate ClientAI into your project, extend its functionality, or simply explore its capabilities, this section will guide you through the intricacies of our codebase.</p>"},{"location":"api/overview/#key-components","title":"Key Components","text":"<p>ClientAI's API is comprised of several key components, each serving a specific purpose:</p> <ol> <li> <p>ClientAI Class: This is the main class of our library. It provides a unified interface for interacting with different AI providers and is the primary entry point for using ClientAI.</p> <ul> <li>ClientAI Class Reference</li> </ul> </li> <li> <p>AIProvider Class: An abstract base class that defines the interface for all AI provider implementations. It ensures consistency across different providers.</p> <ul> <li>AIProvider Class Reference</li> </ul> </li> <li> <p>Provider-Specific Classes: These classes implement the AIProvider interface for each supported AI service (Ollama, OpenAI, Replicate, Groq).</p> <ul> <li>Ollama Provider Reference</li> <li>OpenAI Provider Reference</li> <li>Replicate Provider Reference</li> <li>Groq Provider Reference</li> </ul> </li> <li> <p>Ollama Manager: These classes handle the local Ollama server configuration and lifecycle management.</p> <ul> <li>OllamaManager Class Reference</li> <li>OllamaServerConfig Class Reference</li> </ul> </li> </ol>"},{"location":"api/overview/#usage","title":"Usage","text":"<p>Each component is documented with its own dedicated page, where you can find detailed information about its methods, parameters, return types, and usage examples. These pages are designed to provide you with all the information you need to understand and work with ClientAI effectively.</p>"},{"location":"api/overview/#basic-usage-example","title":"Basic Usage Example","text":"<p>Here's a quick example of how to use the main ClientAI class:</p> <pre><code>from clientai import ClientAI\n\n# Initialize the client\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Generate text\nresponse = client.generate_text(\n    \"Explain quantum computing\",\n    model=\"gpt-3.5-turbo\"\n)\n\nprint(response)\n</code></pre> <p>For more detailed usage instructions and examples, please refer to the Usage Guide.</p>"},{"location":"api/overview/#contribution","title":"Contribution","text":"<p>We welcome contributions to ClientAI! If you're interested in contributing, please refer to our Contributing Guidelines. Contributions can range from bug fixes and documentation improvements to adding support for new AI providers.</p>"},{"location":"api/overview/#feedback","title":"Feedback","text":"<p>Your feedback is crucial in helping us improve ClientAI and its documentation. If you have any suggestions, corrections, or queries, please don't hesitate to reach out to us via GitHub issues.</p>"},{"location":"api/agent/core/agent/","title":"Agent Class API Reference","text":"<p>The <code>Agent</code> class is the foundation for creating AI-powered agents with automated tool selection and workflow management capabilities. It provides a flexible framework for building agents that can execute multi-step workflows, manage tools, and interact with language models.</p>"},{"location":"api/agent/core/agent/#class-definition","title":"Class Definition","text":"<p>A framework for creating and managing LLM-powered agents with automated tool selection.</p> <p>The Agent class provides a flexible system for building AI agents that can: - Execute multi-step workflows with LLM integration - Automatically select and use appropriate tools - Maintain context and state across steps - Handle streaming responses - Manage tool registration and scoping</p> <p>Attributes:</p> Name Type Description <code>context</code> <p>Manages the agent's state and memory</p> <code>tool_registry</code> <p>Registry of available tools</p> <code>execution_engine</code> <p>Handles step execution</p> <code>workflow_manager</code> <p>Manages workflow execution order</p> Example <p>Create a simple agent with tools: <pre><code>class AnalysisAgent(Agent):\n    @think(\"analyze\")\n    def analyze_data(self, input_data: str) -&gt; str:\n        return f\"Please analyze this data: {input_data}\"\n\n    @act(\"process\")\n    def process_results(self, analysis: str) -&gt; str:\n        return f\"Process these results: {analysis}\"\n\n# Initialize with tools\nagent = AnalysisAgent(\n    client=client,\n    default_model=\"gpt-4\",\n    tools=[calculator, text_processor],\n    tool_confidence=0.8\n)\n\n# Run the agent\nresult = agent.run(\"Analyze data: [1, 2, 3]\")\n</code></pre></p> <p>Using tools with custom registration: <pre><code>class UtilityAgent(Agent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # Register tools directly\n        self.register_tool(\n            utility_function,\n            name=\"Utility\",\n            description=\"Utility function\",\n            scopes=[\"think\", \"act\"]\n        )\n\n    # Or use decorator\n    @register_tool(\n        name=\"Calculator\",\n        description=\"Performs calculations\",\n        scopes=[\"think\"]\n    )\n    def calculate(self, x: int, y: int) -&gt; int:\n        return x + y\n</code></pre></p> Notes <ul> <li>Tools can be registered via decorator or direct registration</li> <li>Steps are executed in order of definition</li> <li>Context maintains state across workflow execution</li> <li>Tool selection is automatic based on confidence thresholds</li> <li>Streaming can be controlled at step or run level</li> </ul> Source code in <code>clientai/agent/core/agent.py</code> <pre><code>class Agent:\n    \"\"\"A framework for creating and managing LLM-powered\n    agents with automated tool selection.\n\n    The Agent class provides a flexible system for building AI agents that can:\n    - Execute multi-step workflows with LLM integration\n    - Automatically select and use appropriate tools\n    - Maintain context and state across steps\n    - Handle streaming responses\n    - Manage tool registration and scoping\n\n    Attributes:\n        context: Manages the agent's state and memory\n        tool_registry: Registry of available tools\n        execution_engine: Handles step execution\n        workflow_manager: Manages workflow execution order\n\n    Example:\n        Create a simple agent with tools:\n        ```python\n        class AnalysisAgent(Agent):\n            @think(\"analyze\")\n            def analyze_data(self, input_data: str) -&gt; str:\n                return f\"Please analyze this data: {input_data}\"\n\n            @act(\"process\")\n            def process_results(self, analysis: str) -&gt; str:\n                return f\"Process these results: {analysis}\"\n\n        # Initialize with tools\n        agent = AnalysisAgent(\n            client=client,\n            default_model=\"gpt-4\",\n            tools=[calculator, text_processor],\n            tool_confidence=0.8\n        )\n\n        # Run the agent\n        result = agent.run(\"Analyze data: [1, 2, 3]\")\n        ```\n\n        Using tools with custom registration:\n        ```python\n        class UtilityAgent(Agent):\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n\n                # Register tools directly\n                self.register_tool(\n                    utility_function,\n                    name=\"Utility\",\n                    description=\"Utility function\",\n                    scopes=[\"think\", \"act\"]\n                )\n\n            # Or use decorator\n            @register_tool(\n                name=\"Calculator\",\n                description=\"Performs calculations\",\n                scopes=[\"think\"]\n            )\n            def calculate(self, x: int, y: int) -&gt; int:\n                return x + y\n        ```\n\n    Notes:\n        - Tools can be registered via decorator or direct registration\n        - Steps are executed in order of definition\n        - Context maintains state across workflow execution\n        - Tool selection is automatic based on confidence thresholds\n        - Streaming can be controlled at step or run level\n    \"\"\"\n\n    def __init__(\n        self,\n        client: ClientAI[AIProviderProtocol, Any, Any],\n        default_model: Union[str, Dict[str, Any], ModelConfig],\n        tools: Optional[List[ToolConfig]] = None,\n        tool_selection_config: Optional[ToolSelectionConfig] = None,\n        tool_confidence: Optional[float] = None,\n        tool_model: Optional[Union[str, Dict[str, Any], ModelConfig]] = None,\n        max_tools_per_step: Optional[int] = None,\n        max_history_size: Optional[int] = None,\n        **default_model_kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize an Agent instance with specified configurations.\n\n        Args:\n            client: The AI client for model interactions\n            default_model: Primary model config for the agent. Can be:\n                - A string (model name)\n                - A dict with model parameters\n                - A ModelConfig instance\n            tools: Optional list of tools to register. Can be:\n                - Functions (with proper type hints and docstrings)\n                - Tool instances\n                - ToolConfig instances\n            tool_selection_config: Complete tool selection configuration\n            tool_confidence: Confidence threshold for tool selection (0.0-1.0)\n            tool_model: Model to use for tool selection decisions\n            max_tools_per_step: Maximum tools allowed per step\n            max_history_size: Maximum number of previous interactions to\n                              maintain in context history (defaults to 10)\n            **default_model_kwargs: Additional kwargs for default model\n\n        Raises:\n            AgentError: If initialization fails due to:\n                - Invalid model configuration\n                - Incompatible tool configurations\n                - Component initialization failure\n\n        Example:\n            Basic initialization:\n            ```python\n            agent = MyAgent(\n                client=client,\n                default_model=\"gpt-4\",\n                tool_confidence=0.8\n            )\n            ```\n\n            Detailed configuration:\n            ```python\n            agent = MyAgent(\n                client=client,\n                default_model=ModelConfig(\n                    name=\"gpt-4\",\n                    temperature=0.7\n                ),\n                tool_selection_config=ToolSelectionConfig(\n                    confidence_threshold=0.8,\n                    max_tools_per_step=3\n                ),\n                tool_model=\"llama-2\"\n            )\n            ```\n\n        Notes:\n            - Cannot specify both tool_selection_config and\n              individual tool parameters\n            - Model can be specified as string, dict, or ModelConfig\n            - Tools can be pre-configured or added after initialization\n        \"\"\"\n        try:\n            if not default_model:\n                raise ValueError(\"default_model must be specified\")\n\n            if tool_selection_config and any(\n                x is not None\n                for x in [tool_confidence, tool_model, max_tools_per_step]\n            ):\n                raise ValueError(\n                    \"Cannot specify both tool_selection_config and individual \"\n                    \"tool parameters \"\n                    \"(tool_confidence, tool_model, max_tools_per_step)\"\n                )\n\n            self._client = client\n            self._default_model_kwargs = default_model_kwargs\n            self._default_model = self._create_model_config(default_model)\n\n            if tool_selection_config:\n                self._tool_selection_config = tool_selection_config\n            else:\n                config_params = {}\n                if tool_confidence is not None:\n                    config_params[\"confidence_threshold\"] = tool_confidence\n                if max_tools_per_step is not None:\n                    config_params[\"max_tools_per_step\"] = max_tools_per_step\n                self._tool_selection_config = ToolSelectionConfig.create(\n                    **config_params\n                )\n\n            self._tool_model = (\n                self._create_model_config(tool_model)\n                if tool_model is not None\n                else self._default_model\n            )\n\n            self.context = AgentContext(\n                max_history_size=max_history_size\n                if max_history_size is not None\n                else 10\n            )\n            self.tool_registry = ToolRegistry()\n            self.execution_engine = StepExecutionEngine(\n                client=self._client,\n                default_model=self._default_model,\n                default_kwargs=self._default_model_kwargs,\n                tool_selection_config=self._tool_selection_config,\n                tool_model=self._tool_model,\n            )\n            self.workflow_manager = WorkflowManager()\n\n            if tools:\n                for tool_item in tools:\n                    if isinstance(tool_item, ToolConfig):\n                        tool_config = tool_item\n                    elif isinstance(tool_item, Tool):\n                        tool_config = ToolConfig(tool=tool_item)\n                    else:\n                        try:\n                            tool_instance = Tool.create(func=tool_item)\n                            tool_config = ToolConfig(tool=tool_instance)\n                        except Exception as e:\n                            raise ValueError(\n                                f\"Failed to create tool from \"\n                                f\"function: {str(e)}\"\n                            )\n                    self.tool_registry.register(tool_config)\n\n            self._register_class_tools()\n            self.workflow_manager.register_class_steps(self)\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize agent: {e}\")\n            raise AgentError(f\"Failed to initialize agent: {str(e)}\") from e\n\n    def _create_model_config(\n        self, model: Union[str, Dict[str, Any], ModelConfig]\n    ) -&gt; ModelConfig:\n        \"\"\"Create a ModelConfig instance from various input types.\n\n        Args:\n            model: Model specification (string, dict, or ModelConfig)\n\n        Returns:\n            ModelConfig: Validated model configuration\n\n        Raises:\n            AgentError: If model specification is invalid\n        \"\"\"\n        try:\n            if isinstance(model, str):\n                return ModelConfig(name=model)\n\n            if isinstance(model, dict):\n                if \"name\" not in model:\n                    raise ValueError(\n                        \"Model configuration must include a 'name' parameter\"\n                    )\n                try:\n                    return ModelConfig(**model)\n                except TypeError as e:\n                    raise ValueError(\n                        f\"Invalid model configuration parameters: {str(e)}\"\n                    )\n\n            if isinstance(model, ModelConfig):\n                return model\n\n            raise ValueError(\n                \"Model must be a string, dict with \"\n                \"'name', or ModelConfig instance\"\n            )\n\n        except ValueError as e:\n            logger.error(f\"Model configuration error: {e}\")\n            raise AgentError(str(e)) from e\n        except Exception as e:\n            logger.error(f\"Unexpected error creating model configuration: {e}\")\n            raise AgentError(\n                f\"Unexpected error creating model configuration: {str(e)}\"\n            ) from e\n\n    def _should_stream(self, stream_override: Optional[bool] = None) -&gt; bool:\n        \"\"\"Determine if workflow should return streaming response.\n\n        Args:\n            stream_override: Optional bool to override step configuration\n\n        Returns:\n            bool: Whether to enable streaming\n\n        Raises:\n            AgentError: If stream configuration cannot be determined\n        \"\"\"\n        try:\n            if stream_override is not None:\n                return stream_override\n\n            try:\n                steps = self.workflow_manager.get_steps()\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to retrieve workflow steps: {str(e)}\"\n                )\n\n            if not steps:\n                return False\n\n            try:\n                last_step = next(reversed(steps.values()))\n                return getattr(last_step, \"stream\", False)\n            except StopIteration:\n                raise ValueError(\"No steps found in workflow\")\n            except AttributeError as e:\n                raise ValueError(f\"Invalid step configuration: {str(e)}\")\n\n        except ValueError as e:\n            logger.error(f\"Stream configuration error: {e}\")\n            raise AgentError(str(e)) from e\n        except Exception as e:\n            logger.error(\n                f\"Unexpected error determining stream configuration: {e}\"\n            )\n            raise AgentError(\n                f\"Unexpected error determining stream configuration: {str(e)}\"\n            ) from e\n\n    def _handle_streaming(\n        self,\n        result: Union[str, Iterator[str]],\n        stream_override: Optional[bool] = None,\n    ) -&gt; Union[str, Iterator[str]]:\n        \"\"\"Process workflow result based on streaming configuration.\n\n        Args:\n            result: Raw result from workflow execution\n            stream_override: Optional streaming configuration override\n\n        Returns:\n            Processed result based on streaming settings\n\n        Raises:\n            AgentError: If stream handling fails\n        \"\"\"\n        try:\n            try:\n                should_stream = self._should_stream(stream_override)\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to determine streaming configuration: {str(e)}\"\n                )\n\n            if not should_stream:\n                if isinstance(result, str):\n                    return result\n                try:\n                    return \"\".join(list(result))\n                except Exception as e:\n                    raise ValueError(\n                        f\"Failed to join stream results: {str(e)}\"\n                    )\n\n            if isinstance(result, str):\n                return iter([result])\n\n            if not hasattr(result, \"__iter__\"):\n                raise ValueError(\n                    \"Result must be either a string or an iterator\"\n                )\n\n            return result\n\n        except ValueError as e:\n            logger.error(f\"Stream handling error: {e}\")\n            raise AgentError(str(e)) from e\n        except Exception as e:\n            logger.error(f\"Unexpected error handling stream: {e}\")\n            raise AgentError(\n                f\"Unexpected error handling stream: {str(e)}\"\n            ) from e\n\n    def use_tool(self, name: str, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Execute a registered tool by its name.\n\n        Allows direct tool execution with provided arguments, bypassing\n        automatic tool selection.\n\n        Args:\n            name: Name of the tool to execute\n            *args: Positional arguments for the tool\n            **kwargs: Keyword arguments for the tool\n\n        Returns:\n            Any: Result of the tool execution\n\n        Raises:\n            ToolError: If tool execution fails or tool isn't found\n\n        Example:\n            Direct tool execution:\n            ```python\n            # Execute a calculator tool\n            result = agent.use_tool(\"calculator\", x=5, y=3)\n            print(result)  # Output: 8\n\n            # Execute a text processor\n            result = agent.use_tool(\n                \"text_processor\",\n                text=\"hello\",\n                uppercase=True\n            )\n            print(result)  # Output: \"HELLO\"\n            ```\n\n        Notes:\n            - Tool must be registered before use\n            - Arguments must match tool's signature\n            - Does not affect agent's tool usage history\n        \"\"\"\n        try:\n            tool = self.tool_registry.get(name)\n            if not tool:\n                raise ValueError(f\"Tool '{name}' not found in registry\")\n\n            try:\n                return tool(*args, **kwargs)\n            except TypeError as e:\n                raise ValueError(\n                    f\"Invalid arguments for tool '{name}': {str(e)}\"\n                )\n            except Exception as e:\n                raise ValueError(f\"Tool execution failed: {str(e)}\")\n\n        except ValueError as e:\n            logger.error(f\"Tool usage error: {e}\")\n            raise ToolError(str(e)) from e\n        except Exception as e:\n            logger.error(f\"Unexpected error using tool '{name}': {e}\")\n            raise ToolError(\n                f\"Unexpected error using tool '{name}': {str(e)}\"\n            ) from e\n\n    def get_tools(self, scope: Optional[str] = None) -&gt; List[Tool]:\n        \"\"\"Retrieve tools available to the agent, optionally filtered by scope.\n\n        Args:\n            scope: Optional scope to filter tools by. Valid scopes:\n                - \"think\": Analysis and reasoning tools\n                - \"act\": Action and execution tools\n                - \"observe\": Data gathering tools\n                - \"synthesize\": Summary and integration tools\n                - None: Return all tools\n\n        Returns:\n            List[Tool]: List of available tools matching the criteria\n\n        Example:\n            Get tools by scope:\n            ```python\n            # Get all tools\n            all_tools = agent.get_tools()\n            print(f\"Total tools: {len(all_tools)}\")\n\n            # Get thinking tools\n            think_tools = agent.get_tools(\"think\")\n            for tool in think_tools:\n                print(f\"- {tool.name}: {tool.description}\")\n\n            # Get action tools\n            act_tools = agent.get_tools(\"act\")\n            print(f\"Action tools: {[t.name for t in act_tools]}\")\n            ```\n        \"\"\"\n        return self.tool_registry.get_for_scope(scope)\n\n    def run(\n        self, input_data: Any, *, stream: Optional[bool] = None\n    ) -&gt; Union[str, Iterator[str]]:\n        \"\"\"Execute the agent's workflow with the provided input data.\n\n        Args:\n            input_data: The initial input to process\n            stream: Optional bool to override streaming configuration.\n                If provided, overrides the last step's stream setting.\n\n        Returns:\n            Union[str, Iterator[str]]: Either:\n                - Complete response string (streaming disabled)\n                - Iterator of response chunks (streaming enabled)\n\n        Raises:\n            WorkflowError: If workflow execution fails\n            StepError: If a required step fails\n            ClientAIError: If LLM interaction fails\n\n        Example:\n            Basic execution:\n            ```python\n            # Without streaming\n            result = agent.run(\"Analyze this data\")\n            print(result)\n\n            # With streaming\n            for chunk in agent.run(\"Process this\", stream=True):\n                print(chunk, end=\"\", flush=True)\n\n            # Use step configuration\n            result = agent.run(\"Analyze this\")  # Uses last step's setting\n            ```\n\n        Notes:\n            - Streaming can be controlled by parameter or step configuration\n            - Workflow executes steps in defined order\n            - Context is updated after each step\n            - Tool selection occurs automatically if enabled\n        \"\"\"\n        try:\n            self.context.set_input(input_data)\n\n            try:\n                result = self.workflow_manager.execute(\n                    agent=self,\n                    input_data=input_data,\n                    engine=self.execution_engine,\n                    stream_override=stream,\n                )\n            except Exception as e:\n                raise ValueError(f\"Workflow execution failed: {str(e)}\")\n\n            try:\n                return self._handle_streaming(result, stream)\n            except Exception as e:\n                raise ValueError(f\"Stream handling failed: {str(e)}\")\n\n        except ValueError as e:\n            logger.error(f\"Workflow execution error: {e}\")\n            raise WorkflowError(str(e)) from e\n        except Exception as e:\n            logger.error(f\"Unexpected error during workflow execution: {e}\")\n            raise WorkflowError(\n                f\"Unexpected error during workflow execution: {str(e)}\"\n            ) from e\n\n    def reset_context(self) -&gt; None:\n        \"\"\"Reset the agent's context, clearing all memory and state.\n\n        Example:\n            ```python\n            # After processing\n            print(len(agent.context.memory))  # Output: 5\n\n            # Reset context\n            agent.reset_context()\n            print(len(agent.context.memory))  # Output: 0\n            print(agent.context.state)  # Output: {}\n            ```\n\n        Notes:\n            - Clears memory, state, and results\n            - Does not affect workflow or tool registration\n            - Resets iteration counter\n        \"\"\"\n        self.context.clear()\n\n    def reset(self) -&gt; None:\n        \"\"\"Perform a complete reset of the agent.\n\n        Resets all state including context,\n        workflow state, and iteration counters.\n\n        Example:\n            ```python\n            # Complete reset\n            agent.reset()\n            print(len(agent.context.memory))  # Output: 0\n            print(len(agent.workflow_manager.get_steps()))  # Output: 0\n            ```\n\n        Notes:\n            - More comprehensive than reset_context\n            - Clears workflow state\n            - Maintains tool registration\n            - Returns agent to initial state\n        \"\"\"\n        self.context.clear()\n        self.workflow_manager.reset()\n\n    def register_tool(\n        self,\n        tool: Union[Callable[..., Any], Tool],\n        *,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        scopes: Union[List[str], str] = \"all\",\n    ) -&gt; Tool:\n        \"\"\"\n        Register a tool with the agent for use in specified workflow scopes.\n\n        Provides a flexible way to register tools with\n        the agent through multiple methods:\n\n        1. Direct registration of functions\n        2. Registration of pre-created Tool instances\n        3. As a decorator for class methods\n\n        The unified registration supports scope control and can be used both\n        as a method and decorator.\n\n        Args:\n            tool: Function to register as a tool or a pre-created Tool instance\n            name: Optional custom name for the tool\n            description: Optional description of the tool's functionality\n            scopes: List of scopes where the tool can be used, or a single\n                    scope string. Valid scopes are:\n                    \"think\", \"act\", \"observe\", \"synthesize\", \"all\"\n                    Defaults to \"all\"\n\n        Returns:\n            Tool: The registered Tool instance\n\n        Raises:\n            ToolError: If tool validation or registration fails\n            ValueError: If scopes are invalid or tool is already registered\n\n        Example:\n            Direct function registration:\n            ```python\n            def add(x: int, y: int) -&gt; int:\n                return x + y\n\n            tool = agent.register_tool(\n                add,\n                name=\"Calculator\",\n                description=\"Adds numbers\",\n                scopes=[\"think\", \"act\"]\n            )\n            ```\n\n            Register pre-created Tool:\n            ```python\n            my_tool = Tool.create(multiply, name=\"Multiplier\")\n            agent.register_tool(my_tool, scopes=\"all\")\n            ```\n\n            As a decorator:\n            ```python\n            class MyAgent(Agent):\n                @register_tool(\n                    name=\"TextProcessor\",\n                    description=\"Processes text\",\n                    scopes=[\"act\", \"synthesize\"]\n                )\n                def process_text(text: str) -&gt; str:\n                    return text.upper()\n            ```\n\n            Register with specific scopes:\n            ```python\n            agent.register_tool(\n                utility_function,\n                name=\"Utility\",\n                description=\"Utility function\",\n                scopes=[\"think\", \"observe\"]\n            )\n            ```\n        \"\"\"\n        try:\n            if isinstance(scopes, str):\n                scopes = [scopes]\n\n            try:\n                tool_scopes = frozenset(ToolScope.from_str(s) for s in scopes)\n            except ValueError as e:\n                raise ValueError(f\"Invalid tool scope: {str(e)}\")\n\n            if isinstance(tool, Tool):\n                tool_instance = tool\n            else:\n                try:\n                    tool_instance = Tool.create(\n                        func=tool,\n                        name=name,\n                        description=description,\n                    )\n                except ValueError as e:\n                    raise ValueError(f\"Invalid tool function: {str(e)}\")\n\n            tool_config = ToolConfig(\n                tool=tool_instance,\n                scopes=tool_scopes,\n                name=tool_instance.name,\n                description=tool_instance.description,\n            )\n\n            try:\n                self.tool_registry.register(tool_config)\n            except ValueError as e:\n                raise ValueError(f\"Tool registration failed: {str(e)}\")\n\n            return tool_instance\n\n        except ValueError as e:\n            logger.error(f\"Tool validation error: {e}\")\n            raise ToolError(str(e)) from e\n        except Exception as e:\n            logger.error(f\"Failed to register tool: {e}\")\n            raise ToolError(f\"Failed to register tool: {str(e)}\") from e\n\n    def register_tool_decorator(\n        self,\n        *,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        scopes: Union[List[str], str] = \"all\",\n    ) -&gt; Callable[[Callable[..., Any]], Tool]:\n        \"\"\"Create a decorator for registering tools with the agent.\n\n        A decorator-based approach for registering tools,\n        providing a clean way to integrate tool registration\n        with method definitions.\n\n        Args:\n            name: Optional custom name for the tool\n            description: Optional description of the tool's functionality\n            scopes: List of scopes or single scope where tool can be used.\n                Valid scopes: \"think\", \"act\", \"observe\", \"synthesize\", \"all\"\n\n        Returns:\n            Callable: A decorator function that registers\n                      the decorated method as a tool\n\n        Example:\n            Basic tool registration with scopes:\n            ```python\n            class MyAgent(Agent):\n                @register_tool_decorator(\n                    name=\"Calculator\",\n                    description=\"Adds two numbers\",\n                    scopes=[\"think\", \"act\"]\n                )\n                def add_numbers(self, x: int, y: int) -&gt; int:\n                    return x + y\n\n                @register_tool_decorator(scopes=\"observe\")\n                def get_data(self, query: str) -&gt; List[int]:\n                    return [1, 2, 3]  # Example data\n            ```\n\n            Multiple tools with different scopes:\n            ```python\n            class AnalysisAgent(Agent):\n                @register_tool_decorator(\n                    name=\"TextAnalyzer\",\n                    scopes=[\"think\"]\n                )\n                def analyze_text(self, text: str) -&gt; dict:\n                    return {\"words\": len(text.split())}\n\n                @register_tool_decorator(\n                    name=\"DataFormatter\",\n                    scopes=[\"synthesize\"]\n                )\n                def format_data(self, data: dict) -&gt; str:\n                    return json.dumps(data, indent=2)\n            ```\n\n        Notes:\n            - Decorated methods become available tools\n            - Tool name defaults to method name if not provided\n            - Description defaults to method docstring\n            - Tools are registered during agent initialization\n        \"\"\"\n\n        def decorator(func: Callable[..., Any]) -&gt; Tool:\n            return self.register_tool(\n                func, name=name, description=description, scopes=scopes\n            )\n\n        return decorator\n\n    def _register_class_tools(self) -&gt; None:\n        \"\"\"Register tools defined as class methods using decorators.\n\n        Raises:\n            ToolError: If tool registration fails\n        \"\"\"\n        try:\n            logger.debug(\"Registering class-level tools\")\n            for name, attr in self.__class__.__dict__.items():\n                if hasattr(attr, \"_is_tool\"):\n                    logger.debug(f\"Found class tool: {name}\")\n                    method = getattr(self, name)\n                    scopes = getattr(attr, \"_tool_scopes\", \"all\")\n                    self.register_tool(\n                        tool=method,\n                        name=getattr(attr, \"_tool_name\", name),\n                        description=getattr(attr, \"_tool_description\", None),\n                        scopes=scopes,\n                    )\n        except AttributeError as e:\n            logger.error(\n                f\"Invalid tool attribute during class registration: {e}\"\n            )\n            raise ToolError(f\"Invalid tool attribute: {str(e)}\") from e\n        except Exception as e:\n            logger.error(f\"Failed to register class tools: {e}\")\n            raise ToolError(f\"Failed to register class tools: {str(e)}\") from e\n\n    def __str__(self) -&gt; str:\n        \"\"\"Provide a formatted string representation of the agent.\n\n        Returns:\n            str: A human-readable description of the agent's configuration\n\n        Example:\n            ```python\n            agent = MyAgent(client=client, model=\"gpt-4\")\n            print(agent)\n            # Output example:\n            # \u256d\u2500 MyAgent (openai provider)\n            # \u2502\n            # \u2502 Configuration:\n            # \u2502 \u251c\u2500 Model: gpt-4\n            # \u2502 \u2514\u2500 Parameters: temperature=0.7\n            # \u2502\n            # \u2502 Workflow:\n            # \u2502 \u251c\u2500 1. analyze\n            # \u2502 \u2502  \u251c\u2500 Type: think\n            # \u2502 \u2502  \u251c\u2500 Model: gpt-4\n            # \u2502 \u2502  \u2514\u2500 Description: Analyzes input data\n            # ...\n            ```\n        \"\"\"\n        formatter = AgentFormatter()\n        return formatter.format_agent(self)\n</code></pre>"},{"location":"api/agent/core/agent/#clientai.agent.core.Agent.__init__","title":"<code>__init__(client, default_model, tools=None, tool_selection_config=None, tool_confidence=None, tool_model=None, max_tools_per_step=None, max_history_size=None, **default_model_kwargs)</code>","text":"<p>Initialize an Agent instance with specified configurations.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>ClientAI[AIProviderProtocol, Any, Any]</code> <p>The AI client for model interactions</p> required <code>default_model</code> <code>Union[str, Dict[str, Any], ModelConfig]</code> <p>Primary model config for the agent. Can be: - A string (model name) - A dict with model parameters - A ModelConfig instance</p> required <code>tools</code> <code>Optional[List[ToolConfig]]</code> <p>Optional list of tools to register. Can be: - Functions (with proper type hints and docstrings) - Tool instances - ToolConfig instances</p> <code>None</code> <code>tool_selection_config</code> <code>Optional[ToolSelectionConfig]</code> <p>Complete tool selection configuration</p> <code>None</code> <code>tool_confidence</code> <code>Optional[float]</code> <p>Confidence threshold for tool selection (0.0-1.0)</p> <code>None</code> <code>tool_model</code> <code>Optional[Union[str, Dict[str, Any], ModelConfig]]</code> <p>Model to use for tool selection decisions</p> <code>None</code> <code>max_tools_per_step</code> <code>Optional[int]</code> <p>Maximum tools allowed per step</p> <code>None</code> <code>max_history_size</code> <code>Optional[int]</code> <p>Maximum number of previous interactions to               maintain in context history (defaults to 10)</p> <code>None</code> <code>**default_model_kwargs</code> <code>Any</code> <p>Additional kwargs for default model</p> <code>{}</code> <p>Raises:</p> Type Description <code>AgentError</code> <p>If initialization fails due to: - Invalid model configuration - Incompatible tool configurations - Component initialization failure</p> Example <p>Basic initialization: <pre><code>agent = MyAgent(\n    client=client,\n    default_model=\"gpt-4\",\n    tool_confidence=0.8\n)\n</code></pre></p> <p>Detailed configuration: <pre><code>agent = MyAgent(\n    client=client,\n    default_model=ModelConfig(\n        name=\"gpt-4\",\n        temperature=0.7\n    ),\n    tool_selection_config=ToolSelectionConfig(\n        confidence_threshold=0.8,\n        max_tools_per_step=3\n    ),\n    tool_model=\"llama-2\"\n)\n</code></pre></p> Notes <ul> <li>Cannot specify both tool_selection_config and   individual tool parameters</li> <li>Model can be specified as string, dict, or ModelConfig</li> <li>Tools can be pre-configured or added after initialization</li> </ul> Source code in <code>clientai/agent/core/agent.py</code> <pre><code>def __init__(\n    self,\n    client: ClientAI[AIProviderProtocol, Any, Any],\n    default_model: Union[str, Dict[str, Any], ModelConfig],\n    tools: Optional[List[ToolConfig]] = None,\n    tool_selection_config: Optional[ToolSelectionConfig] = None,\n    tool_confidence: Optional[float] = None,\n    tool_model: Optional[Union[str, Dict[str, Any], ModelConfig]] = None,\n    max_tools_per_step: Optional[int] = None,\n    max_history_size: Optional[int] = None,\n    **default_model_kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize an Agent instance with specified configurations.\n\n    Args:\n        client: The AI client for model interactions\n        default_model: Primary model config for the agent. Can be:\n            - A string (model name)\n            - A dict with model parameters\n            - A ModelConfig instance\n        tools: Optional list of tools to register. Can be:\n            - Functions (with proper type hints and docstrings)\n            - Tool instances\n            - ToolConfig instances\n        tool_selection_config: Complete tool selection configuration\n        tool_confidence: Confidence threshold for tool selection (0.0-1.0)\n        tool_model: Model to use for tool selection decisions\n        max_tools_per_step: Maximum tools allowed per step\n        max_history_size: Maximum number of previous interactions to\n                          maintain in context history (defaults to 10)\n        **default_model_kwargs: Additional kwargs for default model\n\n    Raises:\n        AgentError: If initialization fails due to:\n            - Invalid model configuration\n            - Incompatible tool configurations\n            - Component initialization failure\n\n    Example:\n        Basic initialization:\n        ```python\n        agent = MyAgent(\n            client=client,\n            default_model=\"gpt-4\",\n            tool_confidence=0.8\n        )\n        ```\n\n        Detailed configuration:\n        ```python\n        agent = MyAgent(\n            client=client,\n            default_model=ModelConfig(\n                name=\"gpt-4\",\n                temperature=0.7\n            ),\n            tool_selection_config=ToolSelectionConfig(\n                confidence_threshold=0.8,\n                max_tools_per_step=3\n            ),\n            tool_model=\"llama-2\"\n        )\n        ```\n\n    Notes:\n        - Cannot specify both tool_selection_config and\n          individual tool parameters\n        - Model can be specified as string, dict, or ModelConfig\n        - Tools can be pre-configured or added after initialization\n    \"\"\"\n    try:\n        if not default_model:\n            raise ValueError(\"default_model must be specified\")\n\n        if tool_selection_config and any(\n            x is not None\n            for x in [tool_confidence, tool_model, max_tools_per_step]\n        ):\n            raise ValueError(\n                \"Cannot specify both tool_selection_config and individual \"\n                \"tool parameters \"\n                \"(tool_confidence, tool_model, max_tools_per_step)\"\n            )\n\n        self._client = client\n        self._default_model_kwargs = default_model_kwargs\n        self._default_model = self._create_model_config(default_model)\n\n        if tool_selection_config:\n            self._tool_selection_config = tool_selection_config\n        else:\n            config_params = {}\n            if tool_confidence is not None:\n                config_params[\"confidence_threshold\"] = tool_confidence\n            if max_tools_per_step is not None:\n                config_params[\"max_tools_per_step\"] = max_tools_per_step\n            self._tool_selection_config = ToolSelectionConfig.create(\n                **config_params\n            )\n\n        self._tool_model = (\n            self._create_model_config(tool_model)\n            if tool_model is not None\n            else self._default_model\n        )\n\n        self.context = AgentContext(\n            max_history_size=max_history_size\n            if max_history_size is not None\n            else 10\n        )\n        self.tool_registry = ToolRegistry()\n        self.execution_engine = StepExecutionEngine(\n            client=self._client,\n            default_model=self._default_model,\n            default_kwargs=self._default_model_kwargs,\n            tool_selection_config=self._tool_selection_config,\n            tool_model=self._tool_model,\n        )\n        self.workflow_manager = WorkflowManager()\n\n        if tools:\n            for tool_item in tools:\n                if isinstance(tool_item, ToolConfig):\n                    tool_config = tool_item\n                elif isinstance(tool_item, Tool):\n                    tool_config = ToolConfig(tool=tool_item)\n                else:\n                    try:\n                        tool_instance = Tool.create(func=tool_item)\n                        tool_config = ToolConfig(tool=tool_instance)\n                    except Exception as e:\n                        raise ValueError(\n                            f\"Failed to create tool from \"\n                            f\"function: {str(e)}\"\n                        )\n                self.tool_registry.register(tool_config)\n\n        self._register_class_tools()\n        self.workflow_manager.register_class_steps(self)\n\n    except Exception as e:\n        logger.error(f\"Failed to initialize agent: {e}\")\n        raise AgentError(f\"Failed to initialize agent: {str(e)}\") from e\n</code></pre>"},{"location":"api/agent/core/agent/#clientai.agent.core.Agent.__str__","title":"<code>__str__()</code>","text":"<p>Provide a formatted string representation of the agent.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A human-readable description of the agent's configuration</p> Example <pre><code>agent = MyAgent(client=client, model=\"gpt-4\")\nprint(agent)\n# Output example:\n# \u256d\u2500 MyAgent (openai provider)\n# \u2502\n# \u2502 Configuration:\n# \u2502 \u251c\u2500 Model: gpt-4\n# \u2502 \u2514\u2500 Parameters: temperature=0.7\n# \u2502\n# \u2502 Workflow:\n# \u2502 \u251c\u2500 1. analyze\n# \u2502 \u2502  \u251c\u2500 Type: think\n# \u2502 \u2502  \u251c\u2500 Model: gpt-4\n# \u2502 \u2502  \u2514\u2500 Description: Analyzes input data\n# ...\n</code></pre> Source code in <code>clientai/agent/core/agent.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Provide a formatted string representation of the agent.\n\n    Returns:\n        str: A human-readable description of the agent's configuration\n\n    Example:\n        ```python\n        agent = MyAgent(client=client, model=\"gpt-4\")\n        print(agent)\n        # Output example:\n        # \u256d\u2500 MyAgent (openai provider)\n        # \u2502\n        # \u2502 Configuration:\n        # \u2502 \u251c\u2500 Model: gpt-4\n        # \u2502 \u2514\u2500 Parameters: temperature=0.7\n        # \u2502\n        # \u2502 Workflow:\n        # \u2502 \u251c\u2500 1. analyze\n        # \u2502 \u2502  \u251c\u2500 Type: think\n        # \u2502 \u2502  \u251c\u2500 Model: gpt-4\n        # \u2502 \u2502  \u2514\u2500 Description: Analyzes input data\n        # ...\n        ```\n    \"\"\"\n    formatter = AgentFormatter()\n    return formatter.format_agent(self)\n</code></pre>"},{"location":"api/agent/core/agent/#clientai.agent.core.Agent.get_tools","title":"<code>get_tools(scope=None)</code>","text":"<p>Retrieve tools available to the agent, optionally filtered by scope.</p> <p>Parameters:</p> Name Type Description Default <code>scope</code> <code>Optional[str]</code> <p>Optional scope to filter tools by. Valid scopes: - \"think\": Analysis and reasoning tools - \"act\": Action and execution tools - \"observe\": Data gathering tools - \"synthesize\": Summary and integration tools - None: Return all tools</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tool]</code> <p>List[Tool]: List of available tools matching the criteria</p> Example <p>Get tools by scope: <pre><code># Get all tools\nall_tools = agent.get_tools()\nprint(f\"Total tools: {len(all_tools)}\")\n\n# Get thinking tools\nthink_tools = agent.get_tools(\"think\")\nfor tool in think_tools:\n    print(f\"- {tool.name}: {tool.description}\")\n\n# Get action tools\nact_tools = agent.get_tools(\"act\")\nprint(f\"Action tools: {[t.name for t in act_tools]}\")\n</code></pre></p> Source code in <code>clientai/agent/core/agent.py</code> <pre><code>def get_tools(self, scope: Optional[str] = None) -&gt; List[Tool]:\n    \"\"\"Retrieve tools available to the agent, optionally filtered by scope.\n\n    Args:\n        scope: Optional scope to filter tools by. Valid scopes:\n            - \"think\": Analysis and reasoning tools\n            - \"act\": Action and execution tools\n            - \"observe\": Data gathering tools\n            - \"synthesize\": Summary and integration tools\n            - None: Return all tools\n\n    Returns:\n        List[Tool]: List of available tools matching the criteria\n\n    Example:\n        Get tools by scope:\n        ```python\n        # Get all tools\n        all_tools = agent.get_tools()\n        print(f\"Total tools: {len(all_tools)}\")\n\n        # Get thinking tools\n        think_tools = agent.get_tools(\"think\")\n        for tool in think_tools:\n            print(f\"- {tool.name}: {tool.description}\")\n\n        # Get action tools\n        act_tools = agent.get_tools(\"act\")\n        print(f\"Action tools: {[t.name for t in act_tools]}\")\n        ```\n    \"\"\"\n    return self.tool_registry.get_for_scope(scope)\n</code></pre>"},{"location":"api/agent/core/agent/#clientai.agent.core.Agent.register_tool","title":"<code>register_tool(tool, *, name=None, description=None, scopes='all')</code>","text":"<p>Register a tool with the agent for use in specified workflow scopes.</p> <p>Provides a flexible way to register tools with the agent through multiple methods:</p> <ol> <li>Direct registration of functions</li> <li>Registration of pre-created Tool instances</li> <li>As a decorator for class methods</li> </ol> <p>The unified registration supports scope control and can be used both as a method and decorator.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>Union[Callable[..., Any], Tool]</code> <p>Function to register as a tool or a pre-created Tool instance</p> required <code>name</code> <code>Optional[str]</code> <p>Optional custom name for the tool</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Optional description of the tool's functionality</p> <code>None</code> <code>scopes</code> <code>Union[List[str], str]</code> <p>List of scopes where the tool can be used, or a single     scope string. Valid scopes are:     \"think\", \"act\", \"observe\", \"synthesize\", \"all\"     Defaults to \"all\"</p> <code>'all'</code> <p>Returns:</p> Name Type Description <code>Tool</code> <code>Tool</code> <p>The registered Tool instance</p> <p>Raises:</p> Type Description <code>ToolError</code> <p>If tool validation or registration fails</p> <code>ValueError</code> <p>If scopes are invalid or tool is already registered</p> Example <p>Direct function registration: <pre><code>def add(x: int, y: int) -&gt; int:\n    return x + y\n\ntool = agent.register_tool(\n    add,\n    name=\"Calculator\",\n    description=\"Adds numbers\",\n    scopes=[\"think\", \"act\"]\n)\n</code></pre></p> <p>Register pre-created Tool: <pre><code>my_tool = Tool.create(multiply, name=\"Multiplier\")\nagent.register_tool(my_tool, scopes=\"all\")\n</code></pre></p> <p>As a decorator: <pre><code>class MyAgent(Agent):\n    @register_tool(\n        name=\"TextProcessor\",\n        description=\"Processes text\",\n        scopes=[\"act\", \"synthesize\"]\n    )\n    def process_text(text: str) -&gt; str:\n        return text.upper()\n</code></pre></p> <p>Register with specific scopes: <pre><code>agent.register_tool(\n    utility_function,\n    name=\"Utility\",\n    description=\"Utility function\",\n    scopes=[\"think\", \"observe\"]\n)\n</code></pre></p> Source code in <code>clientai/agent/core/agent.py</code> <pre><code>def register_tool(\n    self,\n    tool: Union[Callable[..., Any], Tool],\n    *,\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n    scopes: Union[List[str], str] = \"all\",\n) -&gt; Tool:\n    \"\"\"\n    Register a tool with the agent for use in specified workflow scopes.\n\n    Provides a flexible way to register tools with\n    the agent through multiple methods:\n\n    1. Direct registration of functions\n    2. Registration of pre-created Tool instances\n    3. As a decorator for class methods\n\n    The unified registration supports scope control and can be used both\n    as a method and decorator.\n\n    Args:\n        tool: Function to register as a tool or a pre-created Tool instance\n        name: Optional custom name for the tool\n        description: Optional description of the tool's functionality\n        scopes: List of scopes where the tool can be used, or a single\n                scope string. Valid scopes are:\n                \"think\", \"act\", \"observe\", \"synthesize\", \"all\"\n                Defaults to \"all\"\n\n    Returns:\n        Tool: The registered Tool instance\n\n    Raises:\n        ToolError: If tool validation or registration fails\n        ValueError: If scopes are invalid or tool is already registered\n\n    Example:\n        Direct function registration:\n        ```python\n        def add(x: int, y: int) -&gt; int:\n            return x + y\n\n        tool = agent.register_tool(\n            add,\n            name=\"Calculator\",\n            description=\"Adds numbers\",\n            scopes=[\"think\", \"act\"]\n        )\n        ```\n\n        Register pre-created Tool:\n        ```python\n        my_tool = Tool.create(multiply, name=\"Multiplier\")\n        agent.register_tool(my_tool, scopes=\"all\")\n        ```\n\n        As a decorator:\n        ```python\n        class MyAgent(Agent):\n            @register_tool(\n                name=\"TextProcessor\",\n                description=\"Processes text\",\n                scopes=[\"act\", \"synthesize\"]\n            )\n            def process_text(text: str) -&gt; str:\n                return text.upper()\n        ```\n\n        Register with specific scopes:\n        ```python\n        agent.register_tool(\n            utility_function,\n            name=\"Utility\",\n            description=\"Utility function\",\n            scopes=[\"think\", \"observe\"]\n        )\n        ```\n    \"\"\"\n    try:\n        if isinstance(scopes, str):\n            scopes = [scopes]\n\n        try:\n            tool_scopes = frozenset(ToolScope.from_str(s) for s in scopes)\n        except ValueError as e:\n            raise ValueError(f\"Invalid tool scope: {str(e)}\")\n\n        if isinstance(tool, Tool):\n            tool_instance = tool\n        else:\n            try:\n                tool_instance = Tool.create(\n                    func=tool,\n                    name=name,\n                    description=description,\n                )\n            except ValueError as e:\n                raise ValueError(f\"Invalid tool function: {str(e)}\")\n\n        tool_config = ToolConfig(\n            tool=tool_instance,\n            scopes=tool_scopes,\n            name=tool_instance.name,\n            description=tool_instance.description,\n        )\n\n        try:\n            self.tool_registry.register(tool_config)\n        except ValueError as e:\n            raise ValueError(f\"Tool registration failed: {str(e)}\")\n\n        return tool_instance\n\n    except ValueError as e:\n        logger.error(f\"Tool validation error: {e}\")\n        raise ToolError(str(e)) from e\n    except Exception as e:\n        logger.error(f\"Failed to register tool: {e}\")\n        raise ToolError(f\"Failed to register tool: {str(e)}\") from e\n</code></pre>"},{"location":"api/agent/core/agent/#clientai.agent.core.Agent.register_tool_decorator","title":"<code>register_tool_decorator(*, name=None, description=None, scopes='all')</code>","text":"<p>Create a decorator for registering tools with the agent.</p> <p>A decorator-based approach for registering tools, providing a clean way to integrate tool registration with method definitions.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Optional custom name for the tool</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Optional description of the tool's functionality</p> <code>None</code> <code>scopes</code> <code>Union[List[str], str]</code> <p>List of scopes or single scope where tool can be used. Valid scopes: \"think\", \"act\", \"observe\", \"synthesize\", \"all\"</p> <code>'all'</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[[Callable[..., Any]], Tool]</code> <p>A decorator function that registers       the decorated method as a tool</p> Example <p>Basic tool registration with scopes: <pre><code>class MyAgent(Agent):\n    @register_tool_decorator(\n        name=\"Calculator\",\n        description=\"Adds two numbers\",\n        scopes=[\"think\", \"act\"]\n    )\n    def add_numbers(self, x: int, y: int) -&gt; int:\n        return x + y\n\n    @register_tool_decorator(scopes=\"observe\")\n    def get_data(self, query: str) -&gt; List[int]:\n        return [1, 2, 3]  # Example data\n</code></pre></p> <p>Multiple tools with different scopes: <pre><code>class AnalysisAgent(Agent):\n    @register_tool_decorator(\n        name=\"TextAnalyzer\",\n        scopes=[\"think\"]\n    )\n    def analyze_text(self, text: str) -&gt; dict:\n        return {\"words\": len(text.split())}\n\n    @register_tool_decorator(\n        name=\"DataFormatter\",\n        scopes=[\"synthesize\"]\n    )\n    def format_data(self, data: dict) -&gt; str:\n        return json.dumps(data, indent=2)\n</code></pre></p> Notes <ul> <li>Decorated methods become available tools</li> <li>Tool name defaults to method name if not provided</li> <li>Description defaults to method docstring</li> <li>Tools are registered during agent initialization</li> </ul> Source code in <code>clientai/agent/core/agent.py</code> <pre><code>def register_tool_decorator(\n    self,\n    *,\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n    scopes: Union[List[str], str] = \"all\",\n) -&gt; Callable[[Callable[..., Any]], Tool]:\n    \"\"\"Create a decorator for registering tools with the agent.\n\n    A decorator-based approach for registering tools,\n    providing a clean way to integrate tool registration\n    with method definitions.\n\n    Args:\n        name: Optional custom name for the tool\n        description: Optional description of the tool's functionality\n        scopes: List of scopes or single scope where tool can be used.\n            Valid scopes: \"think\", \"act\", \"observe\", \"synthesize\", \"all\"\n\n    Returns:\n        Callable: A decorator function that registers\n                  the decorated method as a tool\n\n    Example:\n        Basic tool registration with scopes:\n        ```python\n        class MyAgent(Agent):\n            @register_tool_decorator(\n                name=\"Calculator\",\n                description=\"Adds two numbers\",\n                scopes=[\"think\", \"act\"]\n            )\n            def add_numbers(self, x: int, y: int) -&gt; int:\n                return x + y\n\n            @register_tool_decorator(scopes=\"observe\")\n            def get_data(self, query: str) -&gt; List[int]:\n                return [1, 2, 3]  # Example data\n        ```\n\n        Multiple tools with different scopes:\n        ```python\n        class AnalysisAgent(Agent):\n            @register_tool_decorator(\n                name=\"TextAnalyzer\",\n                scopes=[\"think\"]\n            )\n            def analyze_text(self, text: str) -&gt; dict:\n                return {\"words\": len(text.split())}\n\n            @register_tool_decorator(\n                name=\"DataFormatter\",\n                scopes=[\"synthesize\"]\n            )\n            def format_data(self, data: dict) -&gt; str:\n                return json.dumps(data, indent=2)\n        ```\n\n    Notes:\n        - Decorated methods become available tools\n        - Tool name defaults to method name if not provided\n        - Description defaults to method docstring\n        - Tools are registered during agent initialization\n    \"\"\"\n\n    def decorator(func: Callable[..., Any]) -&gt; Tool:\n        return self.register_tool(\n            func, name=name, description=description, scopes=scopes\n        )\n\n    return decorator\n</code></pre>"},{"location":"api/agent/core/agent/#clientai.agent.core.Agent.reset","title":"<code>reset()</code>","text":"<p>Perform a complete reset of the agent.</p> <p>Resets all state including context, workflow state, and iteration counters.</p> Example <pre><code># Complete reset\nagent.reset()\nprint(len(agent.context.memory))  # Output: 0\nprint(len(agent.workflow_manager.get_steps()))  # Output: 0\n</code></pre> Notes <ul> <li>More comprehensive than reset_context</li> <li>Clears workflow state</li> <li>Maintains tool registration</li> <li>Returns agent to initial state</li> </ul> Source code in <code>clientai/agent/core/agent.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Perform a complete reset of the agent.\n\n    Resets all state including context,\n    workflow state, and iteration counters.\n\n    Example:\n        ```python\n        # Complete reset\n        agent.reset()\n        print(len(agent.context.memory))  # Output: 0\n        print(len(agent.workflow_manager.get_steps()))  # Output: 0\n        ```\n\n    Notes:\n        - More comprehensive than reset_context\n        - Clears workflow state\n        - Maintains tool registration\n        - Returns agent to initial state\n    \"\"\"\n    self.context.clear()\n    self.workflow_manager.reset()\n</code></pre>"},{"location":"api/agent/core/agent/#clientai.agent.core.Agent.reset_context","title":"<code>reset_context()</code>","text":"<p>Reset the agent's context, clearing all memory and state.</p> Example <pre><code># After processing\nprint(len(agent.context.memory))  # Output: 5\n\n# Reset context\nagent.reset_context()\nprint(len(agent.context.memory))  # Output: 0\nprint(agent.context.state)  # Output: {}\n</code></pre> Notes <ul> <li>Clears memory, state, and results</li> <li>Does not affect workflow or tool registration</li> <li>Resets iteration counter</li> </ul> Source code in <code>clientai/agent/core/agent.py</code> <pre><code>def reset_context(self) -&gt; None:\n    \"\"\"Reset the agent's context, clearing all memory and state.\n\n    Example:\n        ```python\n        # After processing\n        print(len(agent.context.memory))  # Output: 5\n\n        # Reset context\n        agent.reset_context()\n        print(len(agent.context.memory))  # Output: 0\n        print(agent.context.state)  # Output: {}\n        ```\n\n    Notes:\n        - Clears memory, state, and results\n        - Does not affect workflow or tool registration\n        - Resets iteration counter\n    \"\"\"\n    self.context.clear()\n</code></pre>"},{"location":"api/agent/core/agent/#clientai.agent.core.Agent.run","title":"<code>run(input_data, *, stream=None)</code>","text":"<p>Execute the agent's workflow with the provided input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The initial input to process</p> required <code>stream</code> <code>Optional[bool]</code> <p>Optional bool to override streaming configuration. If provided, overrides the last step's stream setting.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[str, Iterator[str]]</code> <p>Union[str, Iterator[str]]: Either: - Complete response string (streaming disabled) - Iterator of response chunks (streaming enabled)</p> <p>Raises:</p> Type Description <code>WorkflowError</code> <p>If workflow execution fails</p> <code>StepError</code> <p>If a required step fails</p> <code>ClientAIError</code> <p>If LLM interaction fails</p> Example <p>Basic execution: <pre><code># Without streaming\nresult = agent.run(\"Analyze this data\")\nprint(result)\n\n# With streaming\nfor chunk in agent.run(\"Process this\", stream=True):\n    print(chunk, end=\"\", flush=True)\n\n# Use step configuration\nresult = agent.run(\"Analyze this\")  # Uses last step's setting\n</code></pre></p> Notes <ul> <li>Streaming can be controlled by parameter or step configuration</li> <li>Workflow executes steps in defined order</li> <li>Context is updated after each step</li> <li>Tool selection occurs automatically if enabled</li> </ul> Source code in <code>clientai/agent/core/agent.py</code> <pre><code>def run(\n    self, input_data: Any, *, stream: Optional[bool] = None\n) -&gt; Union[str, Iterator[str]]:\n    \"\"\"Execute the agent's workflow with the provided input data.\n\n    Args:\n        input_data: The initial input to process\n        stream: Optional bool to override streaming configuration.\n            If provided, overrides the last step's stream setting.\n\n    Returns:\n        Union[str, Iterator[str]]: Either:\n            - Complete response string (streaming disabled)\n            - Iterator of response chunks (streaming enabled)\n\n    Raises:\n        WorkflowError: If workflow execution fails\n        StepError: If a required step fails\n        ClientAIError: If LLM interaction fails\n\n    Example:\n        Basic execution:\n        ```python\n        # Without streaming\n        result = agent.run(\"Analyze this data\")\n        print(result)\n\n        # With streaming\n        for chunk in agent.run(\"Process this\", stream=True):\n            print(chunk, end=\"\", flush=True)\n\n        # Use step configuration\n        result = agent.run(\"Analyze this\")  # Uses last step's setting\n        ```\n\n    Notes:\n        - Streaming can be controlled by parameter or step configuration\n        - Workflow executes steps in defined order\n        - Context is updated after each step\n        - Tool selection occurs automatically if enabled\n    \"\"\"\n    try:\n        self.context.set_input(input_data)\n\n        try:\n            result = self.workflow_manager.execute(\n                agent=self,\n                input_data=input_data,\n                engine=self.execution_engine,\n                stream_override=stream,\n            )\n        except Exception as e:\n            raise ValueError(f\"Workflow execution failed: {str(e)}\")\n\n        try:\n            return self._handle_streaming(result, stream)\n        except Exception as e:\n            raise ValueError(f\"Stream handling failed: {str(e)}\")\n\n    except ValueError as e:\n        logger.error(f\"Workflow execution error: {e}\")\n        raise WorkflowError(str(e)) from e\n    except Exception as e:\n        logger.error(f\"Unexpected error during workflow execution: {e}\")\n        raise WorkflowError(\n            f\"Unexpected error during workflow execution: {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api/agent/core/agent/#clientai.agent.core.Agent.use_tool","title":"<code>use_tool(name, *args, **kwargs)</code>","text":"<p>Execute a registered tool by its name.</p> <p>Allows direct tool execution with provided arguments, bypassing automatic tool selection.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the tool to execute</p> required <code>*args</code> <code>Any</code> <p>Positional arguments for the tool</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for the tool</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of the tool execution</p> <p>Raises:</p> Type Description <code>ToolError</code> <p>If tool execution fails or tool isn't found</p> Example <p>Direct tool execution: <pre><code># Execute a calculator tool\nresult = agent.use_tool(\"calculator\", x=5, y=3)\nprint(result)  # Output: 8\n\n# Execute a text processor\nresult = agent.use_tool(\n    \"text_processor\",\n    text=\"hello\",\n    uppercase=True\n)\nprint(result)  # Output: \"HELLO\"\n</code></pre></p> Notes <ul> <li>Tool must be registered before use</li> <li>Arguments must match tool's signature</li> <li>Does not affect agent's tool usage history</li> </ul> Source code in <code>clientai/agent/core/agent.py</code> <pre><code>def use_tool(self, name: str, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Execute a registered tool by its name.\n\n    Allows direct tool execution with provided arguments, bypassing\n    automatic tool selection.\n\n    Args:\n        name: Name of the tool to execute\n        *args: Positional arguments for the tool\n        **kwargs: Keyword arguments for the tool\n\n    Returns:\n        Any: Result of the tool execution\n\n    Raises:\n        ToolError: If tool execution fails or tool isn't found\n\n    Example:\n        Direct tool execution:\n        ```python\n        # Execute a calculator tool\n        result = agent.use_tool(\"calculator\", x=5, y=3)\n        print(result)  # Output: 8\n\n        # Execute a text processor\n        result = agent.use_tool(\n            \"text_processor\",\n            text=\"hello\",\n            uppercase=True\n        )\n        print(result)  # Output: \"HELLO\"\n        ```\n\n    Notes:\n        - Tool must be registered before use\n        - Arguments must match tool's signature\n        - Does not affect agent's tool usage history\n    \"\"\"\n    try:\n        tool = self.tool_registry.get(name)\n        if not tool:\n            raise ValueError(f\"Tool '{name}' not found in registry\")\n\n        try:\n            return tool(*args, **kwargs)\n        except TypeError as e:\n            raise ValueError(\n                f\"Invalid arguments for tool '{name}': {str(e)}\"\n            )\n        except Exception as e:\n            raise ValueError(f\"Tool execution failed: {str(e)}\")\n\n    except ValueError as e:\n        logger.error(f\"Tool usage error: {e}\")\n        raise ToolError(str(e)) from e\n    except Exception as e:\n        logger.error(f\"Unexpected error using tool '{name}': {e}\")\n        raise ToolError(\n            f\"Unexpected error using tool '{name}': {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api/agent/core/context/","title":"AgentContext Class API Reference","text":"<p>The <code>AgentContext</code> class maintains state, memory, and results across agent workflow steps. It provides a structured way to store and access data during agent execution.</p>"},{"location":"api/agent/core/context/#class-definition","title":"Class Definition","text":"<p>Maintains state, memory, and execution results for an agent.</p> <p>This class serves as a central repository for maintaining agent state across workflow executions. It stores step results, tracks conversation history, maintains iteration counts, and stores arbitrary state data needed during workflow execution.</p> <p>Attributes:</p> Name Type Description <code>memory</code> <code>List[Dict[str, str]]</code> <p>List of dictionaries storing step-by-step execution memory.</p> <code>state</code> <code>Dict[str, Any]</code> <p>Dictionary storing arbitrary state information for the agent.</p> <code>last_results</code> <code>Dict[str, Any]</code> <p>Dictionary mapping steps to their most recent results.</p> <code>current_input</code> <code>Any</code> <p>The current input being processed by the workflow.</p> <code>original_input</code> <code>Any</code> <p>Original input stored separately from current_input.</p> <code>conversation_history</code> <code>List[Dict[str, Any]]</code> <p>List of previous interactions with their results.</p> <code>max_history_size</code> <code>int</code> <p>Maximum number of previous interactions to maintain.</p> <code>iteration</code> <code>int</code> <p>Counter tracking the number of workflow iterations.</p> Example <p>Initialize and manipulate the context: <pre><code>context = AgentContext()\n\n# Set new input\ncontext.set_input(\"What is Python?\")\n\n# Store a step result\ncontext.set_step_result(\"analyze\", \"Python is a programming language\")\n\n# Access conversation history\nhistory = context.get_recent_history(n=2)\nprint(history)  # Shows last 2 interactions\n\n# Reset the context but keep history\ncontext.clear()\n</code></pre></p> Source code in <code>clientai/agent/core/context.py</code> <pre><code>@dataclass\nclass AgentContext:\n    \"\"\"Maintains state, memory, and execution results for an agent.\n\n    This class serves as a central repository for maintaining agent\n    state across workflow executions. It stores step results, tracks\n    conversation history, maintains iteration counts, and stores\n    arbitrary state data needed during workflow execution.\n\n    Attributes:\n        memory: List of dictionaries storing step-by-step execution memory.\n        state: Dictionary storing arbitrary state information for the agent.\n        last_results: Dictionary mapping steps to their most recent results.\n        current_input: The current input being processed by the workflow.\n        original_input: Original input stored separately from current_input.\n        conversation_history: List of previous interactions with their results.\n        max_history_size: Maximum number of previous interactions to maintain.\n        iteration: Counter tracking the number of workflow iterations.\n\n    Example:\n        Initialize and manipulate the context:\n        ```python\n        context = AgentContext()\n\n        # Set new input\n        context.set_input(\"What is Python?\")\n\n        # Store a step result\n        context.set_step_result(\"analyze\", \"Python is a programming language\")\n\n        # Access conversation history\n        history = context.get_recent_history(n=2)\n        print(history)  # Shows last 2 interactions\n\n        # Reset the context but keep history\n        context.clear()\n        ```\n    \"\"\"\n\n    memory: List[Dict[str, str]] = field(default_factory=list)\n    state: Dict[str, Any] = field(default_factory=dict)\n    last_results: Dict[str, Any] = field(default_factory=dict)\n    current_input: Any = None\n    original_input: Any = None\n    conversation_history: List[Dict[str, Any]] = field(default_factory=list)\n    max_history_size: int = 10\n    iteration: int = 0\n\n    def set_input(self, input_data: Any) -&gt; None:\n        \"\"\"Set new input and save previous interaction to history.\n\n        Stores the current interaction in history (if exists) and sets up\n        for a new interaction. Maintains maximum history size by removing\n        oldest interactions when limit is reached.\n\n        Args:\n            input_data: The new input to process.\n\n        Example:\n            ```python\n            context = AgentContext()\n            context.set_input(\"What is Python?\")\n            context.set_step_result(\n                \"analyze\",\n                \"Python is a programming language\"\n            )\n            context.set_input(\n                \"How do I install Python?\"\n            )  # Previous interaction saved\n            ```\n        \"\"\"\n        if self.current_input is not None and self.last_results:\n            interaction = {\n                \"input\": self.original_input,\n                \"results\": self.last_results.copy(),\n                \"iteration\": self.iteration,\n                \"timestamp\": datetime.now().isoformat(),\n            }\n            self.conversation_history.append(interaction)\n            if len(self.conversation_history) &gt; self.max_history_size:\n                self.conversation_history = self.conversation_history[\n                    -self.max_history_size :\n                ]\n\n        self.current_input = input_data\n        self.original_input = input_data\n        self.last_results.clear()\n\n    def clear(self) -&gt; None:\n        \"\"\"Reset the current interaction but preserve conversation history.\n\n        Clears current state, memory, and results while maintaining the\n        conversation history.\n\n        Example:\n            ```python\n            context = AgentContext()\n            context.state[\"key\"] = \"value\"\n            context.clear()\n            print(context.state)  # Output: {}\n            print(len(context.conversation_history))  # Preserved\n            ```\n        \"\"\"\n        self.memory.clear()\n        self.state.clear()\n        self.last_results.clear()\n        self.current_input = None\n        self.original_input = None\n        self.iteration = 0\n\n    def clear_all(self) -&gt; None:\n        \"\"\"Reset everything including conversation history.\n\n        Performs a complete reset of the context, including all history.\n\n        Example:\n            ```python\n            context = AgentContext()\n            context.set_input(\"Test\")\n            context.clear_all()\n            print(len(context.conversation_history))  # Output: 0\n            ```\n        \"\"\"\n        self.clear()\n        self.conversation_history.clear()\n\n    def get_step_result(self, step_name: str) -&gt; Any:\n        \"\"\"Retrieve the result of a specific workflow step.\n\n        Args:\n            step_name: Name of the step whose result should be retrieved.\n\n        Returns:\n            Any: The stored result for the specified step,\n                 or None if no result exists.\n\n        Example:\n            ```python\n            context = AgentContext()\n            context.set_step_result(\"analyze\", \"Result\")\n            print(context.get_step_result(\"analyze\"))  # Output: \"Result\"\n            ```\n        \"\"\"\n        return self.last_results.get(step_name)\n\n    def set_step_result(self, step_name: str, result: Any) -&gt; None:\n        \"\"\"Store the result of a workflow step.\n\n        Args:\n            step_name: Name of the step whose result is being stored.\n            result: The result value to store.\n\n        Example:\n            ```python\n            context = AgentContext()\n            context.set_step_result(\"analyze\", \"Python analysis\")\n            print(context.get_step_result(\"analyze\"))\n            ```\n        \"\"\"\n        self.last_results[step_name] = result\n\n    def set_max_history_size(self, size: int) -&gt; None:\n        \"\"\"Update the maximum history size and trim if necessary.\n\n        Args:\n            size: New maximum number of interactions to maintain.\n\n        Raises:\n            ValueError: If size is negative.\n\n        Example:\n            ```python\n            context = AgentContext()\n            context.set_max_history_size(5)  # Only keep last 5 interactions\n            ```\n        \"\"\"\n        if size &lt; 0:\n            raise ValueError(\"History size must be non-negative\")\n\n        self.max_history_size = size\n        if len(self.conversation_history) &gt; size:\n            self.conversation_history = self.conversation_history[-size:]\n\n    def get_recent_history(\n        self, n: Optional[int] = None, raw: bool = False\n    ) -&gt; Union[str, List[Dict[str, Any]]]:\n        \"\"\"Get recent interactions with formatted context for LLM.\n\n        Retrieves recent interactions either as formatted text for LLM context\n        or as raw data structures for programmatic use.\n\n        Args:\n            n: Number of recent interactions to retrieve.\n                Defaults to all within max_size.\n            raw: If True, returns raw data structure.\n                 If False, returns formatted string.\n\n        Returns:\n            Either a formatted string of conversation history suitable for LLM\n            context, or the raw list of interaction dictionaries.\n\n        Example:\n            ```python\n            context = AgentContext()\n\n            # Get formatted history for LLM\n            history = context.get_recent_history(n=2)\n\n            # Get raw data for processing\n            raw_history = context.get_recent_history(n=2, raw=True)\n            ```\n        \"\"\"\n        history = (\n            self.conversation_history[-n:]\n            if n and n &lt; len(self.conversation_history)\n            else self.conversation_history\n        )\n\n        if raw:\n            return history\n\n        if not history:\n            if self.original_input is not None:\n                return \"No previous interactions. This is the first query.\"\n            return \"No interactions available.\"\n\n        formatted_history = []\n        for i, interaction in enumerate(history, 1):\n            timestamp = datetime.fromisoformat(\n                interaction[\"timestamp\"]\n            ).strftime(\"%Y-%m-%d %H:%M:%S\")\n            formatted_interaction = (\n                f\"Interaction {i}:\\n\"\n                f\"Time: {timestamp}\\n\"\n                f\"Input: {interaction['input']}\\n\"\n                f\"Results:\"\n            )\n\n            for step_name, result in interaction[\"results\"].items():\n                formatted_interaction += f\"\\n- {step_name}: {result}\"\n\n            formatted_history.append(formatted_interaction)\n\n        history_text = \"Previous Interactions:\\n\" + \"\\n\\n\".join(\n            formatted_history\n        )\n\n        if self.original_input is not None:\n            history_text += \"\\n\\nNow handling the current query.\"\n\n        return history_text\n\n    def increment_iteration(self) -&gt; int:\n        \"\"\"Increment and return the workflow iteration counter.\n\n        Returns:\n            int: The new iteration count after incrementing.\n\n        Example:\n            ```python\n            context = AgentContext()\n            print(context.increment_iteration())  # Output: 1\n            print(context.increment_iteration())  # Output: 2\n            ```\n        \"\"\"\n        self.iteration += 1\n        return self.iteration\n</code></pre>"},{"location":"api/agent/core/context/#clientai.agent.core.AgentContext.clear","title":"<code>clear()</code>","text":"<p>Reset the current interaction but preserve conversation history.</p> <p>Clears current state, memory, and results while maintaining the conversation history.</p> Example <pre><code>context = AgentContext()\ncontext.state[\"key\"] = \"value\"\ncontext.clear()\nprint(context.state)  # Output: {}\nprint(len(context.conversation_history))  # Preserved\n</code></pre> Source code in <code>clientai/agent/core/context.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Reset the current interaction but preserve conversation history.\n\n    Clears current state, memory, and results while maintaining the\n    conversation history.\n\n    Example:\n        ```python\n        context = AgentContext()\n        context.state[\"key\"] = \"value\"\n        context.clear()\n        print(context.state)  # Output: {}\n        print(len(context.conversation_history))  # Preserved\n        ```\n    \"\"\"\n    self.memory.clear()\n    self.state.clear()\n    self.last_results.clear()\n    self.current_input = None\n    self.original_input = None\n    self.iteration = 0\n</code></pre>"},{"location":"api/agent/core/context/#clientai.agent.core.AgentContext.clear_all","title":"<code>clear_all()</code>","text":"<p>Reset everything including conversation history.</p> <p>Performs a complete reset of the context, including all history.</p> Example <pre><code>context = AgentContext()\ncontext.set_input(\"Test\")\ncontext.clear_all()\nprint(len(context.conversation_history))  # Output: 0\n</code></pre> Source code in <code>clientai/agent/core/context.py</code> <pre><code>def clear_all(self) -&gt; None:\n    \"\"\"Reset everything including conversation history.\n\n    Performs a complete reset of the context, including all history.\n\n    Example:\n        ```python\n        context = AgentContext()\n        context.set_input(\"Test\")\n        context.clear_all()\n        print(len(context.conversation_history))  # Output: 0\n        ```\n    \"\"\"\n    self.clear()\n    self.conversation_history.clear()\n</code></pre>"},{"location":"api/agent/core/context/#clientai.agent.core.AgentContext.get_recent_history","title":"<code>get_recent_history(n=None, raw=False)</code>","text":"<p>Get recent interactions with formatted context for LLM.</p> <p>Retrieves recent interactions either as formatted text for LLM context or as raw data structures for programmatic use.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Optional[int]</code> <p>Number of recent interactions to retrieve. Defaults to all within max_size.</p> <code>None</code> <code>raw</code> <code>bool</code> <p>If True, returns raw data structure.  If False, returns formatted string.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[str, List[Dict[str, Any]]]</code> <p>Either a formatted string of conversation history suitable for LLM</p> <code>Union[str, List[Dict[str, Any]]]</code> <p>context, or the raw list of interaction dictionaries.</p> Example <pre><code>context = AgentContext()\n\n# Get formatted history for LLM\nhistory = context.get_recent_history(n=2)\n\n# Get raw data for processing\nraw_history = context.get_recent_history(n=2, raw=True)\n</code></pre> Source code in <code>clientai/agent/core/context.py</code> <pre><code>def get_recent_history(\n    self, n: Optional[int] = None, raw: bool = False\n) -&gt; Union[str, List[Dict[str, Any]]]:\n    \"\"\"Get recent interactions with formatted context for LLM.\n\n    Retrieves recent interactions either as formatted text for LLM context\n    or as raw data structures for programmatic use.\n\n    Args:\n        n: Number of recent interactions to retrieve.\n            Defaults to all within max_size.\n        raw: If True, returns raw data structure.\n             If False, returns formatted string.\n\n    Returns:\n        Either a formatted string of conversation history suitable for LLM\n        context, or the raw list of interaction dictionaries.\n\n    Example:\n        ```python\n        context = AgentContext()\n\n        # Get formatted history for LLM\n        history = context.get_recent_history(n=2)\n\n        # Get raw data for processing\n        raw_history = context.get_recent_history(n=2, raw=True)\n        ```\n    \"\"\"\n    history = (\n        self.conversation_history[-n:]\n        if n and n &lt; len(self.conversation_history)\n        else self.conversation_history\n    )\n\n    if raw:\n        return history\n\n    if not history:\n        if self.original_input is not None:\n            return \"No previous interactions. This is the first query.\"\n        return \"No interactions available.\"\n\n    formatted_history = []\n    for i, interaction in enumerate(history, 1):\n        timestamp = datetime.fromisoformat(\n            interaction[\"timestamp\"]\n        ).strftime(\"%Y-%m-%d %H:%M:%S\")\n        formatted_interaction = (\n            f\"Interaction {i}:\\n\"\n            f\"Time: {timestamp}\\n\"\n            f\"Input: {interaction['input']}\\n\"\n            f\"Results:\"\n        )\n\n        for step_name, result in interaction[\"results\"].items():\n            formatted_interaction += f\"\\n- {step_name}: {result}\"\n\n        formatted_history.append(formatted_interaction)\n\n    history_text = \"Previous Interactions:\\n\" + \"\\n\\n\".join(\n        formatted_history\n    )\n\n    if self.original_input is not None:\n        history_text += \"\\n\\nNow handling the current query.\"\n\n    return history_text\n</code></pre>"},{"location":"api/agent/core/context/#clientai.agent.core.AgentContext.get_step_result","title":"<code>get_step_result(step_name)</code>","text":"<p>Retrieve the result of a specific workflow step.</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>Name of the step whose result should be retrieved.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The stored result for the specified step,  or None if no result exists.</p> Example <pre><code>context = AgentContext()\ncontext.set_step_result(\"analyze\", \"Result\")\nprint(context.get_step_result(\"analyze\"))  # Output: \"Result\"\n</code></pre> Source code in <code>clientai/agent/core/context.py</code> <pre><code>def get_step_result(self, step_name: str) -&gt; Any:\n    \"\"\"Retrieve the result of a specific workflow step.\n\n    Args:\n        step_name: Name of the step whose result should be retrieved.\n\n    Returns:\n        Any: The stored result for the specified step,\n             or None if no result exists.\n\n    Example:\n        ```python\n        context = AgentContext()\n        context.set_step_result(\"analyze\", \"Result\")\n        print(context.get_step_result(\"analyze\"))  # Output: \"Result\"\n        ```\n    \"\"\"\n    return self.last_results.get(step_name)\n</code></pre>"},{"location":"api/agent/core/context/#clientai.agent.core.AgentContext.increment_iteration","title":"<code>increment_iteration()</code>","text":"<p>Increment and return the workflow iteration counter.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The new iteration count after incrementing.</p> Example <pre><code>context = AgentContext()\nprint(context.increment_iteration())  # Output: 1\nprint(context.increment_iteration())  # Output: 2\n</code></pre> Source code in <code>clientai/agent/core/context.py</code> <pre><code>def increment_iteration(self) -&gt; int:\n    \"\"\"Increment and return the workflow iteration counter.\n\n    Returns:\n        int: The new iteration count after incrementing.\n\n    Example:\n        ```python\n        context = AgentContext()\n        print(context.increment_iteration())  # Output: 1\n        print(context.increment_iteration())  # Output: 2\n        ```\n    \"\"\"\n    self.iteration += 1\n    return self.iteration\n</code></pre>"},{"location":"api/agent/core/context/#clientai.agent.core.AgentContext.set_input","title":"<code>set_input(input_data)</code>","text":"<p>Set new input and save previous interaction to history.</p> <p>Stores the current interaction in history (if exists) and sets up for a new interaction. Maintains maximum history size by removing oldest interactions when limit is reached.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The new input to process.</p> required Example <pre><code>context = AgentContext()\ncontext.set_input(\"What is Python?\")\ncontext.set_step_result(\n    \"analyze\",\n    \"Python is a programming language\"\n)\ncontext.set_input(\n    \"How do I install Python?\"\n)  # Previous interaction saved\n</code></pre> Source code in <code>clientai/agent/core/context.py</code> <pre><code>def set_input(self, input_data: Any) -&gt; None:\n    \"\"\"Set new input and save previous interaction to history.\n\n    Stores the current interaction in history (if exists) and sets up\n    for a new interaction. Maintains maximum history size by removing\n    oldest interactions when limit is reached.\n\n    Args:\n        input_data: The new input to process.\n\n    Example:\n        ```python\n        context = AgentContext()\n        context.set_input(\"What is Python?\")\n        context.set_step_result(\n            \"analyze\",\n            \"Python is a programming language\"\n        )\n        context.set_input(\n            \"How do I install Python?\"\n        )  # Previous interaction saved\n        ```\n    \"\"\"\n    if self.current_input is not None and self.last_results:\n        interaction = {\n            \"input\": self.original_input,\n            \"results\": self.last_results.copy(),\n            \"iteration\": self.iteration,\n            \"timestamp\": datetime.now().isoformat(),\n        }\n        self.conversation_history.append(interaction)\n        if len(self.conversation_history) &gt; self.max_history_size:\n            self.conversation_history = self.conversation_history[\n                -self.max_history_size :\n            ]\n\n    self.current_input = input_data\n    self.original_input = input_data\n    self.last_results.clear()\n</code></pre>"},{"location":"api/agent/core/context/#clientai.agent.core.AgentContext.set_max_history_size","title":"<code>set_max_history_size(size)</code>","text":"<p>Update the maximum history size and trim if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>New maximum number of interactions to maintain.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If size is negative.</p> Example <pre><code>context = AgentContext()\ncontext.set_max_history_size(5)  # Only keep last 5 interactions\n</code></pre> Source code in <code>clientai/agent/core/context.py</code> <pre><code>def set_max_history_size(self, size: int) -&gt; None:\n    \"\"\"Update the maximum history size and trim if necessary.\n\n    Args:\n        size: New maximum number of interactions to maintain.\n\n    Raises:\n        ValueError: If size is negative.\n\n    Example:\n        ```python\n        context = AgentContext()\n        context.set_max_history_size(5)  # Only keep last 5 interactions\n        ```\n    \"\"\"\n    if size &lt; 0:\n        raise ValueError(\"History size must be non-negative\")\n\n    self.max_history_size = size\n    if len(self.conversation_history) &gt; size:\n        self.conversation_history = self.conversation_history[-size:]\n</code></pre>"},{"location":"api/agent/core/context/#clientai.agent.core.AgentContext.set_step_result","title":"<code>set_step_result(step_name, result)</code>","text":"<p>Store the result of a workflow step.</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>Name of the step whose result is being stored.</p> required <code>result</code> <code>Any</code> <p>The result value to store.</p> required Example <pre><code>context = AgentContext()\ncontext.set_step_result(\"analyze\", \"Python analysis\")\nprint(context.get_step_result(\"analyze\"))\n</code></pre> Source code in <code>clientai/agent/core/context.py</code> <pre><code>def set_step_result(self, step_name: str, result: Any) -&gt; None:\n    \"\"\"Store the result of a workflow step.\n\n    Args:\n        step_name: Name of the step whose result is being stored.\n        result: The result value to store.\n\n    Example:\n        ```python\n        context = AgentContext()\n        context.set_step_result(\"analyze\", \"Python analysis\")\n        print(context.get_step_result(\"analyze\"))\n        ```\n    \"\"\"\n    self.last_results[step_name] = result\n</code></pre>"},{"location":"api/agent/core/execution/","title":"StepExecutionEngine Class API Reference","text":"<p>The <code>StepExecutionEngine</code> class manages the execution of individual workflow steps, handling tool selection and LLM interactions.</p>"},{"location":"api/agent/core/execution/#class-definition","title":"Class Definition","text":"<p>Handles the execution of workflow steps with integrated tool selection and LLM interaction.</p> <p>Manages all aspects of step execution including tool selection, LLM interaction, and error handling. Provides configurable retry logic and streaming support.</p> <p>Attributes:</p> Name Type Description <code>_client</code> <p>The AI client for model interactions</p> <code>_default_model</code> <p>Default model configuration for steps</p> <code>_default_kwargs</code> <p>Default keyword arguments for model calls</p> <code>_default_tool_selection_config</code> <p>Default tool selection settings</p> <code>_default_tool_model</code> <p>Default model for tool selection</p> <code>_tool_selector</code> <p>Instance handling tool selection logic</p> Example <p>Basic execution engine setup: <pre><code>engine = StepExecutionEngine(\n    client=client,\n    default_model=ModelConfig(name=\"gpt-4\"),\n    default_kwargs={\"temperature\": 0.7}\n)\n\nresult = engine.execute_step(step, agent, \"input data\")\n</code></pre></p> <p>Configure tool selection: <pre><code>engine = StepExecutionEngine(\n    client=client,\n    default_model=\"gpt-4\",\n    default_kwargs={},\n    tool_selection_config=ToolSelectionConfig(\n        confidence_threshold=0.8,\n        max_tools_per_step=3\n    ),\n    tool_model=ModelConfig(name=\"llama-2\")\n)\n</code></pre></p> Notes <ul> <li>Manages automatic tool selection with confidence thresholds</li> <li>Supports separate models for workflow and tool selection</li> <li>Implements retry logic for LLM calls</li> <li>Handles streaming configurations</li> <li>Provides comprehensive error handling</li> </ul> Source code in <code>clientai/agent/core/execution.py</code> <pre><code>class StepExecutionEngine:\n    \"\"\"Handles the execution of workflow steps with\n    integrated tool selection and LLM interaction.\n\n    Manages all aspects of step execution including tool selection,\n    LLM interaction, and error handling. Provides configurable retry\n    logic and streaming support.\n\n    Attributes:\n        _client: The AI client for model interactions\n        _default_model: Default model configuration for steps\n        _default_kwargs: Default keyword arguments for model calls\n        _default_tool_selection_config: Default tool selection settings\n        _default_tool_model: Default model for tool selection\n        _tool_selector: Instance handling tool selection logic\n\n    Example:\n        Basic execution engine setup:\n        ```python\n        engine = StepExecutionEngine(\n            client=client,\n            default_model=ModelConfig(name=\"gpt-4\"),\n            default_kwargs={\"temperature\": 0.7}\n        )\n\n        result = engine.execute_step(step, agent, \"input data\")\n        ```\n\n        Configure tool selection:\n        ```python\n        engine = StepExecutionEngine(\n            client=client,\n            default_model=\"gpt-4\",\n            default_kwargs={},\n            tool_selection_config=ToolSelectionConfig(\n                confidence_threshold=0.8,\n                max_tools_per_step=3\n            ),\n            tool_model=ModelConfig(name=\"llama-2\")\n        )\n        ```\n\n    Notes:\n        - Manages automatic tool selection with confidence thresholds\n        - Supports separate models for workflow and tool selection\n        - Implements retry logic for LLM calls\n        - Handles streaming configurations\n        - Provides comprehensive error handling\n    \"\"\"\n\n    def __init__(\n        self,\n        client: ClientAI,\n        default_model: Union[str, ModelConfig],\n        default_kwargs: Dict[str, Any],\n        tool_selection_config: Optional[ToolSelectionConfig] = None,\n        tool_model: Optional[Union[str, ModelConfig]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the execution engine with specified configurations.\n\n        Args:\n            client: The AI client for model interactions\n            default_model: Default model configuration\n                           (string name or ModelConfig)\n            default_kwargs: Default parameters for model calls\n            tool_selection_config: Configuration for tool selection behavior\n            tool_model: Model to use for tool selection\n                        (default default_model)\n\n        Example:\n            ```python\n            engine = StepExecutionEngine(\n                client=my_client,\n                default_model=\"gpt-4\",\n                default_kwargs={\"temperature\": 0.7},\n                tool_selection_config=ToolSelectionConfig(\n                    confidence_threshold=0.8\n                )\n            )\n            ```\n\n        Raises:\n            StepError: If initialization fails or configuration is invalid\n        \"\"\"\n        try:\n            if not client:\n                raise ValueError(\"Client must be specified\")\n            if not default_model:\n                raise ValueError(\"Default model must be specified\")\n\n            self._client = client\n            self._default_model = default_model\n            self._default_kwargs = default_kwargs\n            self._current_agent: Optional[Any] = None\n            self._default_tool_selection_config = (\n                tool_selection_config or ToolSelectionConfig()\n            )\n\n            self._default_tool_model = self._create_tool_model_config(\n                tool_model if tool_model is not None else default_model\n            )\n\n            logger.debug(\n                f\"Initialized StepExecutionEngine with model: {default_model}\"\n            )\n\n        except ValueError as e:\n            logger.error(f\"Initialization error: {e}\")\n            raise StepError(str(e)) from e\n        except Exception as e:\n            logger.error(f\"Unexpected initialization error: {e}\")\n            raise StepError(\n                f\"Unexpected initialization error: {str(e)}\"\n            ) from e\n\n        except ValueError as e:\n            logger.error(f\"Initialization error: {e}\")\n            raise StepError(str(e)) from e\n        except Exception as e:\n            logger.error(f\"Unexpected initialization error: {e}\")\n            raise StepError(\n                f\"Unexpected initialization error: {str(e)}\"\n            ) from e\n\n    def _create_tool_model_config(\n        self, model: Union[str, ModelConfig]\n    ) -&gt; ModelConfig:\n        \"\"\"Create a ModelConfig instance for tool selection.\n\n        Args:\n            model: Model name or configuration\n\n        Returns:\n            Configured ModelConfig for tool selection\n\n        Raises:\n            StepError: If configuration is invalid\n        \"\"\"\n        try:\n            if isinstance(model, str):\n                return ModelConfig(\n                    name=model,\n                    temperature=0.0,\n                    json_output=True,\n                )\n            elif isinstance(model, ModelConfig):\n                return model.merge(\n                    temperature=0.0,\n                    json_output=True,\n                )\n            else:\n                raise ValueError(\n                    f\"Invalid model type: {type(model)}. \"\n                    \"Must be string or ModelConfig\"\n                )\n        except Exception as e:\n            logger.error(f\"Error creating tool model config: {e}\")\n            raise StepError(f\"Invalid tool model configuration: {str(e)}\")\n\n    def _get_effective_tool_model(self, step: Step) -&gt; ModelConfig:\n        \"\"\"Get the model to use for tool selection based on priority order.\n\n        Args:\n            step: Step being executed\n\n        Returns:\n            ModelConfig for tool selection\n\n        Raises:\n            StepError: If model configuration fails\n        \"\"\"\n        try:\n            if step.tool_model is not None:\n                return self._create_tool_model_config(step.tool_model)\n\n            return self._default_tool_model\n\n        except Exception as e:\n            logger.error(f\"Error determining tool model: {e}\")\n            raise StepError(f\"Failed to determine tool model: {str(e)}\")\n\n    def _get_tool_selection_config(self, step: Step) -&gt; ToolSelectionConfig:\n        \"\"\"\n        Get the effective tool selection configuration for a step.\n\n        Determines the tool selection configuration to use by checking for\n        step-specific settings and falling back to defaults if needed.\n\n        Args:\n            step: The step being executed\n\n        Returns:\n            The tool selection configuration to use for this step\n\n        Raises:\n            StepError: If configuration access fails\n\n        Example:\n            ```python\n            config = engine._get_tool_selection_config(step)\n            print(config.confidence_threshold)  # Output: 0.8\n            ```\n        \"\"\"\n        try:\n            step_config = getattr(step, \"tool_selection_config\", None)\n            return step_config or self._default_tool_selection_config\n        except AttributeError as e:\n            logger.error(f\"Invalid step configuration: {e}\")\n            raise StepError(f\"Invalid step configuration: {str(e)}\") from e\n        except Exception as e:\n            logger.error(f\"Error accessing tool selection config: {e}\")\n            raise StepError(\n                f\"Error accessing tool selection config: {str(e)}\"\n            ) from e\n\n    def _get_tool_model(self, step: Step) -&gt; Union[str, ModelConfig]:\n        \"\"\"\n        Get the effective model to use for tool selection in a step.\n\n        Determines which model should be used for tool selection by checking\n        step-specific settings and falling back to defaults if needed.\n\n        Args:\n            step: The step being executed\n\n        Returns:\n            The model configuration to use for tool selection\n\n        Raises:\n            StepError: If model configuration access fails\n\n        Example:\n            ```python\n            model = engine._get_tool_model(step)\n            print(model.name if isinstance(model, ModelConfig) else model)\n            ```\n        \"\"\"\n        try:\n            step_model = getattr(step, \"tool_model\", None)\n            return step_model or self._default_tool_model\n        except AttributeError as e:\n            logger.error(f\"Invalid step model configuration: {e}\")\n            raise StepError(\n                f\"Invalid step model configuration: {str(e)}\"\n            ) from e\n        except Exception as e:\n            logger.error(f\"Error accessing tool model: {e}\")\n            raise StepError(f\"Error accessing tool model: {str(e)}\") from e\n\n    def _build_prompt(\n        self, step: Step, agent: Any, *args: Any, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"Build the prompt for a step, including tool selection if enabled.\n\n        Args:\n            step: Step being executed\n            agent: Agent instance\n            *args: Step arguments\n            **kwargs: Step keyword arguments\n\n        Returns:\n            Complete prompt string\n\n        Raises:\n            StepError: If prompt building fails\n            ToolError: If tool execution fails\n        \"\"\"\n        logger.debug(f\"Building prompt for step '{step.name}'\")\n        logger.debug(f\"Step use_tools setting: {step.use_tools}\")\n\n        try:\n            result = step.func(agent, *args, **kwargs)\n            if not isinstance(result, str):\n                raise ValueError(\n                    f\"Step function must return str, got {type(result)}\"\n                )\n            prompt = result\n\n            if step.use_tools:\n                try:\n                    prompt = self._handle_tool_execution(step, agent, prompt)\n                except Exception as e:\n                    raise ToolError(f\"Tool execution failed: {str(e)}\") from e\n\n            logger.debug(f\"Final prompt: {prompt}\")\n            return prompt\n\n        except ValueError as e:\n            logger.error(f\"Prompt building error: {e}\")\n            raise StepError(str(e)) from e\n        except Exception as e:\n            logger.error(f\"Failed to build prompt: {e}\")\n            raise StepError(f\"Failed to build prompt: {str(e)}\") from e\n\n    def _handle_tool_execution(\n        self, step: Step, agent: Any, base_prompt: str\n    ) -&gt; str:\n        \"\"\"Handle tool selection and execution for a step.\n\n        Args:\n            step: The step being executed\n            agent: The agent instance\n            base_prompt: Initial prompt before tool execution\n\n        Returns:\n            Updated prompt including tool execution results\n\n        Raises:\n            ToolError: If tool selection or execution fails\n\n        Example:\n            ```python\n            prompt = engine._handle_tool_execution(\n                step=analyze_step,\n                agent=agent,\n                base_prompt=\"Analyze the following data...\"\n            )\n            # Returns prompt enhanced with tool execution results\n            ```\n        \"\"\"\n        logger.debug(\"Tool usage is enabled for this step\")\n        available_tools = agent.get_tools(step.step_type.name.lower())\n        logger.debug(f\"Found {len(available_tools)} available tools\")\n\n        if not available_tools:\n            logger.debug(\"No tools available for this step\")\n            return base_prompt\n\n        try:\n            tool_model = self._get_effective_tool_model(step)\n            logger.debug(f\"Using tool model: {tool_model.name}\")\n\n            tool_selector = ToolSelector(\n                model_config=tool_model,\n                config=self._get_tool_selection_config(step),\n            )\n\n            decisions = tool_selector.select_tools(\n                task=base_prompt,\n                tools=available_tools,\n                context=agent.context.state,\n                client=self._client,\n            )\n\n            logger.debug(f\"Tool selector returned {len(decisions)} decisions\")\n\n            if not decisions:\n                return base_prompt\n\n            tools_by_name = {t.name: t for t in available_tools}\n            updated_decisions = tool_selector.execute_tool_decisions(\n                decisions=decisions, tools=tools_by_name\n            )\n\n            object.__setattr__(step, \"tool_decisions\", updated_decisions)\n\n            prompt = base_prompt + \"\\n\\nTool Execution Results:\\n\"\n            for decision in updated_decisions:\n                prompt += self._format_tool_result(decision)\n\n            agent.context.state[\"last_tool_decisions\"] = [\n                self._create_decision_dict(d) for d in updated_decisions\n            ]\n\n            return prompt\n\n        except Exception as e:\n            logger.error(f\"Tool execution error: {e}\", exc_info=True)\n            raise ToolError(f\"Tool execution failed: {str(e)}\") from e\n\n    def _format_tool_result(self, decision: Any) -&gt; str:\n        \"\"\"Format a single tool execution decision into a string.\n\n        Args:\n            decision: Tool execution decision with results\n\n        Returns:\n            Formatted string representing the tool execution result\n\n        Example:\n            ```python\n            formatted = engine._format_tool_result(decision)\n            # Output format:\n            # CalculatorTool:\n            # Result: 42\n            # Confidence: 0.95\n            # Reasoning: Used for precise calculation\n            ```\n        \"\"\"\n        if decision.error:\n            result_line = f\"Error: {decision.error}\"\n        else:\n            result_line = f\"Result: {str(decision.result)}\"\n\n        return (\n            f\"\\n{decision.tool_name}:\"\n            f\"\\n{result_line}\"\n            f\"\\nConfidence: {decision.confidence}\"\n            f\"\\nReasoning: {decision.reasoning}\\n\"\n        )\n\n    def _create_decision_dict(self, decision: Any) -&gt; Dict[str, Any]:\n        \"\"\"Create a dictionary representation of a tool execution decision.\n\n        Args:\n            decision: Tool execution decision to convert\n\n        Returns:\n            Dictionary containing all decision information\n\n        Example:\n            ```python\n            decision_dict = engine._create_decision_dict(decision)\n            # Output structure:\n            # {\n            #     \"tool_name\": \"calculator\",\n            #     \"arguments\": {\"x\": 5, \"y\": 3},\n            #     \"result\": 8,\n            #     \"error\": None,\n            #     \"confidence\": 0.95,\n            #     \"reasoning\": \"Required for calculation\"\n            # }\n            ```\n        \"\"\"\n        return {\n            \"tool_name\": decision.tool_name,\n            \"arguments\": decision.arguments,\n            \"result\": decision.result,\n            \"error\": decision.error,\n            \"confidence\": decision.confidence,\n            \"reasoning\": decision.reasoning,\n        }\n\n    def _execute_single_call(\n        self,\n        step: Step,\n        prompt: str,\n        api_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; Union[str, Iterator[str]]:\n        \"\"\"Execute a single LLM API call with proper configuration.\n\n        Args:\n            step: The step being executed\n            prompt: The prompt to send to the LLM\n            api_kwargs: Optional dictionary of API call arguments\n\n        Returns:\n            Either a string or an iterator of strings for streaming responses\n\n        Raises:\n            ClientAIError: If the LLM call fails\n            StepError: If there's an unexpected error\n        \"\"\"\n        try:\n            if api_kwargs is None:\n                model_config = step.llm_config or self._default_model\n                api_kwargs = self._prepare_api_kwargs(model_config)\n\n            result = self._client.generate_text(\n                prompt,\n                model=self._get_model_name(\n                    step.llm_config or self._default_model\n                ),\n                **api_kwargs,\n            )\n\n            return cast(Union[str, Iterator[str]], result)\n\n        except ClientAIError:\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error during LLM call: {e}\")\n            raise StepError(\n                f\"Unexpected error during LLM call: {str(e)}\"\n            ) from e\n\n    def _prepare_api_kwargs(\n        self, model_config: Union[str, ModelConfig]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Prepare keyword arguments for the LLM API call.\n\n        Args:\n            model_config: The model configuration to use\n\n        Returns:\n            Dictionary of API keyword arguments\n\n        Raises:\n            StepError: If API argument preparation fails\n        \"\"\"\n        try:\n            return {\n                **self._default_kwargs,\n                **(\n                    model_config.get_parameters()\n                    if isinstance(model_config, ModelConfig)\n                    else {}\n                ),\n            }\n        except Exception as e:\n            logger.error(f\"Error preparing API arguments: {e}\")\n            raise StepError(\n                f\"Failed to prepare API arguments: {str(e)}\"\n            ) from e\n\n    def _execute_with_retry(\n        self,\n        step: Step,\n        prompt: str,\n        api_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; Union[str, Iterator[str]]:\n        \"\"\"Execute an LLM call with appropriate retry handling.\n\n        Args:\n            step: The step being executed\n            prompt: The prompt to send to the LLM\n            api_kwargs: Optional dictionary of API call arguments\n\n        Returns:\n            Either a string or an iterator of strings for streaming responses\n\n        Raises:\n            ClientAIError: If the LLM call fails\n        \"\"\"\n        for attempt in range(step.config.retry_count + 1):\n            try:\n                return self._execute_single_call(step, prompt, api_kwargs)\n            except ClientAIError as e:\n                if attempt &gt;= step.config.retry_count:\n                    logger.error(\n                        f\"All retry attempts failed for \"\n                        f\"step '{step.name}': {e}\"\n                    )\n                    raise\n                logger.warning(\n                    f\"Retry {attempt + 1}/{step.config.retry_count} \"\n                    f\"for step '{step.name}': {e}\"\n                )\n                continue\n        raise ClientAIError(\"All retry attempts failed\")\n\n    def _execute_llm_call(\n        self,\n        step: Step,\n        prompt: str,\n        api_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; Union[str, Iterator[str]]:\n        \"\"\"\n        Execute an LLM call with appropriate retry handling.\n\n        Args:\n            step: The step being executed\n            prompt: The prompt to send to the LLM\n            api_kwargs: Optional dictionary of API call arguments\n\n        Returns:\n            Either a string or an iterator of strings for streaming responses\n\n        Raises:\n            ClientAIError: If the LLM call fails\n        \"\"\"\n        try:\n            if step.config.use_internal_retry:\n                return self._execute_with_retry(step, prompt, api_kwargs)\n            return self._execute_single_call(step, prompt, api_kwargs)\n        except ClientAIError:\n            raise\n        except Exception as e:\n            logger.error(f\"LLM execution failed: {e}\")\n            raise StepError(f\"LLM execution failed: {str(e)}\") from e\n\n    def _get_model_name(\n        self, model_config: Union[str, ModelConfig, None]\n    ) -&gt; str:\n        \"\"\"Extract the model name from various configuration formats.\n\n        Args:\n            model_config: The model configuration to process\n\n        Returns:\n            The model name as a string\n\n        Raises:\n            StepError: If model name extraction fails\n\n        Example:\n            ```python\n            name = engine._get_model_name(ModelConfig(name=\"gpt-4\"))\n            print(name)  # Output: \"gpt-4\"\n            ```\n        \"\"\"\n        try:\n            if isinstance(model_config, str):\n                return model_config\n            if isinstance(model_config, ModelConfig):\n                return model_config.name\n            if isinstance(self._default_model, str):\n                return self._default_model\n            if isinstance(self._default_model, ModelConfig):\n                return self._default_model.name\n            raise ValueError(\"No valid model configuration found\")\n        except Exception as e:\n            logger.error(f\"Error getting model name: {e}\")\n            raise StepError(f\"Failed to get model name: {str(e)}\") from e\n\n    def execute_step(\n        self,\n        step: Step,\n        *args: Any,\n        stream: Optional[bool] = None,\n        **kwargs: Any,\n    ) -&gt; Optional[Union[str, Iterator[str]]]:\n        \"\"\"Execute a single workflow step with full configuration.\n\n        Main entry point for step execution, handling tool selection,\n        LLM interaction, error handling, and result management.\n\n        Args:\n            step: The step to execute\n            *args: Additional positional arguments for the step\n            stream: Optional bool to override step's stream configuration\n            **kwargs: Additional keyword arguments for the step\n\n        Returns:\n            Optional[Union[str, Iterator[str]]]: The step execution result:\n                - None if step is disabled or failed\n                - Complete string if streaming is disabled\n                - Iterator of string chunks if streaming is enabled\n\n        Raises:\n            StepError: If step execution fails and step is required\n            ToolError: If tool execution fails\n            ClientAIError: If LLM interaction fails\n\n        Example:\n            Basic step execution:\n            ```python\n            # Execute with default stream setting\n            result = engine.execute_step(step, agent, \"input data\")\n\n            # Force streaming on\n            result = engine.execute_step(\n                step,\n                agent,\n                \"input data\",\n                stream=True\n            )\n\n            # Handle streaming results\n            if isinstance(result, Iterator):\n                for chunk in result:\n                    print(chunk, end=\"\")\n            else:\n                print(result)\n            ```\n\n        Notes:\n            - Handles both streaming and non-streaming responses\n            - Manages tool selection if enabled for step\n            - Updates agent context with results\n            - Supports retry logic for failed steps\n        \"\"\"\n        if self._current_agent is None:\n            raise StepError(\"No agent context available for step execution\")\n\n        logger.info(f\"Executing step '{step.name}'\")\n        if stream is None:\n            stream = getattr(step, \"stream\", False)\n\n        logger.debug(\n            f\"Step configuration: use_tools={step.use_tools}, \"\n            f\"send_to_llm={step.send_to_llm}, \"\n            f\"stream={stream}\"\n        )\n\n        if not step.config.enabled:\n            logger.info(f\"Step '{step.name}' is disabled, skipping\")\n            return None\n\n        try:\n            result = None\n\n            if step.send_to_llm:\n                try:\n                    result = self._handle_llm_step(\n                        step, self._current_agent, stream, args, kwargs\n                    )\n                except (ClientAIError, StepError, ToolError):\n                    raise\n                except Exception as e:\n                    raise StepError(\n                        f\"LLM step execution failed: {str(e)}\"\n                    ) from e\n            else:\n                try:\n                    result = self._handle_non_llm_step(\n                        step, self._current_agent, args, kwargs\n                    )\n                except (ValueError, TimeoutError):\n                    raise\n                except Exception as e:\n                    raise StepError(\n                        f\"Non-LLM step execution failed: {str(e)}\"\n                    ) from e\n\n            self._update_context(step, self._current_agent, result)\n            return result\n\n        except (ClientAIError, StepError, ToolError, ValueError, TimeoutError):\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error executing step '{step.name}': {e}\")\n            if step.config.required:\n                raise StepError(\n                    f\"Required step '{step.name}' failed: {str(e)}\"\n                ) from e\n            return None\n\n    def _handle_llm_step(\n        self,\n        step: Step,\n        agent: Any,\n        stream: Optional[bool],\n        args: Any,\n        kwargs: Any,\n    ) -&gt; Union[str, Iterator[str]]:\n        \"\"\"Handle execution of a step that involves LLM interaction.\n\n        Args:\n            step: The step being executed\n            agent: The agent instance\n            stream: Optional stream configuration override\n            args: Positional arguments for the step\n            kwargs: Keyword arguments for the step\n\n        Returns:\n            Either a string or an iterator of strings for streaming responses\n\n        Raises:\n            StepError: If step execution fails\n            ToolError: If tool execution fails\n            ClientAIError: If LLM interaction fails\n        \"\"\"\n        prompt = self._build_prompt(step, agent, *args, **kwargs)\n        model_config = self._prepare_model_config(step, stream)\n        api_kwargs = self._prepare_api_kwargs(model_config)\n\n        logger.debug(f\"Executing LLM call with streaming={stream}\")\n        return self._execute_llm_call(step, prompt, api_kwargs)\n\n    def _handle_non_llm_step(\n        self, step: Step, agent: Any, args: Any, kwargs: Any\n    ) -&gt; Union[str, Iterator[str]]:\n        \"\"\"Handle execution of a step that doesn't involve LLM interaction.\n\n        Args:\n            step: The step being executed\n            agent: The agent instance\n            args: Positional arguments for the step\n            kwargs: Keyword arguments for the step\n\n        Returns:\n            Either a string or an iterator of strings\n\n        Raises:\n            StepError: If step execution fails or returns invalid type\n        \"\"\"\n        result = step.func(agent, *args, **kwargs)\n        if not isinstance(result, (str, Iterator)):  # noqa: UP038\n            raise StepError(\n                f\"Step function must return str or Iterator, \"\n                f\"got {type(result)}\"\n            )\n        return result\n\n    def _prepare_model_config(\n        self, step: Step, stream: Optional[bool]\n    ) -&gt; Union[str, ModelConfig]:\n        \"\"\"Prepare the model configuration for a step execution.\n\n        Args:\n            step: The step being executed\n            stream: Optional stream configuration override\n\n        Returns:\n            The prepared model configuration\n\n        Raises:\n            StepError: If model configuration preparation fails\n        \"\"\"\n        try:\n            model_config = step.llm_config or self._default_model\n\n            if isinstance(model_config, ModelConfig):\n                if stream is not None:\n                    return model_config.merge(stream=stream)\n                return model_config\n\n            return ModelConfig(\n                name=model_config,\n                stream=stream if stream is not None else False,\n            )\n        except Exception as e:\n            logger.error(f\"Error preparing model configuration: {e}\")\n            raise StepError(\n                f\"Failed to prepare model configuration: {str(e)}\"\n            ) from e\n\n    def _update_context(\n        self,\n        step: Step,\n        agent: Any,\n        result: Optional[Union[str, Iterator[str]]],\n    ) -&gt; None:\n        \"\"\"Update the agent's context with step execution results.\n\n        Args:\n            step: Executed step\n            agent: Agent instance\n            result: Step execution result\n\n        Raises:\n            StepError: If context update fails\n        \"\"\"\n        try:\n            if result is not None and not isinstance(result, Iterator):\n                agent.context.set_step_result(step.name, result)\n\n                if step.config.pass_result:\n                    agent.context.current_input = result\n\n        except Exception as e:\n            logger.error(f\"Error updating context: {e}\")\n            raise StepError(f\"Failed to update context: {str(e)}\") from e\n</code></pre>"},{"location":"api/agent/core/execution/#clientai.agent.core.StepExecutionEngine.__init__","title":"<code>__init__(client, default_model, default_kwargs, tool_selection_config=None, tool_model=None)</code>","text":"<p>Initialize the execution engine with specified configurations.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>ClientAI</code> <p>The AI client for model interactions</p> required <code>default_model</code> <code>Union[str, ModelConfig]</code> <p>Default model configuration            (string name or ModelConfig)</p> required <code>default_kwargs</code> <code>Dict[str, Any]</code> <p>Default parameters for model calls</p> required <code>tool_selection_config</code> <code>Optional[ToolSelectionConfig]</code> <p>Configuration for tool selection behavior</p> <code>None</code> <code>tool_model</code> <code>Optional[Union[str, ModelConfig]]</code> <p>Model to use for tool selection         (default default_model)</p> <code>None</code> Example <pre><code>engine = StepExecutionEngine(\n    client=my_client,\n    default_model=\"gpt-4\",\n    default_kwargs={\"temperature\": 0.7},\n    tool_selection_config=ToolSelectionConfig(\n        confidence_threshold=0.8\n    )\n)\n</code></pre> <p>Raises:</p> Type Description <code>StepError</code> <p>If initialization fails or configuration is invalid</p> Source code in <code>clientai/agent/core/execution.py</code> <pre><code>def __init__(\n    self,\n    client: ClientAI,\n    default_model: Union[str, ModelConfig],\n    default_kwargs: Dict[str, Any],\n    tool_selection_config: Optional[ToolSelectionConfig] = None,\n    tool_model: Optional[Union[str, ModelConfig]] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the execution engine with specified configurations.\n\n    Args:\n        client: The AI client for model interactions\n        default_model: Default model configuration\n                       (string name or ModelConfig)\n        default_kwargs: Default parameters for model calls\n        tool_selection_config: Configuration for tool selection behavior\n        tool_model: Model to use for tool selection\n                    (default default_model)\n\n    Example:\n        ```python\n        engine = StepExecutionEngine(\n            client=my_client,\n            default_model=\"gpt-4\",\n            default_kwargs={\"temperature\": 0.7},\n            tool_selection_config=ToolSelectionConfig(\n                confidence_threshold=0.8\n            )\n        )\n        ```\n\n    Raises:\n        StepError: If initialization fails or configuration is invalid\n    \"\"\"\n    try:\n        if not client:\n            raise ValueError(\"Client must be specified\")\n        if not default_model:\n            raise ValueError(\"Default model must be specified\")\n\n        self._client = client\n        self._default_model = default_model\n        self._default_kwargs = default_kwargs\n        self._current_agent: Optional[Any] = None\n        self._default_tool_selection_config = (\n            tool_selection_config or ToolSelectionConfig()\n        )\n\n        self._default_tool_model = self._create_tool_model_config(\n            tool_model if tool_model is not None else default_model\n        )\n\n        logger.debug(\n            f\"Initialized StepExecutionEngine with model: {default_model}\"\n        )\n\n    except ValueError as e:\n        logger.error(f\"Initialization error: {e}\")\n        raise StepError(str(e)) from e\n    except Exception as e:\n        logger.error(f\"Unexpected initialization error: {e}\")\n        raise StepError(\n            f\"Unexpected initialization error: {str(e)}\"\n        ) from e\n\n    except ValueError as e:\n        logger.error(f\"Initialization error: {e}\")\n        raise StepError(str(e)) from e\n    except Exception as e:\n        logger.error(f\"Unexpected initialization error: {e}\")\n        raise StepError(\n            f\"Unexpected initialization error: {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api/agent/core/execution/#clientai.agent.core.StepExecutionEngine.execute_step","title":"<code>execute_step(step, *args, stream=None, **kwargs)</code>","text":"<p>Execute a single workflow step with full configuration.</p> <p>Main entry point for step execution, handling tool selection, LLM interaction, error handling, and result management.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>Step</code> <p>The step to execute</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments for the step</p> <code>()</code> <code>stream</code> <code>Optional[bool]</code> <p>Optional bool to override step's stream configuration</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the step</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Union[str, Iterator[str]]]</code> <p>Optional[Union[str, Iterator[str]]]: The step execution result: - None if step is disabled or failed - Complete string if streaming is disabled - Iterator of string chunks if streaming is enabled</p> <p>Raises:</p> Type Description <code>StepError</code> <p>If step execution fails and step is required</p> <code>ToolError</code> <p>If tool execution fails</p> <code>ClientAIError</code> <p>If LLM interaction fails</p> Example <p>Basic step execution: <pre><code># Execute with default stream setting\nresult = engine.execute_step(step, agent, \"input data\")\n\n# Force streaming on\nresult = engine.execute_step(\n    step,\n    agent,\n    \"input data\",\n    stream=True\n)\n\n# Handle streaming results\nif isinstance(result, Iterator):\n    for chunk in result:\n        print(chunk, end=\"\")\nelse:\n    print(result)\n</code></pre></p> Notes <ul> <li>Handles both streaming and non-streaming responses</li> <li>Manages tool selection if enabled for step</li> <li>Updates agent context with results</li> <li>Supports retry logic for failed steps</li> </ul> Source code in <code>clientai/agent/core/execution.py</code> <pre><code>def execute_step(\n    self,\n    step: Step,\n    *args: Any,\n    stream: Optional[bool] = None,\n    **kwargs: Any,\n) -&gt; Optional[Union[str, Iterator[str]]]:\n    \"\"\"Execute a single workflow step with full configuration.\n\n    Main entry point for step execution, handling tool selection,\n    LLM interaction, error handling, and result management.\n\n    Args:\n        step: The step to execute\n        *args: Additional positional arguments for the step\n        stream: Optional bool to override step's stream configuration\n        **kwargs: Additional keyword arguments for the step\n\n    Returns:\n        Optional[Union[str, Iterator[str]]]: The step execution result:\n            - None if step is disabled or failed\n            - Complete string if streaming is disabled\n            - Iterator of string chunks if streaming is enabled\n\n    Raises:\n        StepError: If step execution fails and step is required\n        ToolError: If tool execution fails\n        ClientAIError: If LLM interaction fails\n\n    Example:\n        Basic step execution:\n        ```python\n        # Execute with default stream setting\n        result = engine.execute_step(step, agent, \"input data\")\n\n        # Force streaming on\n        result = engine.execute_step(\n            step,\n            agent,\n            \"input data\",\n            stream=True\n        )\n\n        # Handle streaming results\n        if isinstance(result, Iterator):\n            for chunk in result:\n                print(chunk, end=\"\")\n        else:\n            print(result)\n        ```\n\n    Notes:\n        - Handles both streaming and non-streaming responses\n        - Manages tool selection if enabled for step\n        - Updates agent context with results\n        - Supports retry logic for failed steps\n    \"\"\"\n    if self._current_agent is None:\n        raise StepError(\"No agent context available for step execution\")\n\n    logger.info(f\"Executing step '{step.name}'\")\n    if stream is None:\n        stream = getattr(step, \"stream\", False)\n\n    logger.debug(\n        f\"Step configuration: use_tools={step.use_tools}, \"\n        f\"send_to_llm={step.send_to_llm}, \"\n        f\"stream={stream}\"\n    )\n\n    if not step.config.enabled:\n        logger.info(f\"Step '{step.name}' is disabled, skipping\")\n        return None\n\n    try:\n        result = None\n\n        if step.send_to_llm:\n            try:\n                result = self._handle_llm_step(\n                    step, self._current_agent, stream, args, kwargs\n                )\n            except (ClientAIError, StepError, ToolError):\n                raise\n            except Exception as e:\n                raise StepError(\n                    f\"LLM step execution failed: {str(e)}\"\n                ) from e\n        else:\n            try:\n                result = self._handle_non_llm_step(\n                    step, self._current_agent, args, kwargs\n                )\n            except (ValueError, TimeoutError):\n                raise\n            except Exception as e:\n                raise StepError(\n                    f\"Non-LLM step execution failed: {str(e)}\"\n                ) from e\n\n        self._update_context(step, self._current_agent, result)\n        return result\n\n    except (ClientAIError, StepError, ToolError, ValueError, TimeoutError):\n        raise\n    except Exception as e:\n        logger.error(f\"Unexpected error executing step '{step.name}': {e}\")\n        if step.config.required:\n            raise StepError(\n                f\"Required step '{step.name}' failed: {str(e)}\"\n            ) from e\n        return None\n</code></pre>"},{"location":"api/agent/core/workflow/","title":"WorkflowManager Class API Reference","text":"<p>The <code>WorkflowManager</code> class handles the registration and execution of workflow steps. It manages step ordering, dependencies, and execution flow.</p>"},{"location":"api/agent/core/workflow/#class-definition","title":"Class Definition","text":"<p>Manages the workflow of steps for an agent.</p> <p>This class registers steps, organizes them into a sequence, and executes them in the defined order. It supports two methods of passing data between steps:</p> <ol> <li> <p>Automatic Parameter Binding</p> <ul> <li>Steps receive previous results through their parameters</li> <li>Parameters filled in reverse chronological order (most recent first)</li> <li>Number of parameters determines how many previous results are passed</li> </ul> </li> <li> <p>Context Access</p> <ul> <li>Steps can access any previous result through the agent's context</li> <li>Useful for complex workflows or accessing results out of order</li> </ul> </li> </ol> <p>Attributes:</p> Name Type Description <code>_steps</code> <code>OrderedDict[str, Step]</code> <p>Ordered dictionary of registered steps</p> <code>_custom_run</code> <code>Optional[Callable[[Any], Any]]</code> <p>Optional custom workflow execution function</p> <code>_results_history</code> <code>Optional[Callable[[Any], Any]]</code> <p>History of step execution results</p> Example <p>Define an agent with different step parameter patterns: <pre><code>class MyAgent(Agent):\n    @think\n    def step1(self, input_text: str) -&gt; str:\n        # Receives the initial input\n        return \"First result\"\n\n    @act\n    def step2(self, prev_result: str) -&gt; str:\n        # Receives result from step1\n        return \"Second result\"\n\n    @synthesize\n    def step3(self, latest: str, previous: str) -&gt; str:\n        # Receives results from step2 and step1 in that order\n        return \"Final result\"\n</code></pre></p> <p>Using context access pattern: <pre><code>class MyAgent(Agent):\n    @think\n    def step1(self) -&gt; str:\n        input_text = self.context.current_input\n        return \"First result\"\n\n    @act\n    def step2(self) -&gt; str:\n        step1_result = self.context.get_step_result(\"step1\")\n        return \"Second result\"\n</code></pre></p> Notes <ul> <li>Steps cannot declare more parameters than available results</li> <li>Available results include the initial input and all previous results</li> <li>Both parameter binding and context access can be used in an agent</li> <li>A ValueError is raised if a step requests more results than available</li> </ul> Source code in <code>clientai/agent/core/workflow.py</code> <pre><code>class WorkflowManager:\n    \"\"\"Manages the workflow of steps for an agent.\n\n    This class registers steps, organizes them into a sequence,\n    and executes them in the defined order. It supports two\n    methods of passing data between steps:\n\n    1. Automatic Parameter Binding\n        - Steps receive previous results through their parameters\n        - Parameters filled in reverse chronological order (most recent first)\n        - Number of parameters determines how many previous results are passed\n\n    2. Context Access\n        - Steps can access any previous result through the agent's context\n        - Useful for complex workflows or accessing results out of order\n\n    Attributes:\n        _steps: Ordered dictionary of registered steps\n        _custom_run: Optional custom workflow execution function\n        _results_history: History of step execution results\n\n    Example:\n        Define an agent with different step parameter patterns:\n        ```python\n        class MyAgent(Agent):\n            @think\n            def step1(self, input_text: str) -&gt; str:\n                # Receives the initial input\n                return \"First result\"\n\n            @act\n            def step2(self, prev_result: str) -&gt; str:\n                # Receives result from step1\n                return \"Second result\"\n\n            @synthesize\n            def step3(self, latest: str, previous: str) -&gt; str:\n                # Receives results from step2 and step1 in that order\n                return \"Final result\"\n        ```\n\n        Using context access pattern:\n        ```python\n        class MyAgent(Agent):\n            @think\n            def step1(self) -&gt; str:\n                input_text = self.context.current_input\n                return \"First result\"\n\n            @act\n            def step2(self) -&gt; str:\n                step1_result = self.context.get_step_result(\"step1\")\n                return \"Second result\"\n        ```\n\n    Notes:\n        - Steps cannot declare more parameters than available results\n        - Available results include the initial input and all previous results\n        - Both parameter binding and context access can be used in an agent\n        - A ValueError is raised if a step requests more results than available\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initialize the WorkflowManager.\n\n        Sets up empty step registry and custom run function.\n\n        Example:\n            ```python\n            manager = WorkflowManager()\n            ```\n        \"\"\"\n        try:\n            self._steps: OrderedDict[str, Step] = OrderedDict()\n            self._custom_run: Optional[Callable[[Any], Any]] = None\n            logger.debug(\"Initialized WorkflowManager\")\n        except Exception as e:\n            logger.error(f\"Failed to initialize WorkflowManager: {e}\")\n            raise WorkflowError(\n                f\"Failed to initialize WorkflowManager: {str(e)}\"\n            ) from e\n\n    def _validate_agent_instance(self, agent_instance: Any) -&gt; Dict[str, Any]:\n        \"\"\"Validate and retrieve the agent class dictionary.\n\n        Args:\n            agent_instance: The agent instance to validate\n\n        Returns:\n            Dict[str, Any]: The validated agent class dictionary\n\n        Raises:\n            WorkflowError: If agent instance is invalid\n        \"\"\"\n        try:\n            return agent_instance.__class__.__dict__  # type: ignore\n        except AttributeError as e:\n            raise WorkflowError(f\"Invalid agent instance: {str(e)}\")\n\n    def _process_step_info(\n        self, name: str, func: Any\n    ) -&gt; Optional[Tuple[str, Step]]:\n        \"\"\"Process a single step or run method from the class.\n\n        Args:\n            name: Name of the step/method\n            func: Function to process\n\n        Returns:\n            Optional tuple of (step name, Step instance) if valid step\n\n        Raises:\n            StepError: If step info is invalid\n        \"\"\"\n        try:\n            if not callable(func):\n                return None\n\n            if hasattr(func, \"_step_info\"):\n                logger.debug(f\"Processing step: {name}\")\n                step_info = func._step_info\n                if not isinstance(step_info, Step):\n                    raise StepError(\n                        f\"Invalid step info for {name}: \"\n                        f\"expected Step, got {type(step_info)}\"\n                    )\n                return (step_info.name, step_info)\n            return None\n        except Exception as e:\n            logger.error(f\"Error processing step '{name}': {e}\")\n            raise StepError(f\"Error processing step '{name}': {str(e)}\") from e\n\n    def _process_run_method(\n        self, name: str, func: Any, agent_instance: Any\n    ) -&gt; None:\n        \"\"\"Process a custom run method if found.\n\n        Args:\n            name: Name of the method\n            func: Function to process\n            agent_instance: Agent instance owning the method\n\n        Raises:\n            StepError: If run method binding fails\n        \"\"\"\n        try:\n            if callable(func) and hasattr(func, \"_is_run\"):\n                logger.debug(f\"Found custom run method: {name}\")\n                self._custom_run = getattr(agent_instance, name)\n        except AttributeError as e:\n            raise StepError(f\"Failed to bind custom run method: {str(e)}\")\n\n    def register_class_steps(self, agent_instance: Any) -&gt; None:\n        \"\"\"Register steps defined in an agent class.\n\n        Scans the agent class for methods decorated as steps\n        and registers them for execution. Also identifies\n        any custom run method if present.\n\n        Args:\n            agent_instance: The agent instance containing step definitions\n\n        Raises:\n            StepError: If step registration fails\n            WorkflowError: If class scanning fails\n\n        Example:\n            Register steps in an agent class:\n            ```python\n            class MyAgent(Agent):\n                @think(\"analyze\")\n                def analyze_step(self, data: str) -&gt; str:\n                    return f\"Analyzing: {data}\"\n\n                @act(\"process\")\n                def process_step(self, analysis: str) -&gt; str:\n                    return f\"Processing: {analysis}\"\n\n            agent = MyAgent(...)\n            workflow = WorkflowManager()\n            workflow.register_class_steps(agent)\n            ```\n\n        Notes:\n            - Steps must be decorated with appropriate step decorators\n            - Steps are registered in the order they're defined\n            - Custom run methods are detected and stored separately\n        \"\"\"\n        try:\n            class_dict = self._validate_agent_instance(agent_instance)\n            steps: List[Tuple[str, Step]] = []\n\n            for name, func in class_dict.items():\n                step_info = self._process_step_info(name, func)\n                if step_info:\n                    steps.append(step_info)\n                self._process_run_method(name, func, agent_instance)\n\n            self._steps = OrderedDict(steps)\n            logger.info(\n                f\"Registered {len(steps)} steps: {list(self._steps.keys())}\"\n            )\n\n        except (StepError, WorkflowError):\n            raise\n        except Exception as e:\n            logger.error(f\"Failed to register class steps: {e}\")\n            raise WorkflowError(\n                f\"Failed to register class steps: {str(e)}\"\n            ) from e\n\n    def _execute_custom_run(\n        self,\n        agent: Any,\n        input_data: Any,\n        engine: StepExecutionProtocol,\n        stream_override: Optional[bool] = None,\n    ) -&gt; Any:\n        \"\"\"Execute the custom run method with proper stream handling.\"\"\"\n        try:\n            logger.info(\"Using custom run method\")\n            if self._custom_run is None:\n                raise WorkflowError(\"Custom run method is None\")\n\n            try:\n                if stream_override is not None:\n                    original_steps = {}\n                    for step in self._steps.values():\n                        original_steps[step.name] = getattr(\n                            step, \"stream\", False\n                        )\n                        object.__setattr__(step, \"stream\", stream_override)\n\n                result = self._custom_run(input_data)\n                return result\n\n            finally:\n                if (\n                    stream_override is not None\n                    and \"original_steps\" in locals()\n                ):\n                    for step in self._steps.values():\n                        object.__setattr__(\n                            step, \"stream\", original_steps[step.name]\n                        )\n\n        except Exception as e:\n            logger.error(f\"Custom run method failed: {e}\")\n            raise WorkflowError(f\"Custom run method failed: {str(e)}\") from e\n\n    def _initialize_execution(self, agent: Any, input_data: Any) -&gt; None:\n        \"\"\"Process a custom run method if found.\n\n        Args:\n            name: Name of the method\n            func: Function to process\n            agent_instance: Agent instance owning the method\n\n        Raises:\n            StepError: If run method binding fails\n        \"\"\"\n        try:\n            if not self._steps and not self._custom_run:\n                raise WorkflowError(\n                    \"No steps registered and no custom run method defined\"\n                )\n\n            agent.context.clear()\n            agent.context.current_input = input_data\n        except Exception as e:\n            raise WorkflowError(f\"Failed to initialize execution: {str(e)}\")\n\n    def _get_step_parameters(self, step: Step) -&gt; int:\n        \"\"\"Get number of parameters for a step.\n\n        Args:\n            step: Step to analyze\n\n        Returns:\n            Number of non-self parameters\n\n        Raises:\n            StepError: If step function is invalid\n        \"\"\"\n        try:\n            params = [\n                param\n                for param in step.func.__code__.co_varnames[\n                    : step.func.__code__.co_argcount\n                ]\n                if param != \"self\"\n            ]\n            return len(params)\n        except AttributeError as e:\n            raise StepError(f\"Invalid step function: {str(e)}\")\n\n    def _validate_parameter_count(\n        self, step: Step, param_count: int, available_results: int\n    ) -&gt; None:\n        \"\"\"Validate parameter count against available results.\n\n        Args:\n            step: Step to validate\n            param_count: Number of parameters required\n            available_results: Number of results available\n\n        Raises:\n            ValueError: If step requires more parameters than available results\n        \"\"\"\n        if param_count &gt; available_results:\n            raise ValueError(\n                f\"Step '{step.name}' declares {param_count} parameters, \"\n                f\"but only {available_results} previous results are \"\n                f\"available (including input data and results from steps: \"\n                f\"{', '.join(self._steps.keys())})\"\n            )\n\n    def _get_previous_results(self, agent: Any, param_count: int) -&gt; List[Any]:\n        \"\"\"Gather results from previous steps.\n\n        Args:\n            agent: Agent instance\n            param_count: Number of results needed\n\n        Returns:\n            List of previous results in reverse chronological order\n\n        Raises:\n            StepError: If result gathering fails\n        \"\"\"\n        try:\n            results = []\n\n            step_results = [\n                agent.context.last_results[step.name]\n                for step in self._steps.values()\n                if step.name in agent.context.last_results\n            ][: param_count - 1]\n\n            if len(step_results) &lt; param_count:\n                results = step_results + [agent.context.original_input]\n            else:\n                results = step_results\n\n            return results\n\n        except Exception as e:\n            logger.error(f\"Failed to gather previous results: {e}\")\n            raise StepError(f\"Failed to gather previous results: {str(e)}\")\n\n    def _execute_step(\n        self,\n        step: Step,\n        agent: Any,\n        last_result: Any,\n        param_count: int,\n        current_stream: bool,\n        engine: StepExecutionProtocol,\n    ) -&gt; Any:\n        \"\"\"Execute a single step with proper parameter handling.\n\n        Args:\n            step: Step to execute\n            agent: Agent instance\n            last_result: Most recent result\n            param_count: Number of parameters needed\n            current_stream: Whether to stream output\n            engine: Execution engine to use\n\n        Returns:\n            Step execution result\n\n        Raises:\n            StepError: If step execution fails\n        \"\"\"\n        try:\n            if param_count == 0:\n                return engine.execute_step(step, agent, stream=current_stream)\n            elif param_count == 1:\n                input_data = (\n                    agent.context.original_input\n                    if len(agent.context.last_results) == 0\n                    else last_result\n                )\n                return engine.execute_step(\n                    step, agent, input_data, stream=current_stream\n                )\n            else:\n                previous_results = self._get_previous_results(\n                    agent, param_count\n                )\n                return engine.execute_step(\n                    step, agent, *previous_results, stream=current_stream\n                )\n        except Exception as e:\n            raise StepError(\n                f\"Failed to execute step '{step.name}': {str(e)}\"\n            ) from e\n\n    def _handle_step_result(\n        self, step: Step, result: Any, agent: Any\n    ) -&gt; Optional[Any]:\n        \"\"\"Handle the result of a step execution.\n\n        Args:\n            step: Executed step\n            result: Step execution result\n            agent: Agent instance\n\n        Returns:\n            Result to pass to next step if step.config.pass_result is True\n        \"\"\"\n        if result is not None:\n            agent.context.last_results[step.name] = result\n            if step.config.pass_result:\n                agent.context.current_input = result\n                return result\n        return None\n\n    def _get_step_stream_setting(\n        self,\n        step: Step,\n        stream_override: Optional[bool],\n        is_intermediate_step: bool,\n    ) -&gt; bool:\n        \"\"\"Determine the appropriate streaming setting for a step.\n\n        Args:\n            step: The step being executed\n            stream_override: Optional streaming override from run()\n            is_intermediate_step: Whether this is an intermediate step\n\n        Returns:\n            bool: The determined streaming setting\n        \"\"\"\n        if stream_override is True and is_intermediate_step:\n            return False\n\n        if stream_override is not None:\n            return stream_override\n\n        return getattr(step, \"stream\", False)\n\n    def execute(\n        self,\n        agent: Any,\n        input_data: Any,\n        engine: StepExecutionProtocol,\n        stream_override: Optional[bool] = None,\n    ) -&gt; Any:\n        \"\"\"Execute the workflow with provided input data.\n\n        Processes each step in sequence, passing results between steps\n        as configured. Uses either default sequential execution or\n        custom run method if defined.\n\n        Args:\n            agent: The agent executing the workflow\n            input_data: The initial input data\n            engine: The execution engine for processing steps\n            stream_override: Optional bool to override steps'\n                            stream configuration\n\n        Returns:\n            Any: The final result of workflow execution\n\n        Raises:\n            StepError: If a required step fails\n            WorkflowError: If workflow execution fails\n            ValueError: If step parameter validation fails\n        \"\"\"\n        try:\n            logger.info(\n                f\"Starting workflow execution with {len(self._steps)} steps\"\n            )\n            logger.debug(f\"Input data: {input_data}\")\n\n            self._initialize_execution(agent, input_data)\n            agent.context.set_input(input_data)\n            agent.context.increment_iteration()\n\n            engine._current_agent = agent  # type: ignore\n\n            try:\n                if self._custom_run:\n                    return self._execute_custom_run(\n                        agent=agent,\n                        input_data=input_data,\n                        engine=engine,\n                        stream_override=stream_override,\n                    )\n\n                last_result = input_data\n                steps = list(self._steps.values())\n\n                for step in steps[:-1]:\n                    try:\n                        logger.info(\n                            f\"Executing step: {step.name} ({step.step_type})\"\n                        )\n\n                        param_count = self._get_step_parameters(step)\n                        available_results = len(agent.context.last_results) + 1\n                        self._validate_parameter_count(\n                            step, param_count, available_results\n                        )\n\n                        current_stream = self._get_step_stream_setting(\n                            step=step,\n                            stream_override=stream_override,\n                            is_intermediate_step=True,\n                        )\n\n                        result = engine.execute_step(\n                            step,\n                            last_result,\n                            stream=current_stream,\n                        )\n\n                        step_result = self._handle_step_result(\n                            step, result, agent\n                        )\n                        if step_result is not None:\n                            last_result = step_result\n\n                        logger.debug(f\"Step {step.name} completed\")\n                    except (StepError, ValueError) as e:\n                        logger.error(f\"Error in step '{step.name}': {e}\")\n                        if step.config.required:\n                            raise\n                        logger.warning(\n                            f\"Continuing workflow after non-required \"\n                            f\"step failure: {step.name}\"\n                        )\n                        continue\n\n                if steps:\n                    final_step = steps[-1]\n\n                    param_count = self._get_step_parameters(final_step)\n                    available_results = len(agent.context.last_results) + 1\n                    self._validate_parameter_count(\n                        final_step, param_count, available_results\n                    )\n\n                    current_stream = self._get_step_stream_setting(\n                        step=final_step,\n                        stream_override=stream_override,\n                        is_intermediate_step=False,\n                    )\n\n                    result = engine.execute_step(\n                        final_step, last_result, stream=current_stream\n                    )\n\n                    if not current_stream:\n                        self._handle_step_result(final_step, result, agent)\n\n                    return result\n\n                logger.info(\"Workflow execution completed\")\n                return last_result\n\n            finally:\n                engine._current_agent = None  # type: ignore\n\n        except (StepError, WorkflowError, ValueError):\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected workflow execution error: {e}\")\n            raise WorkflowError(\n                f\"Unexpected workflow execution error: {str(e)}\"\n            ) from e\n\n    def get_step(self, name: str) -&gt; Optional[Step]:\n        \"\"\"Retrieve a registered step by its name.\n\n        Args:\n            name: The name of the step to retrieve\n\n        Returns:\n            Optional[Step]: The requested step if found, None otherwise\n\n        Example:\n            Retrieve and check a step:\n            ```python\n            workflow = WorkflowManager()\n            step = workflow.get_step(\"analyze\")\n            if step:\n                print(f\"Found step: {step.name}\")\n                print(f\"Step type: {step.step_type}\")\n            ```\n        \"\"\"\n        try:\n            return self._steps.get(name)\n        except Exception as e:\n            logger.error(f\"Failed to retrieve step '{name}': {e}\")\n            raise WorkflowError(\n                f\"Failed to retrieve step '{name}': {str(e)}\"\n            ) from e\n\n    def get_steps(self) -&gt; OrderedDict[str, Step]:\n        \"\"\"Retrieve all registered steps in execution order.\n\n        Returns:\n            OrderedDict[str, Step]: Dictionary mapping step names\n                                    to their instances\n\n        Example:\n            Get and inspect all steps:\n            ```python\n            workflow = WorkflowManager()\n            steps = workflow.get_steps()\n\n            for name, step in steps.items():\n                print(f\"Step: {name}\")\n                print(f\"Type: {step.step_type}\")\n                print(f\"Uses tools: {step.use_tools}\")\n            ```\n        \"\"\"\n        try:\n            return self._steps.copy()\n        except Exception as e:\n            logger.error(\"Failed to retrieve steps: {e}\")\n            raise WorkflowError(f\"Failed to retrieve steps: {str(e)}\") from e\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset the workflow manager to its initial state.\n\n        Clears all registered steps and custom run method.\n\n        Example:\n            Reset workflow state:\n            ```python\n            workflow = WorkflowManager()\n            # ... register steps and execute ...\n\n            workflow.reset()\n            print(len(workflow.get_steps()))  # Output: 0\n            ```\n\n        Notes:\n            - Removes all registered steps\n            - Clears custom run method if set\n            - Does not affect step configurations\n        \"\"\"\n        try:\n            self._steps.clear()\n            self._custom_run = None\n            logger.debug(\"Workflow manager reset completed\")\n        except Exception as e:\n            logger.error(f\"Failed to reset workflow manager: {e}\")\n            raise WorkflowError(\n                f\"Failed to reset workflow manager: {str(e)}\"\n            ) from e\n\n    def get_steps_by_type(self, step_type: StepType) -&gt; Dict[str, Step]:\n        \"\"\"Retrieve all steps of a specific type.\n\n        Args:\n            step_type: The type of steps to retrieve\n\n        Returns:\n            Dict[str, Step]: Dictionary mapping step names to their instances\n                for the given type\n\n        Raises:\n            ValueError: If step_type is invalid\n            WorkflowError: If step retrieval fails\n\n        Example:\n            Get steps by type:\n            ```python\n            workflow = WorkflowManager()\n            think_steps = workflow.get_steps_by_type(StepType.THINK)\n\n            print(f\"Found {len(think_steps)} thinking steps:\")\n            for name, step in think_steps.items():\n                print(f\"- {name}: {step.description}\")\n            ```\n\n        Notes:\n            - Returns empty dict if no steps of the type exist\n            - Maintains original step order within type\n            - Steps are returned by reference\n        \"\"\"\n        try:\n            if not isinstance(step_type, StepType):\n                raise ValueError(\n                    \"Invalid step type: expected StepType, \"\n                    f\"got {type(step_type)}\"\n                )\n\n            return {\n                name: step\n                for name, step in self._steps.items()\n                if step.step_type == step_type\n            }\n        except ValueError:\n            raise\n        except Exception as e:\n            logger.error(\n                f\"Failed to retrieve steps by type '{step_type}': {e}\"\n            )\n            raise WorkflowError(\n                f\"Failed to retrieve steps by type '{step_type}': {str(e)}\"\n            ) from e\n</code></pre>"},{"location":"api/agent/core/workflow/#clientai.agent.core.WorkflowManager.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the WorkflowManager.</p> <p>Sets up empty step registry and custom run function.</p> Example <pre><code>manager = WorkflowManager()\n</code></pre> Source code in <code>clientai/agent/core/workflow.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initialize the WorkflowManager.\n\n    Sets up empty step registry and custom run function.\n\n    Example:\n        ```python\n        manager = WorkflowManager()\n        ```\n    \"\"\"\n    try:\n        self._steps: OrderedDict[str, Step] = OrderedDict()\n        self._custom_run: Optional[Callable[[Any], Any]] = None\n        logger.debug(\"Initialized WorkflowManager\")\n    except Exception as e:\n        logger.error(f\"Failed to initialize WorkflowManager: {e}\")\n        raise WorkflowError(\n            f\"Failed to initialize WorkflowManager: {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api/agent/core/workflow/#clientai.agent.core.WorkflowManager.execute","title":"<code>execute(agent, input_data, engine, stream_override=None)</code>","text":"<p>Execute the workflow with provided input data.</p> <p>Processes each step in sequence, passing results between steps as configured. Uses either default sequential execution or custom run method if defined.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Any</code> <p>The agent executing the workflow</p> required <code>input_data</code> <code>Any</code> <p>The initial input data</p> required <code>engine</code> <code>StepExecutionProtocol</code> <p>The execution engine for processing steps</p> required <code>stream_override</code> <code>Optional[bool]</code> <p>Optional bool to override steps'             stream configuration</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The final result of workflow execution</p> <p>Raises:</p> Type Description <code>StepError</code> <p>If a required step fails</p> <code>WorkflowError</code> <p>If workflow execution fails</p> <code>ValueError</code> <p>If step parameter validation fails</p> Source code in <code>clientai/agent/core/workflow.py</code> <pre><code>def execute(\n    self,\n    agent: Any,\n    input_data: Any,\n    engine: StepExecutionProtocol,\n    stream_override: Optional[bool] = None,\n) -&gt; Any:\n    \"\"\"Execute the workflow with provided input data.\n\n    Processes each step in sequence, passing results between steps\n    as configured. Uses either default sequential execution or\n    custom run method if defined.\n\n    Args:\n        agent: The agent executing the workflow\n        input_data: The initial input data\n        engine: The execution engine for processing steps\n        stream_override: Optional bool to override steps'\n                        stream configuration\n\n    Returns:\n        Any: The final result of workflow execution\n\n    Raises:\n        StepError: If a required step fails\n        WorkflowError: If workflow execution fails\n        ValueError: If step parameter validation fails\n    \"\"\"\n    try:\n        logger.info(\n            f\"Starting workflow execution with {len(self._steps)} steps\"\n        )\n        logger.debug(f\"Input data: {input_data}\")\n\n        self._initialize_execution(agent, input_data)\n        agent.context.set_input(input_data)\n        agent.context.increment_iteration()\n\n        engine._current_agent = agent  # type: ignore\n\n        try:\n            if self._custom_run:\n                return self._execute_custom_run(\n                    agent=agent,\n                    input_data=input_data,\n                    engine=engine,\n                    stream_override=stream_override,\n                )\n\n            last_result = input_data\n            steps = list(self._steps.values())\n\n            for step in steps[:-1]:\n                try:\n                    logger.info(\n                        f\"Executing step: {step.name} ({step.step_type})\"\n                    )\n\n                    param_count = self._get_step_parameters(step)\n                    available_results = len(agent.context.last_results) + 1\n                    self._validate_parameter_count(\n                        step, param_count, available_results\n                    )\n\n                    current_stream = self._get_step_stream_setting(\n                        step=step,\n                        stream_override=stream_override,\n                        is_intermediate_step=True,\n                    )\n\n                    result = engine.execute_step(\n                        step,\n                        last_result,\n                        stream=current_stream,\n                    )\n\n                    step_result = self._handle_step_result(\n                        step, result, agent\n                    )\n                    if step_result is not None:\n                        last_result = step_result\n\n                    logger.debug(f\"Step {step.name} completed\")\n                except (StepError, ValueError) as e:\n                    logger.error(f\"Error in step '{step.name}': {e}\")\n                    if step.config.required:\n                        raise\n                    logger.warning(\n                        f\"Continuing workflow after non-required \"\n                        f\"step failure: {step.name}\"\n                    )\n                    continue\n\n            if steps:\n                final_step = steps[-1]\n\n                param_count = self._get_step_parameters(final_step)\n                available_results = len(agent.context.last_results) + 1\n                self._validate_parameter_count(\n                    final_step, param_count, available_results\n                )\n\n                current_stream = self._get_step_stream_setting(\n                    step=final_step,\n                    stream_override=stream_override,\n                    is_intermediate_step=False,\n                )\n\n                result = engine.execute_step(\n                    final_step, last_result, stream=current_stream\n                )\n\n                if not current_stream:\n                    self._handle_step_result(final_step, result, agent)\n\n                return result\n\n            logger.info(\"Workflow execution completed\")\n            return last_result\n\n        finally:\n            engine._current_agent = None  # type: ignore\n\n    except (StepError, WorkflowError, ValueError):\n        raise\n    except Exception as e:\n        logger.error(f\"Unexpected workflow execution error: {e}\")\n        raise WorkflowError(\n            f\"Unexpected workflow execution error: {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api/agent/core/workflow/#clientai.agent.core.WorkflowManager.get_step","title":"<code>get_step(name)</code>","text":"<p>Retrieve a registered step by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the step to retrieve</p> required <p>Returns:</p> Type Description <code>Optional[Step]</code> <p>Optional[Step]: The requested step if found, None otherwise</p> Example <p>Retrieve and check a step: <pre><code>workflow = WorkflowManager()\nstep = workflow.get_step(\"analyze\")\nif step:\n    print(f\"Found step: {step.name}\")\n    print(f\"Step type: {step.step_type}\")\n</code></pre></p> Source code in <code>clientai/agent/core/workflow.py</code> <pre><code>def get_step(self, name: str) -&gt; Optional[Step]:\n    \"\"\"Retrieve a registered step by its name.\n\n    Args:\n        name: The name of the step to retrieve\n\n    Returns:\n        Optional[Step]: The requested step if found, None otherwise\n\n    Example:\n        Retrieve and check a step:\n        ```python\n        workflow = WorkflowManager()\n        step = workflow.get_step(\"analyze\")\n        if step:\n            print(f\"Found step: {step.name}\")\n            print(f\"Step type: {step.step_type}\")\n        ```\n    \"\"\"\n    try:\n        return self._steps.get(name)\n    except Exception as e:\n        logger.error(f\"Failed to retrieve step '{name}': {e}\")\n        raise WorkflowError(\n            f\"Failed to retrieve step '{name}': {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api/agent/core/workflow/#clientai.agent.core.WorkflowManager.get_steps","title":"<code>get_steps()</code>","text":"<p>Retrieve all registered steps in execution order.</p> <p>Returns:</p> Type Description <code>OrderedDict[str, Step]</code> <p>OrderedDict[str, Step]: Dictionary mapping step names                     to their instances</p> Example <p>Get and inspect all steps: <pre><code>workflow = WorkflowManager()\nsteps = workflow.get_steps()\n\nfor name, step in steps.items():\n    print(f\"Step: {name}\")\n    print(f\"Type: {step.step_type}\")\n    print(f\"Uses tools: {step.use_tools}\")\n</code></pre></p> Source code in <code>clientai/agent/core/workflow.py</code> <pre><code>def get_steps(self) -&gt; OrderedDict[str, Step]:\n    \"\"\"Retrieve all registered steps in execution order.\n\n    Returns:\n        OrderedDict[str, Step]: Dictionary mapping step names\n                                to their instances\n\n    Example:\n        Get and inspect all steps:\n        ```python\n        workflow = WorkflowManager()\n        steps = workflow.get_steps()\n\n        for name, step in steps.items():\n            print(f\"Step: {name}\")\n            print(f\"Type: {step.step_type}\")\n            print(f\"Uses tools: {step.use_tools}\")\n        ```\n    \"\"\"\n    try:\n        return self._steps.copy()\n    except Exception as e:\n        logger.error(\"Failed to retrieve steps: {e}\")\n        raise WorkflowError(f\"Failed to retrieve steps: {str(e)}\") from e\n</code></pre>"},{"location":"api/agent/core/workflow/#clientai.agent.core.WorkflowManager.get_steps_by_type","title":"<code>get_steps_by_type(step_type)</code>","text":"<p>Retrieve all steps of a specific type.</p> <p>Parameters:</p> Name Type Description Default <code>step_type</code> <code>StepType</code> <p>The type of steps to retrieve</p> required <p>Returns:</p> Type Description <code>Dict[str, Step]</code> <p>Dict[str, Step]: Dictionary mapping step names to their instances for the given type</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If step_type is invalid</p> <code>WorkflowError</code> <p>If step retrieval fails</p> Example <p>Get steps by type: <pre><code>workflow = WorkflowManager()\nthink_steps = workflow.get_steps_by_type(StepType.THINK)\n\nprint(f\"Found {len(think_steps)} thinking steps:\")\nfor name, step in think_steps.items():\n    print(f\"- {name}: {step.description}\")\n</code></pre></p> Notes <ul> <li>Returns empty dict if no steps of the type exist</li> <li>Maintains original step order within type</li> <li>Steps are returned by reference</li> </ul> Source code in <code>clientai/agent/core/workflow.py</code> <pre><code>def get_steps_by_type(self, step_type: StepType) -&gt; Dict[str, Step]:\n    \"\"\"Retrieve all steps of a specific type.\n\n    Args:\n        step_type: The type of steps to retrieve\n\n    Returns:\n        Dict[str, Step]: Dictionary mapping step names to their instances\n            for the given type\n\n    Raises:\n        ValueError: If step_type is invalid\n        WorkflowError: If step retrieval fails\n\n    Example:\n        Get steps by type:\n        ```python\n        workflow = WorkflowManager()\n        think_steps = workflow.get_steps_by_type(StepType.THINK)\n\n        print(f\"Found {len(think_steps)} thinking steps:\")\n        for name, step in think_steps.items():\n            print(f\"- {name}: {step.description}\")\n        ```\n\n    Notes:\n        - Returns empty dict if no steps of the type exist\n        - Maintains original step order within type\n        - Steps are returned by reference\n    \"\"\"\n    try:\n        if not isinstance(step_type, StepType):\n            raise ValueError(\n                \"Invalid step type: expected StepType, \"\n                f\"got {type(step_type)}\"\n            )\n\n        return {\n            name: step\n            for name, step in self._steps.items()\n            if step.step_type == step_type\n        }\n    except ValueError:\n        raise\n    except Exception as e:\n        logger.error(\n            f\"Failed to retrieve steps by type '{step_type}': {e}\"\n        )\n        raise WorkflowError(\n            f\"Failed to retrieve steps by type '{step_type}': {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api/agent/core/workflow/#clientai.agent.core.WorkflowManager.register_class_steps","title":"<code>register_class_steps(agent_instance)</code>","text":"<p>Register steps defined in an agent class.</p> <p>Scans the agent class for methods decorated as steps and registers them for execution. Also identifies any custom run method if present.</p> <p>Parameters:</p> Name Type Description Default <code>agent_instance</code> <code>Any</code> <p>The agent instance containing step definitions</p> required <p>Raises:</p> Type Description <code>StepError</code> <p>If step registration fails</p> <code>WorkflowError</code> <p>If class scanning fails</p> Example <p>Register steps in an agent class: <pre><code>class MyAgent(Agent):\n    @think(\"analyze\")\n    def analyze_step(self, data: str) -&gt; str:\n        return f\"Analyzing: {data}\"\n\n    @act(\"process\")\n    def process_step(self, analysis: str) -&gt; str:\n        return f\"Processing: {analysis}\"\n\nagent = MyAgent(...)\nworkflow = WorkflowManager()\nworkflow.register_class_steps(agent)\n</code></pre></p> Notes <ul> <li>Steps must be decorated with appropriate step decorators</li> <li>Steps are registered in the order they're defined</li> <li>Custom run methods are detected and stored separately</li> </ul> Source code in <code>clientai/agent/core/workflow.py</code> <pre><code>def register_class_steps(self, agent_instance: Any) -&gt; None:\n    \"\"\"Register steps defined in an agent class.\n\n    Scans the agent class for methods decorated as steps\n    and registers them for execution. Also identifies\n    any custom run method if present.\n\n    Args:\n        agent_instance: The agent instance containing step definitions\n\n    Raises:\n        StepError: If step registration fails\n        WorkflowError: If class scanning fails\n\n    Example:\n        Register steps in an agent class:\n        ```python\n        class MyAgent(Agent):\n            @think(\"analyze\")\n            def analyze_step(self, data: str) -&gt; str:\n                return f\"Analyzing: {data}\"\n\n            @act(\"process\")\n            def process_step(self, analysis: str) -&gt; str:\n                return f\"Processing: {analysis}\"\n\n        agent = MyAgent(...)\n        workflow = WorkflowManager()\n        workflow.register_class_steps(agent)\n        ```\n\n    Notes:\n        - Steps must be decorated with appropriate step decorators\n        - Steps are registered in the order they're defined\n        - Custom run methods are detected and stored separately\n    \"\"\"\n    try:\n        class_dict = self._validate_agent_instance(agent_instance)\n        steps: List[Tuple[str, Step]] = []\n\n        for name, func in class_dict.items():\n            step_info = self._process_step_info(name, func)\n            if step_info:\n                steps.append(step_info)\n            self._process_run_method(name, func, agent_instance)\n\n        self._steps = OrderedDict(steps)\n        logger.info(\n            f\"Registered {len(steps)} steps: {list(self._steps.keys())}\"\n        )\n\n    except (StepError, WorkflowError):\n        raise\n    except Exception as e:\n        logger.error(f\"Failed to register class steps: {e}\")\n        raise WorkflowError(\n            f\"Failed to register class steps: {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api/agent/core/workflow/#clientai.agent.core.WorkflowManager.reset","title":"<code>reset()</code>","text":"<p>Reset the workflow manager to its initial state.</p> <p>Clears all registered steps and custom run method.</p> Example <p>Reset workflow state: <pre><code>workflow = WorkflowManager()\n# ... register steps and execute ...\n\nworkflow.reset()\nprint(len(workflow.get_steps()))  # Output: 0\n</code></pre></p> Notes <ul> <li>Removes all registered steps</li> <li>Clears custom run method if set</li> <li>Does not affect step configurations</li> </ul> Source code in <code>clientai/agent/core/workflow.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the workflow manager to its initial state.\n\n    Clears all registered steps and custom run method.\n\n    Example:\n        Reset workflow state:\n        ```python\n        workflow = WorkflowManager()\n        # ... register steps and execute ...\n\n        workflow.reset()\n        print(len(workflow.get_steps()))  # Output: 0\n        ```\n\n    Notes:\n        - Removes all registered steps\n        - Clears custom run method if set\n        - Does not affect step configurations\n    \"\"\"\n    try:\n        self._steps.clear()\n        self._custom_run = None\n        logger.debug(\"Workflow manager reset completed\")\n    except Exception as e:\n        logger.error(f\"Failed to reset workflow manager: {e}\")\n        raise WorkflowError(\n            f\"Failed to reset workflow manager: {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api/agent/steps/decorators/","title":"Step Decorators API Reference","text":"<p>The step decorators provide a way to define workflow steps using Python decorators like @think, @act, @observe, and @synthesize.</p>"},{"location":"api/agent/steps/decorators/#function-definitions","title":"Function Definitions","text":""},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.act","title":"<code>act = create_step_decorator(StepType.ACT)</code>  <code>module-attribute</code>","text":"<p>Decorator for creating action/execution steps.</p> Example <pre><code>@act(\"process\", description=\"Processes analyzed data\")\ndef process_data(self, analysis: str) -&gt; str:\n    return f\"Processing results: {analysis}\"\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.observe","title":"<code>observe = create_step_decorator(StepType.OBSERVE)</code>  <code>module-attribute</code>","text":"<p>Decorator for creating observation/data gathering steps.</p> Example <pre><code>@observe(\"gather\", description=\"Gathers input data\")\ndef gather_data(self, query: str) -&gt; str:\n    return f\"Gathering data for: {query}\"\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.synthesize","title":"<code>synthesize = create_step_decorator(StepType.SYNTHESIZE)</code>  <code>module-attribute</code>","text":"<p>Decorator for creating synthesis/summary steps.</p> Example <pre><code>@synthesize(\"summarize\", description=\"Summarizes results\")\ndef summarize_data(self, data: str) -&gt; str:\n    return f\"Summary of: {data}\"\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.think","title":"<code>think = create_step_decorator(StepType.THINK)</code>  <code>module-attribute</code>","text":"<p>Decorator for creating thinking/analysis steps.</p> Example <pre><code>@think(\"analyze\", description=\"Analyzes input data\")\ndef analyze_data(self, data: str) -&gt; str:\n    return f\"Analysis task: {data}\"\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.BoundRunFunction","title":"<code>BoundRunFunction</code>","text":"<p>Run function bound to a specific agent instance.</p> <p>Maintains the binding between a run function and its agent instance while preserving method attributes and proper execution context.</p> <p>Attributes:</p> Name Type Description <code>func</code> <p>The original function to be bound</p> <code>instance</code> <p>The agent instance to bind to</p> <code>_is_run</code> <p>Flag indicating this is a run method</p> <code>_run_description</code> <code>Optional[str]</code> <p>Optional description of the run behavior</p> Example <p>Create bound function: <pre><code>bound = BoundRunFunction(run_method, agent_instance)\nresult = bound(\"input data\")  # Executes with proper agent binding\n</code></pre></p> Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>class BoundRunFunction:\n    \"\"\"Run function bound to a specific agent instance.\n\n    Maintains the binding between a run function and its agent instance while\n    preserving method attributes and proper execution context.\n\n    Attributes:\n        func: The original function to be bound\n        instance: The agent instance to bind to\n        _is_run: Flag indicating this is a run method\n        _run_description: Optional description of the run behavior\n\n    Example:\n        Create bound function:\n        ```python\n        bound = BoundRunFunction(run_method, agent_instance)\n        result = bound(\"input data\")  # Executes with proper agent binding\n        ```\n    \"\"\"\n\n    def __init__(self, func: Callable[..., Any], instance: Any) -&gt; None:\n        \"\"\"\n        Initialize a bound run function.\n\n        Args:\n            func: The function to bind.\n            instance: The instance to bind the function to.\n        \"\"\"\n        self.func = func\n        self.instance = instance\n        self._is_run = True\n        self._run_description: Optional[str] = None\n        for attr in [\"__name__\", \"__doc__\", \"_is_run\", \"_run_description\"]:\n            if hasattr(func, attr):\n                setattr(self, attr, getattr(func, attr))\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        Execute the function with the bound instance.\n\n        Automatically prepends the bound instance as the first argument (self)\n        when calling the wrapped function.\n\n        Args:\n            *args: Positional arguments to pass to the function.\n            **kwargs: Keyword arguments to pass to the function.\n\n        Returns:\n            Any: The result of executing the bound function.\n        \"\"\"\n        return self.func(self.instance, *args, **kwargs)\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.BoundRunFunction.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Execute the function with the bound instance.</p> <p>Automatically prepends the bound instance as the first argument (self) when calling the wrapped function.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of executing the bound function.</p> Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Execute the function with the bound instance.\n\n    Automatically prepends the bound instance as the first argument (self)\n    when calling the wrapped function.\n\n    Args:\n        *args: Positional arguments to pass to the function.\n        **kwargs: Keyword arguments to pass to the function.\n\n    Returns:\n        Any: The result of executing the bound function.\n    \"\"\"\n    return self.func(self.instance, *args, **kwargs)\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.BoundRunFunction.__init__","title":"<code>__init__(func, instance)</code>","text":"<p>Initialize a bound run function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to bind.</p> required <code>instance</code> <code>Any</code> <p>The instance to bind the function to.</p> required Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>def __init__(self, func: Callable[..., Any], instance: Any) -&gt; None:\n    \"\"\"\n    Initialize a bound run function.\n\n    Args:\n        func: The function to bind.\n        instance: The instance to bind the function to.\n    \"\"\"\n    self.func = func\n    self.instance = instance\n    self._is_run = True\n    self._run_description: Optional[str] = None\n    for attr in [\"__name__\", \"__doc__\", \"_is_run\", \"_run_description\"]:\n        if hasattr(func, attr):\n            setattr(self, attr, getattr(func, attr))\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.RunFunction","title":"<code>RunFunction</code>","text":"<p>A wrapper class for custom run methods in agents.</p> <p>This class implements Python's descriptor protocol to enable proper method binding when the wrapped function is accessed as a class attribute. It ensures that the function behaves correctly as an instance method while maintaining its custom attributes and metadata.</p> <p>Attributes:</p> Name Type Description <code>func</code> <p>The original run function being wrapped.</p> <code>_is_run</code> <p>Flag indicating this is a run method.</p> <code>_run_description</code> <code>Optional[str]</code> <p>Optional description of the run behavior.</p> Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>class RunFunction:\n    \"\"\"\n    A wrapper class for custom run methods in agents.\n\n    This class implements Python's descriptor protocol to enable proper\n    method binding when the wrapped function is accessed as a class\n    attribute. It ensures that the function behaves correctly as an\n    instance method while maintaining its custom attributes and metadata.\n\n    Attributes:\n        func: The original run function being wrapped.\n        _is_run: Flag indicating this is a run method.\n        _run_description: Optional description of the run behavior.\n    \"\"\"\n\n    def __init__(self, func: Callable[..., Any]) -&gt; None:\n        \"\"\"\n        Initialize the run function wrapper.\n\n        Args:\n            func: The run function to wrap.\n        \"\"\"\n        self.func = func\n        self._is_run = True\n        self._run_description: Optional[str] = None\n        wraps(func)(self)\n\n    def __get__(\n        self, obj: Any, objtype: Optional[type] = None\n    ) -&gt; BoundRunFunction:\n        \"\"\"\n        Support descriptor protocol for instance method binding.\n\n        Implements Python's descriptor protocol to create bound methods when\n        the function is accessed through an instance.\n\n        Args:\n            obj: The instance that the method is being accessed from.\n            objtype: The type of the instance (not used).\n\n        Returns:\n            BoundRunFunction: A bound version of the run function.\n\n        Raises:\n            TypeError: If accessed without an instance (obj is None).\n        \"\"\"\n        if obj is None:\n            raise TypeError(\"Cannot access run method without instance\")\n        return BoundRunFunction(self.func, obj)\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        Execute the wrapped run function directly.\n\n        Note: This method is typically only called when the descriptor is used\n        without being bound to an instance, which should raise a TypeError\n        through __get__.\n\n        Args:\n            *args: Positional arguments to pass to the function.\n            **kwargs: Keyword arguments to pass to the function.\n\n        Returns:\n            Any: The result of the run function execution.\n        \"\"\"\n        return self.func(*args, **kwargs)\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.RunFunction.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Execute the wrapped run function directly.</p> <p>Note: This method is typically only called when the descriptor is used without being bound to an instance, which should raise a TypeError through get.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The result of the run function execution.</p> Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Execute the wrapped run function directly.\n\n    Note: This method is typically only called when the descriptor is used\n    without being bound to an instance, which should raise a TypeError\n    through __get__.\n\n    Args:\n        *args: Positional arguments to pass to the function.\n        **kwargs: Keyword arguments to pass to the function.\n\n    Returns:\n        Any: The result of the run function execution.\n    \"\"\"\n    return self.func(*args, **kwargs)\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.RunFunction.__get__","title":"<code>__get__(obj, objtype=None)</code>","text":"<p>Support descriptor protocol for instance method binding.</p> <p>Implements Python's descriptor protocol to create bound methods when the function is accessed through an instance.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The instance that the method is being accessed from.</p> required <code>objtype</code> <code>Optional[type]</code> <p>The type of the instance (not used).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>BoundRunFunction</code> <code>BoundRunFunction</code> <p>A bound version of the run function.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If accessed without an instance (obj is None).</p> Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>def __get__(\n    self, obj: Any, objtype: Optional[type] = None\n) -&gt; BoundRunFunction:\n    \"\"\"\n    Support descriptor protocol for instance method binding.\n\n    Implements Python's descriptor protocol to create bound methods when\n    the function is accessed through an instance.\n\n    Args:\n        obj: The instance that the method is being accessed from.\n        objtype: The type of the instance (not used).\n\n    Returns:\n        BoundRunFunction: A bound version of the run function.\n\n    Raises:\n        TypeError: If accessed without an instance (obj is None).\n    \"\"\"\n    if obj is None:\n        raise TypeError(\"Cannot access run method without instance\")\n    return BoundRunFunction(self.func, obj)\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.RunFunction.__init__","title":"<code>__init__(func)</code>","text":"<p>Initialize the run function wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The run function to wrap.</p> required Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>def __init__(self, func: Callable[..., Any]) -&gt; None:\n    \"\"\"\n    Initialize the run function wrapper.\n\n    Args:\n        func: The run function to wrap.\n    \"\"\"\n    self.func = func\n    self._is_run = True\n    self._run_description: Optional[str] = None\n    wraps(func)(self)\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.StepFunction","title":"<code>StepFunction</code>","text":"<p>A wrapper class for step functions that maintains metadata and execution context.</p> <p>Wraps agent step functions while preserving their metadata and allowing attachment of additional step information through the _step_info attribute.</p> <p>Attributes:</p> Name Type Description <code>func</code> <p>The original step function being wrapped</p> <code>_step_info</code> <code>Optional[Step]</code> <p>Optional Step instance containing step metadata</p> Example <p>Create a wrapped step function: <pre><code>def example_step(self, input: str) -&gt; str:\n    return f\"Processing: {input}\"\n\nwrapped = StepFunction(example_step)\nwrapped._step_info = Step.create(\n    func=example_step,\n    step_type=StepType.THINK,\n    name=\"example\"\n)\n</code></pre></p> Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>class StepFunction:\n    \"\"\"A wrapper class for step functions that maintains\n    metadata and execution context.\n\n    Wraps agent step functions while preserving their metadata and allowing\n    attachment of additional step information through the _step_info attribute.\n\n    Attributes:\n        func: The original step function being wrapped\n        _step_info: Optional Step instance containing step metadata\n\n    Example:\n        Create a wrapped step function:\n        ```python\n        def example_step(self, input: str) -&gt; str:\n            return f\"Processing: {input}\"\n\n        wrapped = StepFunction(example_step)\n        wrapped._step_info = Step.create(\n            func=example_step,\n            step_type=StepType.THINK,\n            name=\"example\"\n        )\n        ```\n    \"\"\"\n\n    def __init__(self, func: Callable[..., str]) -&gt; None:\n        \"\"\"\n        Initialize the step function wrapper.\n\n        Args:\n            func: The step function to wrap.\n        \"\"\"\n        self.func = func\n        self._step_info: Optional[Step] = None\n        wraps(func)(self)\n\n    def __call__(\n        self, instance: Optional[Any], *args: Any, **kwargs: Any\n    ) -&gt; Any:\n        \"\"\"\n        Execute the step function with instance binding.\n\n        Args:\n            instance: The agent instance the step is being called from.\n                      If None, executes the raw function without engine\n                      involvement.\n            *args: Positional arguments to pass to the step\n            **kwargs: Keyword arguments to pass to the step\n\n        Returns:\n            Any: Either:\n                - Result from execution_engine if called from instance\n                  with step info\n                - Raw function result if called without instance or step info\n\n        Example:\n            When called from an agent instance:\n            ```python\n            result = step(\n                agent_instance,\n                \"input data\"\n            )  # Uses execution engine\n            ```\n\n            When called directly:\n            ```python\n            result = step(\n                None,\n                \"input data\"\n            )  # Calls raw function\n            ```\n        \"\"\"\n        if instance is None:\n            return self.func(*args, **kwargs)\n\n        if self._step_info:\n            return instance.execution_engine.execute_step(\n                self._step_info, *args, **kwargs\n            )\n        return self.func(instance, *args, **kwargs)\n\n    def __get__(\n        self, instance: Optional[object], owner: Optional[type]\n    ) -&gt; Union[\"StepFunction\", Callable[..., str]]:\n        \"\"\"\n        Make steps behave like instance methods via the descriptor protocol.\n\n        When a step is accessed on an agent instance, returns\n        a bound method that automatically passes the instance through\n        __call__ for engine execution. When accessed on the class,\n        returns the StepFunction itself.\n\n        Args:\n            instance: The agent instance accessing the step.\n                      None if accessed on class.\n            owner: The agent class the step is defined on.\n\n        Returns:\n            Union[StepFunction, Callable[..., str]]:\n                - If accessed on class (instance=None), returns\n                  the StepFunction itself\n                - If accessed on instance, returns a callable\n                  that passes the instance through __call__\n                  for engine-managed execution\n\n        Example:\n            ```python\n            # On instance - returns bound method that uses engine\n            agent.analyze_data(\"input\")\n\n            # On class - returns raw StepFunction\n            AgentClass.analyze_data\n            ```\n        \"\"\"\n        if instance is None:\n            return self\n        return lambda *args, **kwargs: self(instance, *args, **kwargs)\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.StepFunction.__call__","title":"<code>__call__(instance, *args, **kwargs)</code>","text":"<p>Execute the step function with instance binding.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Optional[Any]</code> <p>The agent instance the step is being called from.       If None, executes the raw function without engine       involvement.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the step</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the step</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Either: - Result from execution_engine if called from instance   with step info - Raw function result if called without instance or step info</p> Example <p>When called from an agent instance: <pre><code>result = step(\n    agent_instance,\n    \"input data\"\n)  # Uses execution engine\n</code></pre></p> <p>When called directly: <pre><code>result = step(\n    None,\n    \"input data\"\n)  # Calls raw function\n</code></pre></p> Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>def __call__(\n    self, instance: Optional[Any], *args: Any, **kwargs: Any\n) -&gt; Any:\n    \"\"\"\n    Execute the step function with instance binding.\n\n    Args:\n        instance: The agent instance the step is being called from.\n                  If None, executes the raw function without engine\n                  involvement.\n        *args: Positional arguments to pass to the step\n        **kwargs: Keyword arguments to pass to the step\n\n    Returns:\n        Any: Either:\n            - Result from execution_engine if called from instance\n              with step info\n            - Raw function result if called without instance or step info\n\n    Example:\n        When called from an agent instance:\n        ```python\n        result = step(\n            agent_instance,\n            \"input data\"\n        )  # Uses execution engine\n        ```\n\n        When called directly:\n        ```python\n        result = step(\n            None,\n            \"input data\"\n        )  # Calls raw function\n        ```\n    \"\"\"\n    if instance is None:\n        return self.func(*args, **kwargs)\n\n    if self._step_info:\n        return instance.execution_engine.execute_step(\n            self._step_info, *args, **kwargs\n        )\n    return self.func(instance, *args, **kwargs)\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.StepFunction.__get__","title":"<code>__get__(instance, owner)</code>","text":"<p>Make steps behave like instance methods via the descriptor protocol.</p> <p>When a step is accessed on an agent instance, returns a bound method that automatically passes the instance through call for engine execution. When accessed on the class, returns the StepFunction itself.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Optional[object]</code> <p>The agent instance accessing the step.       None if accessed on class.</p> required <code>owner</code> <code>Optional[type]</code> <p>The agent class the step is defined on.</p> required <p>Returns:</p> Type Description <code>Union[StepFunction, Callable[..., str]]</code> <p>Union[StepFunction, Callable[..., str]]: - If accessed on class (instance=None), returns   the StepFunction itself - If accessed on instance, returns a callable   that passes the instance through call   for engine-managed execution</p> Example <pre><code># On instance - returns bound method that uses engine\nagent.analyze_data(\"input\")\n\n# On class - returns raw StepFunction\nAgentClass.analyze_data\n</code></pre> Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>def __get__(\n    self, instance: Optional[object], owner: Optional[type]\n) -&gt; Union[\"StepFunction\", Callable[..., str]]:\n    \"\"\"\n    Make steps behave like instance methods via the descriptor protocol.\n\n    When a step is accessed on an agent instance, returns\n    a bound method that automatically passes the instance through\n    __call__ for engine execution. When accessed on the class,\n    returns the StepFunction itself.\n\n    Args:\n        instance: The agent instance accessing the step.\n                  None if accessed on class.\n        owner: The agent class the step is defined on.\n\n    Returns:\n        Union[StepFunction, Callable[..., str]]:\n            - If accessed on class (instance=None), returns\n              the StepFunction itself\n            - If accessed on instance, returns a callable\n              that passes the instance through __call__\n              for engine-managed execution\n\n    Example:\n        ```python\n        # On instance - returns bound method that uses engine\n        agent.analyze_data(\"input\")\n\n        # On class - returns raw StepFunction\n        AgentClass.analyze_data\n        ```\n    \"\"\"\n    if instance is None:\n        return self\n    return lambda *args, **kwargs: self(instance, *args, **kwargs)\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.StepFunction.__init__","title":"<code>__init__(func)</code>","text":"<p>Initialize the step function wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., str]</code> <p>The step function to wrap.</p> required Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>def __init__(self, func: Callable[..., str]) -&gt; None:\n    \"\"\"\n    Initialize the step function wrapper.\n\n    Args:\n        func: The step function to wrap.\n    \"\"\"\n    self.func = func\n    self._step_info: Optional[Step] = None\n    wraps(func)(self)\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.create_step_decorator","title":"<code>create_step_decorator(step_type)</code>","text":"<p>Generate a decorator for defining workflow steps of a specific type.</p> <p>Creates specialized decorators (like @think, @act) that mark methods as workflow steps with specific configurations and types.</p> <p>Parameters:</p> Name Type Description Default <code>step_type</code> <code>StepType</code> <p>The type of step (THINK, ACT, OBSERVE, SYNTHESIZE)        this decorator creates</p> required <p>Returns:</p> Type Description <code>Callable[..., Union[StepFunction, Callable[[Callable[..., str]], StepFunction]]]</code> <p>A decorator function that can be used to mark methods as workflow steps</p> Example <p>Create custom step decorator: <pre><code>analyze_step = create_step_decorator(StepType.THINK)\n\nclass CustomAgent(Agent):\n    @analyze_step(\n        \"analyze_data\",\n        description=\"Analyzes input data\",\n        tool_confidence=0.8\n    )\n    def analyze(self, data: str) -&gt; str:\n        return f\"Please analyze: {data}\"\n</code></pre></p> Notes <ul> <li>Generated decorators support both parameterized and bare usage</li> <li>Decorators handle both tool selection and LLM configuration</li> <li>Step type influences default configuration values</li> </ul> Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>def create_step_decorator(\n    step_type: StepType,\n) -&gt; Callable[\n    ..., Union[StepFunction, Callable[[Callable[..., str]], StepFunction]]\n]:\n    \"\"\"Generate a decorator for defining workflow steps of a specific type.\n\n    Creates specialized decorators (like @think, @act) that mark methods as\n    workflow steps with specific configurations and types.\n\n    Args:\n        step_type: The type of step (THINK, ACT, OBSERVE, SYNTHESIZE)\n                   this decorator creates\n\n    Returns:\n        A decorator function that can be used to mark methods as workflow steps\n\n    Example:\n        Create custom step decorator:\n        ```python\n        analyze_step = create_step_decorator(StepType.THINK)\n\n        class CustomAgent(Agent):\n            @analyze_step(\n                \"analyze_data\",\n                description=\"Analyzes input data\",\n                tool_confidence=0.8\n            )\n            def analyze(self, data: str) -&gt; str:\n                return f\"Please analyze: {data}\"\n        ```\n\n    Notes:\n        - Generated decorators support both parameterized and bare usage\n        - Decorators handle both tool selection and LLM configuration\n        - Step type influences default configuration values\n    \"\"\"\n\n    def decorator(\n        name: Optional[Union[str, Callable[..., str]]] = None,\n        *,\n        model: Optional[Union[str, Dict[str, Any], ModelConfig]] = None,\n        description: Optional[str] = None,\n        send_to_llm: bool = True,\n        stream: bool = False,\n        json_output: bool = False,\n        use_tools: bool = True,\n        tool_selection_config: Optional[ToolSelectionConfig] = None,\n        tool_confidence: Optional[float] = None,\n        tool_model: Optional[Union[str, Dict[str, Any], ModelConfig]] = None,\n        max_tools_per_step: Optional[int] = None,\n        step_config: Optional[StepConfig] = None,\n    ) -&gt; Union[StepFunction, Callable[[Callable[..., str]], StepFunction]]:\n        \"\"\"\n        Core decorator implementation for workflow steps.\n\n        This decorator configures how a step function should be executed\n        within the workflow, including its interaction with the LLM and\n        tool selection behavior.\n\n        Args:\n            name: The name of the step. If omitted, the function name is used.\n                  Can be the function itself when used as a bare decorator.\n            model: Model configuration for this specific step. Can be:\n                - A string (model name)\n                - A dictionary of model parameters\n                - A ModelConfig instance\n                If not provided, uses the agent's default model.\n            description: Human-readable description of what this step does.\n            send_to_llm: Whether this step's output should be sent to the\n                         language model. Set to False for steps that process\n                         data locally.\n            stream: Whether to stream the LLM's response\n            json_output: Whether the LLM should format its response as JSON.\n            use_tools: Whether this step should use automatic tool selection.\n            tool_selection_config: Complete tool selection configuration for\n                                   this step. Mutually exclusive with\n                                   individual tool parameters.\n            tool_confidence: Minimum confidence for tool selection (0.0-1.0).\n                             Overrides agent's default threshold for this step.\n            tool_model: Specific model to use for tool selection in this step.\n                       Can be a string, dict, or ModelConfig.\n            max_tools_per_step: Maximum number of tools to use in this step.\n                               Overrides the agent's default for this step.\n\n        Returns:\n            A decorated step function that will be executed\n            as part of the workflow\n\n        Raises:\n            ValueError: If both tool_selection_config and individual tool\n                        parameters are provided or if the configuration is\n                        otherwise invalid\n\n        Example:\n            Basic usage with tool selection:\n            ```python\n            class MyAgent(Agent):\n                @think(\n                    \"analyze\",\n                    description=\"Analyzes input data\",\n                    use_tools=True,\n                    tool_confidence=0.8,\n                    tool_model=\"gpt-4\"\n                )\n                def analyze_data(self, input_data: str) -&gt; str:\n                    return f\"Please analyze this data: {input_data}\"\n\n                @act(\n                    \"process\",\n                    description=\"Processes analysis results\",\n                    use_tools=False  # Disable tool selection for this step\n                )\n                def process_results(self, analysis: str) -&gt; str:\n                    return f\"Process these results: {analysis}\"\n            ```\n\n            Using complete tool selection configuration:\n            ```python\n            class MyAgent(Agent):\n                @think(\n                    \"analyze\",\n                    description=\"Complex analysis step\",\n                    tool_selection_config=ToolSelectionConfig(\n                        confidence_threshold=0.8,\n                        max_tools_per_step=3,\n                        prompt_template=\"Custom tool selection prompt: {task}\"\n                    )\n                )\n                def complex_analysis(self, data: str) -&gt; str:\n                    return f\"Perform complex analysis on: {data}\"\n            ```\n\n            Using as a bare decorator:\n            ```python\n            class MyAgent(Agent):\n                @think  # No parameters, uses defaults\n                def simple_analysis(self, data: str) -&gt; str:\n                    return f\"Analyze: {data}\"\n            ```\n\n        Note:\n            - Tool parameters are mutually exclusive with tool_selection_config\n            - The step's model config takes precedence over the agent's default\n            - When used as a bare decorator, parameters use the default values\n        \"\"\"\n\n        def wrapper(func: Callable[..., str]) -&gt; StepFunction:\n            \"\"\"Inner wrapper that creates the StepFunction instance.\"\"\"\n            wrapped = StepFunction(func)\n            step_name = name if isinstance(name, str) else func.__name__\n\n            if tool_selection_config and any(\n                x is not None\n                for x in [tool_confidence, tool_model, max_tools_per_step]\n            ):\n                raise ValueError(\n                    \"Cannot specify both tool_selection_config and \"\n                    \"individual tool parameters \"\n                    \"(tool_confidence, tool_model, max_tools_per_step)\"\n                )\n\n            final_tool_config = None\n            if tool_selection_config:\n                final_tool_config = tool_selection_config\n            elif any(\n                x is not None for x in [tool_confidence, max_tools_per_step]\n            ):\n                config_params = {}\n                if tool_confidence is not None:\n                    config_params[\"confidence_threshold\"] = tool_confidence\n                if max_tools_per_step is not None:\n                    config_params[\"max_tools_per_step\"] = max_tools_per_step\n                final_tool_config = ToolSelectionConfig.create(**config_params)\n\n            tool_model_config = None\n            if tool_model is not None:\n                if isinstance(tool_model, str):\n                    tool_model_config = ModelConfig(name=tool_model)\n                elif isinstance(tool_model, dict):\n                    tool_model_config = ModelConfig(**tool_model)\n                else:\n                    tool_model_config = tool_model\n\n            wrapped._step_info = Step.create(\n                func=func,\n                step_type=step_type,\n                name=step_name,\n                description=description,\n                llm_config=_create_model_config(model, step_type)\n                if model and send_to_llm\n                else None,\n                send_to_llm=send_to_llm,\n                stream=stream,\n                json_output=json_output,\n                use_tools=use_tools,\n                tool_selection_config=final_tool_config,\n                tool_model=tool_model_config,\n                step_config=step_config,\n            )\n            return wrapped\n\n        if callable(name):\n            return wrapper(name)\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/agent/steps/decorators/#clientai.agent.steps.decorators.run","title":"<code>run(func=None, *, description=None)</code>","text":"<pre><code>run(*, description: Optional[str] = None) -&gt; Callable[[Callable[..., T]], RunFunction]\n</code></pre><pre><code>run(func: Callable[..., T]) -&gt; RunFunction\n</code></pre> <p>Decorator for defining a custom <code>run</code> method in an agent class.</p> <p>Marks a method as the custom run implementation for an agent, optionally with a description of its behavior.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Optional[Callable[..., T]]</code> <p>The function to decorate (when used without parameters)</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Optional description of the custom run behavior</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Callable[[Callable[..., T]], RunFunction], RunFunction]</code> <p>Either a decorator function or the decorated function</p> Example <p>Define custom run methods: <pre><code>class CustomAgent(Agent):\n    @run(description=\"Custom workflow execution\")\n    def custom_run(self, input_data: Any) -&gt; Any:\n        # Custom implementation\n        return f\"Custom execution for {input_data}\"\n\n    # Or without parameters\n    @run\n    def another_run(self, data: str) -&gt; str:\n        return f\"Processing: {data}\"\n</code></pre></p> Source code in <code>clientai/agent/steps/decorators.py</code> <pre><code>def run(\n    func: Optional[Callable[..., T]] = None,\n    *,\n    description: Optional[str] = None,\n) -&gt; Union[Callable[[Callable[..., T]], RunFunction], RunFunction]:\n    \"\"\"Decorator for defining a custom `run` method in an agent class.\n\n    Marks a method as the custom run implementation for an agent,\n    optionally with a description of its behavior.\n\n    Args:\n        func: The function to decorate (when used without parameters)\n        description: Optional description of the custom run behavior\n\n    Returns:\n        Either a decorator function or the decorated function\n\n    Example:\n        Define custom run methods:\n        ```python\n        class CustomAgent(Agent):\n            @run(description=\"Custom workflow execution\")\n            def custom_run(self, input_data: Any) -&gt; Any:\n                # Custom implementation\n                return f\"Custom execution for {input_data}\"\n\n            # Or without parameters\n            @run\n            def another_run(self, data: str) -&gt; str:\n                return f\"Processing: {data}\"\n        ```\n    \"\"\"\n\n    def decorator(f: Callable[..., T]) -&gt; RunFunction:\n        wrapped = RunFunction(f)\n        wrapped._run_description = description\n        return wrapped\n\n    if func is None:\n        return decorator\n\n    return decorator(func)\n</code></pre>"},{"location":"api/agent/steps/step/","title":"Step Class API Reference","text":"<p>The <code>Step</code> class represents a single step in an agent's workflow, encapsulating its function, type, and configuration.</p>"},{"location":"api/agent/steps/step/#class-definition","title":"Class Definition","text":"<p>Represents a step in the agent's workflow, encapsulating its function, type, metadata, and configuration.</p> <p>Attributes:</p> Name Type Description <code>func</code> <code>Callable[..., Any]</code> <p>The function representing the step logic.</p> <code>step_type</code> <code>StepType</code> <p>The type of step (e.g., THINK, ACT).</p> <code>name</code> <code>str</code> <p>The name of the step.</p> <code>description</code> <code>Optional[str]</code> <p>A description of the step's purpose.</p> <code>llm_config</code> <code>Optional[ModelConfig]</code> <p>LLM configuration if the step interacts with an LLM.</p> <code>send_to_llm</code> <code>bool</code> <p>Whether the step sends its data to an LLM. Default True.</p> <code>stream</code> <code>bool</code> <p>Whether to stream the LLM's response</p> <code>json_output</code> <code>bool</code> <p>Whether the LLM should return a JSON. Default False.</p> <code>use_tools</code> <code>bool</code> <p>Whether tool selection is enabled for step. Default True.</p> <code>tool_selection_config</code> <code>Optional[ToolSelectionConfig]</code> <p>Configuration for tool selection behavior.</p> <code>tool_model</code> <code>Optional[ModelConfig]</code> <p>Optional specific model to use for tool selection.</p> <code>config</code> <code>StepConfig</code> <p>Configuration settings for the step.</p> <code>metadata</code> <code>Optional[FunctionMetadata]</code> <p>Metadata extracted from the step function.</p> <code>tool_decisions</code> <code>Optional[List[ToolCallDecision]]</code> <p>Records of tool selection and execution decisions.</p> Example <p>Create a step with tool configuration: <pre><code>step = Step.create(\n    func=example_function,\n    step_type=StepType.THINK,\n    name=\"analyze\",\n    use_tools=True,\n    tool_selection_config=ToolSelectionConfig(\n        confidence_threshold=0.8\n    ),\n    tool_model=ModelConfig(\n        name=\"llama-2\",\n        temperature=0.0\n    )\n)\n</code></pre></p> Source code in <code>clientai/agent/steps/base.py</code> <pre><code>@dataclass(frozen=True)\nclass Step:\n    \"\"\"\n    Represents a step in the agent's workflow, encapsulating its function,\n    type, metadata, and configuration.\n\n    Attributes:\n        func: The function representing the step logic.\n        step_type: The type of step (e.g., THINK, ACT).\n        name: The name of the step.\n        description: A description of the step's purpose.\n        llm_config: LLM configuration if the step interacts with an LLM.\n        send_to_llm: Whether the step sends its data to an LLM. Default True.\n        stream: Whether to stream the LLM's response\n        json_output: Whether the LLM should return a JSON. Default False.\n        use_tools: Whether tool selection is enabled for step. Default True.\n        tool_selection_config: Configuration for tool selection behavior.\n        tool_model: Optional specific model to use for tool selection.\n        config: Configuration settings for the step.\n        metadata: Metadata extracted from the step function.\n        tool_decisions: Records of tool selection and execution decisions.\n\n    Example:\n        Create a step with tool configuration:\n        ```python\n        step = Step.create(\n            func=example_function,\n            step_type=StepType.THINK,\n            name=\"analyze\",\n            use_tools=True,\n            tool_selection_config=ToolSelectionConfig(\n                confidence_threshold=0.8\n            ),\n            tool_model=ModelConfig(\n                name=\"llama-2\",\n                temperature=0.0\n            )\n        )\n        ```\n    \"\"\"\n\n    func: Callable[..., Any]\n    step_type: StepType\n    name: str\n    description: Optional[str] = None\n    llm_config: Optional[ModelConfig] = None\n    send_to_llm: bool = True\n    stream: bool = False\n    json_output: bool = False\n    use_tools: bool = True\n    tool_selection_config: Optional[ToolSelectionConfig] = None\n    tool_model: Optional[ModelConfig] = None\n    metadata: Optional[FunctionMetadata] = None\n    tool_decisions: Optional[List[ToolCallDecision]] = None\n    config: StepConfig = field(default_factory=StepConfig)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate step values after initialization.\"\"\"\n        self._validate_function(self.func)\n        self._validate_name(self.name)\n\n    @staticmethod\n    def _validate_function(func: Callable[..., Any]) -&gt; None:\n        \"\"\"\n        Validate the step function's signature and return type.\n\n        Ensures that the function is callable, has a return type annotation,\n        and that the return type is a string.\n\n        Args:\n            func: The function to validate.\n\n        Raises:\n            ValueError: If the function is not callable.\n\n        Example:\n            Validate a function:\n            ```python\n            def example_function(input_data: str) -&gt; str:\n                return f\"Processed: {input_data}\"\n\n            validated_func = Step.validate_function(example_function)\n            ```\n        \"\"\"\n        if not callable(func):\n            raise ValueError(\"func must be a callable\")\n\n    @staticmethod\n    def _validate_name(name: str) -&gt; None:\n        \"\"\"\n        Validate the step's name to ensure it is non-empty\n        and a valid Python identifier.\n\n        Args:\n            name: The name of the step to validate.\n\n        Raises:\n            ValueError: If the name is empty or not a valid identifier.\n\n        Example:\n            Validate a step name:\n            ```python\n            valid_name = Step.validate_name(\"valid_step_name\")\n            print(valid_name)  # Output: \"valid_step_name\"\n\n            Step.validate_name(\"\")  # Raises ValueError\n            ```\n        \"\"\"\n        if not name:\n            raise ValueError(\"Step name cannot be empty\")\n        if not name.isidentifier():\n            raise ValueError(\n                f\"Step name '{name}' must be a valid Python identifier\"\n            )\n\n    def is_compatible_with(self, other: \"Step\") -&gt; bool:\n        \"\"\"Check if this step's input is compatible with another step's output.\n\n        Determines if steps can be connected in a workflow by comparing their\n        input/output types.\n\n        Args:\n            other: The step to check compatibility with\n\n        Returns:\n            bool: True if this step can accept the other step's output type\n\n        Example:\n            Check step compatibility:\n            ```python\n            step1 = Step.create(\n                func=process_text,  # returns str\n                step_type=StepType.THINK,\n                name=\"process\"\n            )\n            step2 = Step.create(\n                func=analyze_text,  # takes str input\n                step_type=StepType.ACT,\n                name=\"analyze\"\n            )\n\n            if step2.is_compatible_with(step1):\n                print(\"Can connect process -&gt; analyze\")\n            ```\n        \"\"\"\n        if not other.metadata or not self.metadata:\n            return False\n\n        arg_types = list(self.metadata.arg_types.values())\n        if not arg_types:\n            return False\n\n        return str(arg_types[0]) == other.metadata.return_type\n\n    def can_execute_with(self, input_data: Any) -&gt; bool:\n        \"\"\"Check if the step can execute with the provided input.\n\n        Args:\n            input_data: The input data to validate\n\n        Returns:\n            bool: True if the input matches the step's expected input type\n\n        Example:\n            Validate input data:\n            ```python\n            step = Step.create(\n                func=process_numbers,  # takes List[int]\n                step_type=StepType.ACT,\n                name=\"process\"\n            )\n\n            data = [1, 2, 3]\n            if step.can_execute_with(data):\n                print(\"Input data is valid\")\n            ```\n        \"\"\"\n        if not self.metadata or not self.metadata.arg_types:\n            return True\n\n        first_arg_type = next(iter(self.metadata.arg_types.values()))\n        return isinstance(input_data, first_arg_type)\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Provide a human-readable string representation of the step.\n\n        Returns:\n            str: A description of the step, including its name, type,\n                 and optional details.\n\n        Example:\n            Print a step's string representation:\n            ```python\n            step = Step.create(\n                func=example_function,\n                step_type=StepType.THINK,\n                name=\"example_step\"\n            )\n            print(str(step))\n            # Output: \"Step(example_step) | Type: THINK | Description: ...\"\n            ```\n        \"\"\"\n        parts = [\n            f\"Step({self.name})\",\n            f\"Type: {self.step_type.name}\",\n        ]\n        if self.description:\n            parts.append(f\"Description: {self.description}\")\n        if self.llm_config:\n            parts.append(f\"Model: {self.llm_config.name}\")\n        if self.json_output:\n            parts.append(\"JSON output enabled\")\n        return \" | \".join(parts)\n\n    @classmethod\n    def create(\n        cls,\n        func: Callable[..., Any],\n        step_type: StepType,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        llm_config: Optional[ModelConfig] = None,\n        send_to_llm: Optional[bool] = None,\n        stream: bool = False,\n        json_output: bool = False,\n        use_tools: bool = True,\n        tool_selection_config: Optional[ToolSelectionConfig] = None,\n        tool_model: Optional[ModelConfig] = None,\n        step_config: Optional[StepConfig] = None,\n    ) -&gt; \"Step\":\n        \"\"\"Create and validate a new step.\n\n        Factory method for creating steps with\n        comprehensive configuration options.\n\n        Args:\n            func: Function implementing the step logic\n            step_type: Type classification for the step\n            name: Optional custom name (defaults to function name)\n            description: Optional step description\n            llm_config: Optional LLM configuration\n            send_to_llm: Whether to send step output to LLM\n            stream: Whether to stream LLM responses\n            json_output: Whether LLM should return JSON\n            use_tools: Whether to enable tool selection\n            tool_selection_config: Optional tool selection configuration\n            tool_model: Optional specific model for tool selection\n            step_config: Optional step-specific configuration\n\n        Returns:\n            Step: Validated step instance\n\n        Raises:\n            ValueError: If step name is invalid or function\n                        lacks required annotations\n\n        Example:\n            Create step with tool selection:\n            ```python\n            step = Step.create(\n                func=analyze_data,\n                step_type=StepType.THINK,\n                name=\"analyze\",\n                description=\"Analyzes input data\",\n                use_tools=True,\n                tool_selection_config=ToolSelectionConfig(\n                    confidence_threshold=0.8\n                )\n            )\n            ```\n\n            Create step with custom model:\n            ```python\n            step = Step.create(\n                func=process_data,\n                step_type=StepType.ACT,\n                llm_config=ModelConfig(\n                    name=\"gpt-4\",\n                    temperature=0.7\n                ),\n                stream=True\n            )\n            ```\n        \"\"\"\n        metadata = FunctionMetadata.from_function(func)\n        return cls(\n            func=func,\n            step_type=step_type,\n            name=name or func.__name__,\n            description=description or func.__doc__,\n            llm_config=llm_config,\n            send_to_llm=send_to_llm if send_to_llm is not None else True,\n            stream=stream,\n            json_output=json_output,\n            use_tools=use_tools,\n            tool_selection_config=tool_selection_config,\n            tool_model=tool_model,\n            config=step_config or StepConfig(),\n            metadata=metadata,\n        )\n</code></pre>"},{"location":"api/agent/steps/step/#clientai.agent.steps.Step.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate step values after initialization.</p> Source code in <code>clientai/agent/steps/base.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate step values after initialization.\"\"\"\n    self._validate_function(self.func)\n    self._validate_name(self.name)\n</code></pre>"},{"location":"api/agent/steps/step/#clientai.agent.steps.Step.__str__","title":"<code>__str__()</code>","text":"<p>Provide a human-readable string representation of the step.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A description of the step, including its name, type,  and optional details.</p> Example <p>Print a step's string representation: <pre><code>step = Step.create(\n    func=example_function,\n    step_type=StepType.THINK,\n    name=\"example_step\"\n)\nprint(str(step))\n# Output: \"Step(example_step) | Type: THINK | Description: ...\"\n</code></pre></p> Source code in <code>clientai/agent/steps/base.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Provide a human-readable string representation of the step.\n\n    Returns:\n        str: A description of the step, including its name, type,\n             and optional details.\n\n    Example:\n        Print a step's string representation:\n        ```python\n        step = Step.create(\n            func=example_function,\n            step_type=StepType.THINK,\n            name=\"example_step\"\n        )\n        print(str(step))\n        # Output: \"Step(example_step) | Type: THINK | Description: ...\"\n        ```\n    \"\"\"\n    parts = [\n        f\"Step({self.name})\",\n        f\"Type: {self.step_type.name}\",\n    ]\n    if self.description:\n        parts.append(f\"Description: {self.description}\")\n    if self.llm_config:\n        parts.append(f\"Model: {self.llm_config.name}\")\n    if self.json_output:\n        parts.append(\"JSON output enabled\")\n    return \" | \".join(parts)\n</code></pre>"},{"location":"api/agent/steps/step/#clientai.agent.steps.Step.can_execute_with","title":"<code>can_execute_with(input_data)</code>","text":"<p>Check if the step can execute with the provided input.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>The input data to validate</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the input matches the step's expected input type</p> Example <p>Validate input data: <pre><code>step = Step.create(\n    func=process_numbers,  # takes List[int]\n    step_type=StepType.ACT,\n    name=\"process\"\n)\n\ndata = [1, 2, 3]\nif step.can_execute_with(data):\n    print(\"Input data is valid\")\n</code></pre></p> Source code in <code>clientai/agent/steps/base.py</code> <pre><code>def can_execute_with(self, input_data: Any) -&gt; bool:\n    \"\"\"Check if the step can execute with the provided input.\n\n    Args:\n        input_data: The input data to validate\n\n    Returns:\n        bool: True if the input matches the step's expected input type\n\n    Example:\n        Validate input data:\n        ```python\n        step = Step.create(\n            func=process_numbers,  # takes List[int]\n            step_type=StepType.ACT,\n            name=\"process\"\n        )\n\n        data = [1, 2, 3]\n        if step.can_execute_with(data):\n            print(\"Input data is valid\")\n        ```\n    \"\"\"\n    if not self.metadata or not self.metadata.arg_types:\n        return True\n\n    first_arg_type = next(iter(self.metadata.arg_types.values()))\n    return isinstance(input_data, first_arg_type)\n</code></pre>"},{"location":"api/agent/steps/step/#clientai.agent.steps.Step.create","title":"<code>create(func, step_type, name=None, description=None, llm_config=None, send_to_llm=None, stream=False, json_output=False, use_tools=True, tool_selection_config=None, tool_model=None, step_config=None)</code>  <code>classmethod</code>","text":"<p>Create and validate a new step.</p> <p>Factory method for creating steps with comprehensive configuration options.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>Function implementing the step logic</p> required <code>step_type</code> <code>StepType</code> <p>Type classification for the step</p> required <code>name</code> <code>Optional[str]</code> <p>Optional custom name (defaults to function name)</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Optional step description</p> <code>None</code> <code>llm_config</code> <code>Optional[ModelConfig]</code> <p>Optional LLM configuration</p> <code>None</code> <code>send_to_llm</code> <code>Optional[bool]</code> <p>Whether to send step output to LLM</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream LLM responses</p> <code>False</code> <code>json_output</code> <code>bool</code> <p>Whether LLM should return JSON</p> <code>False</code> <code>use_tools</code> <code>bool</code> <p>Whether to enable tool selection</p> <code>True</code> <code>tool_selection_config</code> <code>Optional[ToolSelectionConfig]</code> <p>Optional tool selection configuration</p> <code>None</code> <code>tool_model</code> <code>Optional[ModelConfig]</code> <p>Optional specific model for tool selection</p> <code>None</code> <code>step_config</code> <code>Optional[StepConfig]</code> <p>Optional step-specific configuration</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Step</code> <code>Step</code> <p>Validated step instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If step name is invalid or function         lacks required annotations</p> Example <p>Create step with tool selection: <pre><code>step = Step.create(\n    func=analyze_data,\n    step_type=StepType.THINK,\n    name=\"analyze\",\n    description=\"Analyzes input data\",\n    use_tools=True,\n    tool_selection_config=ToolSelectionConfig(\n        confidence_threshold=0.8\n    )\n)\n</code></pre></p> <p>Create step with custom model: <pre><code>step = Step.create(\n    func=process_data,\n    step_type=StepType.ACT,\n    llm_config=ModelConfig(\n        name=\"gpt-4\",\n        temperature=0.7\n    ),\n    stream=True\n)\n</code></pre></p> Source code in <code>clientai/agent/steps/base.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    func: Callable[..., Any],\n    step_type: StepType,\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n    llm_config: Optional[ModelConfig] = None,\n    send_to_llm: Optional[bool] = None,\n    stream: bool = False,\n    json_output: bool = False,\n    use_tools: bool = True,\n    tool_selection_config: Optional[ToolSelectionConfig] = None,\n    tool_model: Optional[ModelConfig] = None,\n    step_config: Optional[StepConfig] = None,\n) -&gt; \"Step\":\n    \"\"\"Create and validate a new step.\n\n    Factory method for creating steps with\n    comprehensive configuration options.\n\n    Args:\n        func: Function implementing the step logic\n        step_type: Type classification for the step\n        name: Optional custom name (defaults to function name)\n        description: Optional step description\n        llm_config: Optional LLM configuration\n        send_to_llm: Whether to send step output to LLM\n        stream: Whether to stream LLM responses\n        json_output: Whether LLM should return JSON\n        use_tools: Whether to enable tool selection\n        tool_selection_config: Optional tool selection configuration\n        tool_model: Optional specific model for tool selection\n        step_config: Optional step-specific configuration\n\n    Returns:\n        Step: Validated step instance\n\n    Raises:\n        ValueError: If step name is invalid or function\n                    lacks required annotations\n\n    Example:\n        Create step with tool selection:\n        ```python\n        step = Step.create(\n            func=analyze_data,\n            step_type=StepType.THINK,\n            name=\"analyze\",\n            description=\"Analyzes input data\",\n            use_tools=True,\n            tool_selection_config=ToolSelectionConfig(\n                confidence_threshold=0.8\n            )\n        )\n        ```\n\n        Create step with custom model:\n        ```python\n        step = Step.create(\n            func=process_data,\n            step_type=StepType.ACT,\n            llm_config=ModelConfig(\n                name=\"gpt-4\",\n                temperature=0.7\n            ),\n            stream=True\n        )\n        ```\n    \"\"\"\n    metadata = FunctionMetadata.from_function(func)\n    return cls(\n        func=func,\n        step_type=step_type,\n        name=name or func.__name__,\n        description=description or func.__doc__,\n        llm_config=llm_config,\n        send_to_llm=send_to_llm if send_to_llm is not None else True,\n        stream=stream,\n        json_output=json_output,\n        use_tools=use_tools,\n        tool_selection_config=tool_selection_config,\n        tool_model=tool_model,\n        config=step_config or StepConfig(),\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"api/agent/steps/step/#clientai.agent.steps.Step.is_compatible_with","title":"<code>is_compatible_with(other)</code>","text":"<p>Check if this step's input is compatible with another step's output.</p> <p>Determines if steps can be connected in a workflow by comparing their input/output types.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Step</code> <p>The step to check compatibility with</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if this step can accept the other step's output type</p> Example <p>Check step compatibility: <pre><code>step1 = Step.create(\n    func=process_text,  # returns str\n    step_type=StepType.THINK,\n    name=\"process\"\n)\nstep2 = Step.create(\n    func=analyze_text,  # takes str input\n    step_type=StepType.ACT,\n    name=\"analyze\"\n)\n\nif step2.is_compatible_with(step1):\n    print(\"Can connect process -&gt; analyze\")\n</code></pre></p> Source code in <code>clientai/agent/steps/base.py</code> <pre><code>def is_compatible_with(self, other: \"Step\") -&gt; bool:\n    \"\"\"Check if this step's input is compatible with another step's output.\n\n    Determines if steps can be connected in a workflow by comparing their\n    input/output types.\n\n    Args:\n        other: The step to check compatibility with\n\n    Returns:\n        bool: True if this step can accept the other step's output type\n\n    Example:\n        Check step compatibility:\n        ```python\n        step1 = Step.create(\n            func=process_text,  # returns str\n            step_type=StepType.THINK,\n            name=\"process\"\n        )\n        step2 = Step.create(\n            func=analyze_text,  # takes str input\n            step_type=StepType.ACT,\n            name=\"analyze\"\n        )\n\n        if step2.is_compatible_with(step1):\n            print(\"Can connect process -&gt; analyze\")\n        ```\n    \"\"\"\n    if not other.metadata or not self.metadata:\n        return False\n\n    arg_types = list(self.metadata.arg_types.values())\n    if not arg_types:\n        return False\n\n    return str(arg_types[0]) == other.metadata.return_type\n</code></pre>"},{"location":"api/agent/steps/types/","title":"StepTypes API Reference","text":"<p>The <code>StepTypes</code> module defines the types of steps available in an agent's workflow, such as THINK, ACT, OBSERVE, and SYNTHESIZE.</p>"},{"location":"api/agent/steps/types/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>Enum</code></p> <p>Type classification for workflow steps.</p> <p>Defines the different types of steps that can exist in a workflow, each representing a different kind of operation or phase in the agent's processing.</p> <p>Attributes:</p> Name Type Description <code>THINK</code> <p>Analysis and reasoning steps that process information</p> <code>ACT</code> <p>Decision-making and action steps that perform operations</p> <code>OBSERVE</code> <p>Data collection and observation steps that gather information</p> <code>SYNTHESIZE</code> <p>Integration steps that combine or summarize information</p> Example <p>Using step types: <pre><code># Reference step types\nstep_type = StepType.THINK\nprint(step_type.name)  # Output: \"THINK\"\n\n# Use in step decoration\n@think(\"analyze\")  # Uses StepType.THINK internally\ndef analyze_data(self, input_data: str) -&gt; str:\n    return f\"Analysis of {input_data}\"\n\n# Compare step types\nif step.step_type == StepType.ACT:\n    print(\"This is an action step\")\n</code></pre></p> Notes <ul> <li>Each step type has default configurations (temperature, etc.)</li> <li>Step types influence tool availability through scoping</li> <li>Custom steps typically default to ACT type behavior</li> </ul> Source code in <code>clientai/agent/steps/types.py</code> <pre><code>class StepType(Enum):\n    \"\"\"Type classification for workflow steps.\n\n    Defines the different types of steps that can exist\n    in a workflow, each representing a different kind\n    of operation or phase in the agent's processing.\n\n    Attributes:\n        THINK: Analysis and reasoning steps that process information\n        ACT: Decision-making and action steps that perform operations\n        OBSERVE: Data collection and observation steps that gather information\n        SYNTHESIZE: Integration steps that combine or summarize information\n\n    Example:\n        Using step types:\n        ```python\n        # Reference step types\n        step_type = StepType.THINK\n        print(step_type.name)  # Output: \"THINK\"\n\n        # Use in step decoration\n        @think(\"analyze\")  # Uses StepType.THINK internally\n        def analyze_data(self, input_data: str) -&gt; str:\n            return f\"Analysis of {input_data}\"\n\n        # Compare step types\n        if step.step_type == StepType.ACT:\n            print(\"This is an action step\")\n        ```\n\n    Notes:\n        - Each step type has default configurations (temperature, etc.)\n        - Step types influence tool availability through scoping\n        - Custom steps typically default to ACT type behavior\n    \"\"\"\n\n    THINK = auto()\n    ACT = auto()\n    OBSERVE = auto()\n    SYNTHESIZE = auto()\n</code></pre>"},{"location":"api/agent/tools/registry/","title":"ToolRegistry Class API Reference","text":"<p>The <code>ToolRegistry</code> class manages the registration and organization of tools, maintaining indices by name and scope.</p>"},{"location":"api/agent/tools/registry/#class-definition","title":"Class Definition","text":"<p>Registry for managing and organizing tools by name and scope.</p> <p>A centralized registry that maintains a collection of tools with efficient lookup by name and scope. It ensures unique tool names and proper scope indexing for quick access to tools available in different execution contexts.</p> <p>Attributes:</p> Name Type Description <code>_tools</code> <code>Dict[str, Tool]</code> <p>Dictionary mapping tool names to Tool instances.</p> <code>_scope_index</code> <code>Dict[ToolScope, Set[str]]</code> <p>Dictionary mapping scopes to sets of tool names.</p> Example <pre><code>registry = ToolRegistry()\n\n# Register a tool with configuration\nconfig = ToolConfig(\n    tool=calculator_func,\n    scopes=[\"think\", \"act\"],\n    name=\"Calculator\"\n)\nregistry.register(config)\n\n# Get tools for a scope\nthink_tools = registry.get_for_scope(\"think\")\n\n# Check if tool exists\nif \"Calculator\" in registry:\n    tool = registry.get(\"Calculator\")\n</code></pre> Source code in <code>clientai/agent/tools/registry.py</code> <pre><code>class ToolRegistry:\n    \"\"\"Registry for managing and organizing tools by name and scope.\n\n    A centralized registry that maintains a collection of tools with\n    efficient lookup by name and scope. It ensures unique tool names\n    and proper scope indexing for quick access to tools available in\n    different execution contexts.\n\n    Attributes:\n        _tools: Dictionary mapping tool names to Tool instances.\n        _scope_index: Dictionary mapping scopes to sets of tool names.\n\n    Example:\n        ```python\n        registry = ToolRegistry()\n\n        # Register a tool with configuration\n        config = ToolConfig(\n            tool=calculator_func,\n            scopes=[\"think\", \"act\"],\n            name=\"Calculator\"\n        )\n        registry.register(config)\n\n        # Get tools for a scope\n        think_tools = registry.get_for_scope(\"think\")\n\n        # Check if tool exists\n        if \"Calculator\" in registry:\n            tool = registry.get(\"Calculator\")\n        ```\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initialize an empty tool registry.\n\n        Creates empty storage for tools and initializes scope indexing for\n        all available tool scopes.\n        \"\"\"\n        self._tools: Dict[str, Tool] = {}\n        self._scope_index: Dict[ToolScope, Set[str]] = {\n            scope: set() for scope in ToolScope\n        }\n\n    def register(self, tool_config: ToolConfig) -&gt; None:\n        \"\"\"Register a new tool with the registry.\n\n        Creates a Tool instance if needed and adds it to the registry with\n        proper scope indexing. Handles scope inheritance for tools marked\n        as available in all scopes.\n\n        Args:\n            tool_config: Configuration specifying the tool and its properties.\n\n        Raises:\n            ValueError: If a tool with the same name is already registered.\n\n        Example:\n            ```python\n            registry = ToolRegistry()\n            registry.register(ToolConfig(\n                tool=my_tool,\n                scopes=[\"think\"],\n                name=\"MyTool\"\n            ))\n            ```\n        \"\"\"\n        tool = (\n            tool_config.tool\n            if isinstance(tool_config.tool, Tool)\n            else Tool.create(\n                func=tool_config.tool,\n                name=tool_config.name,\n                description=tool_config.description,\n            )\n        )\n\n        if tool.name in self._tools:\n            raise ValueError(f\"Tool '{tool.name}' already registered\")\n\n        self._tools[tool.name] = tool\n\n        for scope in tool_config.scopes:\n            self._scope_index[scope].add(tool.name)\n            if scope == ToolScope.ALL:\n                for s in ToolScope:\n                    self._scope_index[s].add(tool.name)\n\n    def get(self, name: str) -&gt; Optional[Tool]:\n        \"\"\"Retrieve a tool by its name.\n\n        Args:\n            name: The name of the tool to retrieve.\n\n        Returns:\n            The requested Tool instance, or None if not found.\n\n        Example:\n            ```python\n            tool = registry.get(\"Calculator\")\n            if tool:\n                result = tool(5, 3)\n            ```\n        \"\"\"\n        return self._tools.get(name)\n\n    def get_for_scope(self, scope: Optional[str] = None) -&gt; List[Tool]:\n        \"\"\"Get all tools available in a specific scope.\n\n        Args:\n            scope: The scope to filter tools by. If None, returns all tools.\n\n        Returns:\n            List of Tool instances available in the specified scope.\n\n        Raises:\n            ValueError: If the specified scope is invalid.\n\n        Example:\n            ```python\n            think_tools = registry.get_for_scope(\"think\")\n            all_tools = registry.get_for_scope(None)\n            ```\n        \"\"\"\n        if scope is None:\n            return list(self._tools.values())\n\n        tool_scope = ToolScope.from_str(scope)\n        return [self._tools[name] for name in self._scope_index[tool_scope]]\n\n    def __contains__(self, name: str) -&gt; bool:\n        \"\"\"\n        Check if a tool is registered by name.\n\n        Args:\n            name: The name of the tool to check.\n\n        Returns:\n            True if the tool is registered, False otherwise.\n\n        Example:\n            ```python\n            if \"Calculator\" in registry:\n                tool = registry.get(\"Calculator\")\n            ```\n        \"\"\"\n        return name in self._tools\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Get the total number of registered tools.\n\n        Returns:\n            Number of tools in the registry.\n\n        Example:\n            ```python\n            print(f\"Registry contains {len(registry)} tools\")\n            ```\n        \"\"\"\n        return len(self._tools)\n</code></pre>"},{"location":"api/agent/tools/registry/#clientai.agent.tools.registry.ToolRegistry.__contains__","title":"<code>__contains__(name)</code>","text":"<p>Check if a tool is registered by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the tool to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the tool is registered, False otherwise.</p> Example <pre><code>if \"Calculator\" in registry:\n    tool = registry.get(\"Calculator\")\n</code></pre> Source code in <code>clientai/agent/tools/registry.py</code> <pre><code>def __contains__(self, name: str) -&gt; bool:\n    \"\"\"\n    Check if a tool is registered by name.\n\n    Args:\n        name: The name of the tool to check.\n\n    Returns:\n        True if the tool is registered, False otherwise.\n\n    Example:\n        ```python\n        if \"Calculator\" in registry:\n            tool = registry.get(\"Calculator\")\n        ```\n    \"\"\"\n    return name in self._tools\n</code></pre>"},{"location":"api/agent/tools/registry/#clientai.agent.tools.registry.ToolRegistry.__init__","title":"<code>__init__()</code>","text":"<p>Initialize an empty tool registry.</p> <p>Creates empty storage for tools and initializes scope indexing for all available tool scopes.</p> Source code in <code>clientai/agent/tools/registry.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initialize an empty tool registry.\n\n    Creates empty storage for tools and initializes scope indexing for\n    all available tool scopes.\n    \"\"\"\n    self._tools: Dict[str, Tool] = {}\n    self._scope_index: Dict[ToolScope, Set[str]] = {\n        scope: set() for scope in ToolScope\n    }\n</code></pre>"},{"location":"api/agent/tools/registry/#clientai.agent.tools.registry.ToolRegistry.__len__","title":"<code>__len__()</code>","text":"<p>Get the total number of registered tools.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of tools in the registry.</p> Example <pre><code>print(f\"Registry contains {len(registry)} tools\")\n</code></pre> Source code in <code>clientai/agent/tools/registry.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Get the total number of registered tools.\n\n    Returns:\n        Number of tools in the registry.\n\n    Example:\n        ```python\n        print(f\"Registry contains {len(registry)} tools\")\n        ```\n    \"\"\"\n    return len(self._tools)\n</code></pre>"},{"location":"api/agent/tools/registry/#clientai.agent.tools.registry.ToolRegistry.get","title":"<code>get(name)</code>","text":"<p>Retrieve a tool by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the tool to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[Tool]</code> <p>The requested Tool instance, or None if not found.</p> Example <pre><code>tool = registry.get(\"Calculator\")\nif tool:\n    result = tool(5, 3)\n</code></pre> Source code in <code>clientai/agent/tools/registry.py</code> <pre><code>def get(self, name: str) -&gt; Optional[Tool]:\n    \"\"\"Retrieve a tool by its name.\n\n    Args:\n        name: The name of the tool to retrieve.\n\n    Returns:\n        The requested Tool instance, or None if not found.\n\n    Example:\n        ```python\n        tool = registry.get(\"Calculator\")\n        if tool:\n            result = tool(5, 3)\n        ```\n    \"\"\"\n    return self._tools.get(name)\n</code></pre>"},{"location":"api/agent/tools/registry/#clientai.agent.tools.registry.ToolRegistry.get_for_scope","title":"<code>get_for_scope(scope=None)</code>","text":"<p>Get all tools available in a specific scope.</p> <p>Parameters:</p> Name Type Description Default <code>scope</code> <code>Optional[str]</code> <p>The scope to filter tools by. If None, returns all tools.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tool]</code> <p>List of Tool instances available in the specified scope.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified scope is invalid.</p> Example <pre><code>think_tools = registry.get_for_scope(\"think\")\nall_tools = registry.get_for_scope(None)\n</code></pre> Source code in <code>clientai/agent/tools/registry.py</code> <pre><code>def get_for_scope(self, scope: Optional[str] = None) -&gt; List[Tool]:\n    \"\"\"Get all tools available in a specific scope.\n\n    Args:\n        scope: The scope to filter tools by. If None, returns all tools.\n\n    Returns:\n        List of Tool instances available in the specified scope.\n\n    Raises:\n        ValueError: If the specified scope is invalid.\n\n    Example:\n        ```python\n        think_tools = registry.get_for_scope(\"think\")\n        all_tools = registry.get_for_scope(None)\n        ```\n    \"\"\"\n    if scope is None:\n        return list(self._tools.values())\n\n    tool_scope = ToolScope.from_str(scope)\n    return [self._tools[name] for name in self._scope_index[tool_scope]]\n</code></pre>"},{"location":"api/agent/tools/registry/#clientai.agent.tools.registry.ToolRegistry.register","title":"<code>register(tool_config)</code>","text":"<p>Register a new tool with the registry.</p> <p>Creates a Tool instance if needed and adds it to the registry with proper scope indexing. Handles scope inheritance for tools marked as available in all scopes.</p> <p>Parameters:</p> Name Type Description Default <code>tool_config</code> <code>ToolConfig</code> <p>Configuration specifying the tool and its properties.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If a tool with the same name is already registered.</p> Example <pre><code>registry = ToolRegistry()\nregistry.register(ToolConfig(\n    tool=my_tool,\n    scopes=[\"think\"],\n    name=\"MyTool\"\n))\n</code></pre> Source code in <code>clientai/agent/tools/registry.py</code> <pre><code>def register(self, tool_config: ToolConfig) -&gt; None:\n    \"\"\"Register a new tool with the registry.\n\n    Creates a Tool instance if needed and adds it to the registry with\n    proper scope indexing. Handles scope inheritance for tools marked\n    as available in all scopes.\n\n    Args:\n        tool_config: Configuration specifying the tool and its properties.\n\n    Raises:\n        ValueError: If a tool with the same name is already registered.\n\n    Example:\n        ```python\n        registry = ToolRegistry()\n        registry.register(ToolConfig(\n            tool=my_tool,\n            scopes=[\"think\"],\n            name=\"MyTool\"\n        ))\n        ```\n    \"\"\"\n    tool = (\n        tool_config.tool\n        if isinstance(tool_config.tool, Tool)\n        else Tool.create(\n            func=tool_config.tool,\n            name=tool_config.name,\n            description=tool_config.description,\n        )\n    )\n\n    if tool.name in self._tools:\n        raise ValueError(f\"Tool '{tool.name}' already registered\")\n\n    self._tools[tool.name] = tool\n\n    for scope in tool_config.scopes:\n        self._scope_index[scope].add(tool.name)\n        if scope == ToolScope.ALL:\n            for s in ToolScope:\n                self._scope_index[s].add(tool.name)\n</code></pre>"},{"location":"api/agent/tools/selector/","title":"ToolSelector Class API Reference","text":"<p>The <code>ToolSelector</code> class handles the automatic selection and execution of tools using LLM-based decision making.</p>"},{"location":"api/agent/tools/selector/#class-definition","title":"Class Definition","text":"<p>Manages the automatic selection and execution of tools using LLM-based decision making.</p> <p>The ToolSelector uses a language model to analyze tasks and determine which tools would be most appropriate to use, considering both the task requirements and the current context. It provides a complete pipeline for tool selection, validation, and execution with comprehensive error handling and logging.</p> <p>The selector's key responsibilities include: 1. Analyzing tasks and context to determine tool requirements 2. Selecting appropriate tools based on capabilities and confidence 3. Validating tool arguments before execution 4. Managing tool execution and error handling 5. Providing detailed logging and error reporting</p> Key Features <ul> <li>LLM-based tool selection with context awareness</li> <li>Configurable confidence thresholds for tool selection</li> <li>Automatic argument validation against tool signatures</li> <li>Comprehensive error handling and recovery</li> <li>Detailed execution logging and debugging support</li> <li>Context-aware decision making</li> </ul> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration for tool selection behavior, including confidence thresholds and tool limits</p> <code>model_config</code> <p>Configuration for the LLM used in selection, including model name and parameters</p> Example <pre><code># Initialize selector with custom configuration\nselector = ToolSelector(\n    config=ToolSelectionConfig(\n        confidence_threshold=0.8,\n        max_tools_per_step=3\n    ),\n    model_config=ModelConfig(\n        name=\"gpt-4\",\n        temperature=0.0\n    )\n)\n\n# Select tools for a task with context\ndecisions = selector.select_tools(\n    task=\"Calculate the average daily sales\",\n    tools=[calculator, aggregator],\n    context={\"sales_data\": [100, 200, 300]},\n    client=llm_client\n)\n\n# Execute the selected tools\nresults = selector.execute_tool_decisions(\n    decisions=decisions,\n    tools={\"calculator\": calculator, \"aggregator\": aggregator}\n)\n\n# Process results\nfor result in results:\n    if result.error:\n        print(f\"Error in {result.tool_name}: {result.error}\")\n    else:\n        print(f\"Result from {result.tool_name}: {result.result}\")\n</code></pre> Source code in <code>clientai/agent/tools/selection/selector.py</code> <pre><code>class ToolSelector:\n    \"\"\"\n    Manages the automatic selection and execution\n    of tools using LLM-based decision making.\n\n    The ToolSelector uses a language model to analyze tasks and determine\n    which tools would be most appropriate to use, considering both the\n    task requirements and the current context. It provides a complete\n    pipeline for tool selection, validation, and execution with\n    comprehensive error handling and logging.\n\n    The selector's key responsibilities include:\n    1. Analyzing tasks and context to determine tool requirements\n    2. Selecting appropriate tools based on capabilities and confidence\n    3. Validating tool arguments before execution\n    4. Managing tool execution and error handling\n    5. Providing detailed logging and error reporting\n\n    Key Features:\n        - LLM-based tool selection with context awareness\n        - Configurable confidence thresholds for tool selection\n        - Automatic argument validation against tool signatures\n        - Comprehensive error handling and recovery\n        - Detailed execution logging and debugging support\n        - Context-aware decision making\n\n    Attributes:\n        config: Configuration for tool selection behavior,\n            including confidence thresholds and tool limits\n        model_config: Configuration for the LLM used in selection,\n            including model name and parameters\n\n    Example:\n        ```python\n        # Initialize selector with custom configuration\n        selector = ToolSelector(\n            config=ToolSelectionConfig(\n                confidence_threshold=0.8,\n                max_tools_per_step=3\n            ),\n            model_config=ModelConfig(\n                name=\"gpt-4\",\n                temperature=0.0\n            )\n        )\n\n        # Select tools for a task with context\n        decisions = selector.select_tools(\n            task=\"Calculate the average daily sales\",\n            tools=[calculator, aggregator],\n            context={\"sales_data\": [100, 200, 300]},\n            client=llm_client\n        )\n\n        # Execute the selected tools\n        results = selector.execute_tool_decisions(\n            decisions=decisions,\n            tools={\"calculator\": calculator, \"aggregator\": aggregator}\n        )\n\n        # Process results\n        for result in results:\n            if result.error:\n                print(f\"Error in {result.tool_name}: {result.error}\")\n            else:\n                print(f\"Result from {result.tool_name}: {result.result}\")\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        config: Optional[ToolSelectionConfig] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ToolSelector with the specified configurations.\n\n        Sets up the selector with either provided configurations or defaults.\n        Initializes logging and validates configuration parameters.\n\n        Args:\n            model_config: Configuration for the LLM used in selection.\n            config: Configuration for tool selection behavior. If None,\n                   uses default configuration with standard thresholds\n                   and limits.\n\n        Raises:\n            ValueError: If provided configurations contain invalid values.\n\n        Example:\n            ```python\n            # With default configuration\n            selector = ToolSelector()\n\n            # With custom configuration\n            selector = ToolSelector(\n                config=ToolSelectionConfig(confidence_threshold=0.8),\n                model_config=ModelConfig(name=\"gpt-4\", temperature=0.0)\n            )\n            ```\n        \"\"\"\n        self.config = config or ToolSelectionConfig()\n\n        self.model_config = model_config.merge(stream=False, json_output=True)\n\n        logger.debug(\"Initialized ToolSelector\")\n        logger.debug(f\"Using model: {self.model_config.name}\")\n        logger.debug(\n            f\"Confidence threshold: {self.config.confidence_threshold}\"\n        )\n\n    def _format_context(self, context: Dict[str, Any]) -&gt; str:\n        \"\"\"\n        Format the context dictionary into a structured string for LLM prompts.\n\n        Converts a context dictionary into a formatted string representation\n        that clearly presents the available context information to the LLM.\n        The format is designed to be both human-readable and easily parseable\n        by the LLM.\n\n        Args:\n            context: Dictionary containing contextual information that may be\n                    relevant for tool selection. Can include any serializable\n                    data relevant to the task.\n\n        Returns:\n            A formatted string representation of the context, organized in a\n            hierarchical structure with clear labeling.\n\n        Example:\n            ```python\n            context_str = selector._format_context({\n                \"user\": \"John Doe\",\n                \"current_data\": [1, 2, 3],\n                \"preferences\": {\"format\": \"json\", \"units\": \"metric\"}\n            })\n            # Output:\n            # Current Context:\n            # - user: John Doe\n            # - current_data: [1, 2, 3]\n            # - preferences: {\"format\": \"json\", \"units\": \"metric\"}\n            ```\n\n        Note:\n            - The output format is designed to be clear and consistent\n            - Complex nested structures are preserved in their\n              string representation\n            - Empty context is handled with a clear \"no context\" message\n        \"\"\"\n        if not context:\n            return \"No additional context available.\"\n\n        formatted = [\"Current Context:\"]\n        for key, value in context.items():\n            formatted.append(f\"- {key}: {value}\")\n        return \"\\n\".join(formatted)\n\n    def _format_tools(self, tools: List[Tool]) -&gt; str:\n        \"\"\"\n        Format a list of tools into a structured string for LLM prompts.\n\n        Creates a consolidated string representation of all available tools\n        using each tool's standardized format_tool_info() method. This ensures\n        consistent formatting throughout the application and provides clear,\n        complete tool information to the LLM.\n\n        Args:\n            tools: List of Tool instances to format. Each tool should provide\n                  its name, signature, and description.\n\n        Returns:\n            A formatted string containing the complete information for\n            all tools, organized in a clear, hierarchical structure.\n\n        Example:\n            ```python\n            tool_str = selector._format_tools([\n                calculator_tool,\n                text_processor_tool\n            ])\n            # Output:\n            # - Calculator\n            #   Signature: add(x: int, y: int) -&gt; int\n            #   Description: Adds two numbers together\n            # - TextProcessor\n            #   Signature: process(text: str, uppercase: bool = False) -&gt; str\n            #   Description: Processes text with optional case conversion\n            ```\n\n        Note:\n            - Uses the standard Tool.format_tool_info() method for consistency\n            - Maintains proper indentation and structure\n            - Separates tools with newlines for clarity\n            - Preserves complete signature information including defaults\n        \"\"\"\n        return \"\\n\".join(tool.format_tool_info() for tool in tools)\n\n    def select_tools(\n        self,\n        task: str,\n        tools: List[Tool],\n        context: Dict[str, Any],\n        client: Any,\n    ) -&gt; List[ToolCallDecision]:\n        \"\"\"Use LLM to select appropriate tools for a given task.\n\n        Analyzes the task description, available tools, and current context\n        to determine which tools would be most appropriate to use.\n        Makes selections based on confidence thresholds,\n        validates arguments, and provides reasoning.\n\n        Args:\n            task: Description of the task to accomplish.\n            tools: List of available tools to choose from.\n            context: Current execution context and state information.\n            client: LLM client for making selection decisions.\n\n        Returns:\n            List of ToolCallDecision objects containing selected tools,\n            arguments, confidence levels, and reasoning.\n\n        Raises:\n            StepError: If LLM interaction fails.\n            ToolError: If tool validation fails.\n\n        Example:\n            ```python\n            decisions = selector.select_tools(\n                task=\"Calculate average daily sales increase\",\n                tools=[calculator, aggregator],\n                context={\"sales_data\": [100, 200, 300]},\n                client=llm_client\n            )\n\n            for decision in decisions:\n                print(f\"Selected: {decision.tool_name}\")\n                print(f\"Arguments: {decision.arguments}\")\n                print(f\"Confidence: {decision.confidence}\")\n            ```\n        \"\"\"\n        logger.debug(f\"Selecting tools for task: {task}\")\n        logger.debug(f\"Available tools: {[t.name for t in tools]}\")\n        logger.debug(f\"Context keys: {list(context.keys())}\")\n\n        if not tools:\n            logger.debug(\"No tools available, returning empty list\")\n            return []\n\n        prompt = f\"\"\"\n        You are a helpful AI that uses tools to solve problems.\n\n        Task: {task}\n\n        {self._format_context(context)}\n\n        Available Tools:\n        {self._format_tools(tools)}\n\n        Respond ONLY with a valid JSON object.\n        Do not include any comments or placeholders.\n        The JSON must follow this exact structure:\n        {{\n            \"tool_calls\": [\n                {{\n                    \"tool_name\": \"name_of_tool\",\n                    \"arguments\": {{\n                        \"param1\": \"value1\",\n                        \"param2\": \"value2\"\n                    }},\n                    \"confidence\": 0.0,\n                    \"reasoning\": \"Clear explanation of why the tool was chosen\"\n                }}\n            ]\n        }}\n\n        All values must be concrete and complete.\n        Do not use placeholders or temporary values.\n        Each tool_name must exactly match one of the available tools.\n        All required parameters for the chosen tool must be provided.\n        Confidence must be a number between 0.0 and 1.0.\n        \"\"\"\n\n        logger.debug(f\"Generated tool selection prompt:\\n{prompt}\")\n\n        try:\n            logger.debug(\"Requesting tool selection from LLM\")\n            response = client.generate_text(\n                prompt,\n                model=self.model_config.name,\n                **self.model_config.get_parameters(),\n            )\n            logger.debug(f\"Raw LLM response: {response}\")\n\n            try:\n                decisions = json.loads(response)\n                logger.debug(f\"Parsed decisions: {decisions}\")\n            except json.JSONDecodeError:\n                logger.warning(\n                    \"Failed to parse JSON response, attempting to extract JSON\"\n                )\n                start = response.find(\"{\")\n                end = response.rfind(\"}\") + 1\n                if start &gt;= 0 and end &gt; start:\n                    json_str = response[start:end]\n                    decisions = json.loads(json_str)\n                    logger.debug(\n                        f\"Successfully extracted and parsed JSON: {decisions}\"\n                    )\n                else:\n                    logger.error(\"Could not find valid JSON in response\")\n                    return []\n\n        except Exception as e:\n            logger.error(\n                f\"Failed to get or parse LLM tool selection response: {e}\"\n            )\n            return []\n\n        tool_decisions = []\n        tools_by_name = {tool.name: tool for tool in tools}\n\n        for call in decisions.get(\"tool_calls\", []):\n            tool_name = call.get(\"tool_name\")\n            if tool_name not in tools_by_name:\n                logger.warning(\n                    f\"Tool '{tool_name}' not found in available tools\"\n                )\n                continue\n\n            arguments = call.get(\"arguments\", {})\n            confidence = call.get(\"confidence\", 0.0)\n            reasoning = call.get(\"reasoning\", \"No reasoning provided\")\n\n            logger.debug(f\"Processing decision for tool '{tool_name}':\")\n            logger.debug(f\"  Arguments: {arguments}\")\n            logger.debug(f\"  Confidence: {confidence}\")\n            logger.debug(f\"  Reasoning: {reasoning}\")\n\n            if confidence &lt; self.config.confidence_threshold:\n                logger.debug(\n                    f\"Skipping tool {tool_name} \"\n                    \"due to low confidence: {confidence}\"\n                )\n                continue\n\n            if not self._validate_tool_arguments(\n                tools_by_name[tool_name], arguments\n            ):\n                logger.warning(\n                    f\"Invalid arguments for tool {tool_name}: {arguments}\"\n                )\n                continue\n\n            tool_decisions.append(\n                ToolCallDecision(\n                    tool_name=tool_name,\n                    arguments=arguments,\n                    confidence=confidence,\n                    reasoning=reasoning,\n                )\n            )\n            logger.debug(f\"Added decision for tool '{tool_name}'\")\n\n        logger.debug(\n            f\"Final tool decisions: {[d.tool_name for d in tool_decisions]}\"\n        )\n        return tool_decisions\n\n    def _validate_tool_arguments(\n        self, tool: Tool, arguments: Dict[str, Any]\n    ) -&gt; bool:\n        \"\"\"\n        Validate that provided arguments match a tool's signature.\n\n        Performs comprehensive validation of tool arguments against the tool's\n        signature, ensuring both required arguments are present and no unknown\n        arguments are included.\n\n        Args:\n            tool: The tool whose arguments to validate, containing signature\n                 information.\n            arguments: Dictionary of argument names to values to validate.\n\n        Returns:\n            bool: True if all arguments are valid, False otherwise.\n\n        Example:\n            ```python\n            # Valid argument check\n            is_valid = selector._validate_tool_arguments(\n                tool=calculator,\n                arguments={\"x\": 5, \"y\": 3}\n            )\n            if not is_valid:\n                print(\"Invalid arguments provided\")\n\n            # Invalid argument check (missing required)\n            is_valid = selector._validate_tool_arguments(\n                tool=calculator,\n                arguments={\"x\": 5}  # missing 'y'\n            )\n            # Returns False, logs warning about missing parameter\n\n            # Invalid argument check (unknown argument)\n            is_valid = selector._validate_tool_arguments(\n                tool=calculator,\n                arguments={\"x\": 5, \"y\": 3, \"z\": 10}  # 'z' is unknown\n            )\n            # Returns False, logs warning about unknown parameter\n            ```\n\n        Note:\n            - Validates presence of all required parameters\n            - Checks for unknown parameters\n            - Logs specific validation failures for debugging\n            - Does not validate argument types (done at execution)\n        \"\"\"\n        param_info = tool.signature.parameters\n\n        required_params = {\n            name for name, info in param_info if info.default is None\n        }\n        if not all(param in arguments for param in required_params):\n            logger.warning(\n                f\"Missing required parameters for {tool.name}: \"\n                f\"needs {required_params}, got {list(arguments.keys())}\"\n            )\n            return False\n\n        valid_params = {name for name, _ in param_info}\n        if not all(arg in valid_params for arg in arguments):\n            logger.warning(\n                f\"Unknown parameters for {tool.name}: \"\n                f\"valid parameters are {valid_params}, \"\n                f\"got {list(arguments.keys())}\"\n            )\n            return False\n\n        return True\n\n    def execute_tool_decisions(\n        self, decisions: List[ToolCallDecision], tools: Dict[str, Tool]\n    ) -&gt; List[ToolCallDecision]:\n        \"\"\"\n        Execute a series of tool decisions and capture their results.\n\n        Takes a list of validated tool selection decisions and executes each\n        tool with its specified arguments. Updates the decision objects with\n        either results or error messages from the execution.\n\n        Args:\n            decisions: List of ToolCallDecision objects containing tool\n                      selections and their arguments.\n            tools: Dictionary mapping tool names to Tool instances that\n                  will be executed.\n\n        Returns:\n            The same list of decisions, updated with execution results or\n            error messages in case of failures.\n\n        Example:\n            ```python\n            # Execute multiple tool decisions\n            updated_decisions = selector.execute_tool_decisions(\n                decisions=[\n                    ToolCallDecision(\n                        tool_name=\"calculator\",\n                        arguments={\"x\": 5, \"y\": 3},\n                        confidence=0.9,\n                        reasoning=\"Need to add numbers\"\n                    ),\n                    ToolCallDecision(\n                        tool_name=\"formatter\",\n                        arguments={\"text\": \"hello\"},\n                        confidence=0.8,\n                        reasoning=\"Need to format text\"\n                    )\n                ],\n                tools={\n                    \"calculator\": calculator_tool,\n                    \"formatter\": formatter_tool\n                }\n            )\n\n            # Process results\n            for decision in updated_decisions:\n                if decision.error:\n                    print(\n                        f\"Error in {decision.tool_name}: {decision.error}\"\n                    )\n                else:\n                    print(\n                        f\"Result from {decision.tool_name}: {decision.result}\"\n                    )\n            ```\n\n        Notes:\n            - Each decision is executed independently\n            - Execution errors in one decision don't prevent others\n              from executing\n            - Original decision objects are modified with results/errors\n            - All execution attempts are logged for debugging\n            - Tools are executed with their provided arguments\n              without modification\n            - Results can be any type that the tools return\n            - Errors are captured as strings in the decision object\n\n        Raises:\n            KeyError: If a tool name in decisions isn't found in the tools\n                      dictionary\n            Exception: Any exception from tool execution is caught and stored\n                      in the decision's error field\n        \"\"\"\n        for decision in decisions:\n            logger.debug(f\"Executing tool '{decision.tool_name}'\")\n            logger.debug(f\"Arguments: {decision.arguments}\")\n\n            try:\n                tool = tools[decision.tool_name]\n                result = tool(**decision.arguments)\n                decision.result = result\n                logger.debug(f\"Tool execution successful: {result}\")\n            except Exception as e:\n                logger.error(\n                    f\"Failed to execute tool {decision.tool_name}: {e}\",\n                    exc_info=True,\n                )\n                decision.error = str(e)\n\n        return decisions\n</code></pre>"},{"location":"api/agent/tools/selector/#clientai.agent.tools.selection.ToolSelector.__init__","title":"<code>__init__(model_config, config=None)</code>","text":"<p>Initialize the ToolSelector with the specified configurations.</p> <p>Sets up the selector with either provided configurations or defaults. Initializes logging and validates configuration parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>ModelConfig</code> <p>Configuration for the LLM used in selection.</p> required <code>config</code> <code>Optional[ToolSelectionConfig]</code> <p>Configuration for tool selection behavior. If None,    uses default configuration with standard thresholds    and limits.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provided configurations contain invalid values.</p> Example <pre><code># With default configuration\nselector = ToolSelector()\n\n# With custom configuration\nselector = ToolSelector(\n    config=ToolSelectionConfig(confidence_threshold=0.8),\n    model_config=ModelConfig(name=\"gpt-4\", temperature=0.0)\n)\n</code></pre> Source code in <code>clientai/agent/tools/selection/selector.py</code> <pre><code>def __init__(\n    self,\n    model_config: ModelConfig,\n    config: Optional[ToolSelectionConfig] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the ToolSelector with the specified configurations.\n\n    Sets up the selector with either provided configurations or defaults.\n    Initializes logging and validates configuration parameters.\n\n    Args:\n        model_config: Configuration for the LLM used in selection.\n        config: Configuration for tool selection behavior. If None,\n               uses default configuration with standard thresholds\n               and limits.\n\n    Raises:\n        ValueError: If provided configurations contain invalid values.\n\n    Example:\n        ```python\n        # With default configuration\n        selector = ToolSelector()\n\n        # With custom configuration\n        selector = ToolSelector(\n            config=ToolSelectionConfig(confidence_threshold=0.8),\n            model_config=ModelConfig(name=\"gpt-4\", temperature=0.0)\n        )\n        ```\n    \"\"\"\n    self.config = config or ToolSelectionConfig()\n\n    self.model_config = model_config.merge(stream=False, json_output=True)\n\n    logger.debug(\"Initialized ToolSelector\")\n    logger.debug(f\"Using model: {self.model_config.name}\")\n    logger.debug(\n        f\"Confidence threshold: {self.config.confidence_threshold}\"\n    )\n</code></pre>"},{"location":"api/agent/tools/selector/#clientai.agent.tools.selection.ToolSelector.execute_tool_decisions","title":"<code>execute_tool_decisions(decisions, tools)</code>","text":"<p>Execute a series of tool decisions and capture their results.</p> <p>Takes a list of validated tool selection decisions and executes each tool with its specified arguments. Updates the decision objects with either results or error messages from the execution.</p> <p>Parameters:</p> Name Type Description Default <code>decisions</code> <code>List[ToolCallDecision]</code> <p>List of ToolCallDecision objects containing tool       selections and their arguments.</p> required <code>tools</code> <code>Dict[str, Tool]</code> <p>Dictionary mapping tool names to Tool instances that   will be executed.</p> required <p>Returns:</p> Type Description <code>List[ToolCallDecision]</code> <p>The same list of decisions, updated with execution results or</p> <code>List[ToolCallDecision]</code> <p>error messages in case of failures.</p> Example <pre><code># Execute multiple tool decisions\nupdated_decisions = selector.execute_tool_decisions(\n    decisions=[\n        ToolCallDecision(\n            tool_name=\"calculator\",\n            arguments={\"x\": 5, \"y\": 3},\n            confidence=0.9,\n            reasoning=\"Need to add numbers\"\n        ),\n        ToolCallDecision(\n            tool_name=\"formatter\",\n            arguments={\"text\": \"hello\"},\n            confidence=0.8,\n            reasoning=\"Need to format text\"\n        )\n    ],\n    tools={\n        \"calculator\": calculator_tool,\n        \"formatter\": formatter_tool\n    }\n)\n\n# Process results\nfor decision in updated_decisions:\n    if decision.error:\n        print(\n            f\"Error in {decision.tool_name}: {decision.error}\"\n        )\n    else:\n        print(\n            f\"Result from {decision.tool_name}: {decision.result}\"\n        )\n</code></pre> Notes <ul> <li>Each decision is executed independently</li> <li>Execution errors in one decision don't prevent others   from executing</li> <li>Original decision objects are modified with results/errors</li> <li>All execution attempts are logged for debugging</li> <li>Tools are executed with their provided arguments   without modification</li> <li>Results can be any type that the tools return</li> <li>Errors are captured as strings in the decision object</li> </ul> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a tool name in decisions isn't found in the tools       dictionary</p> <code>Exception</code> <p>Any exception from tool execution is caught and stored       in the decision's error field</p> Source code in <code>clientai/agent/tools/selection/selector.py</code> <pre><code>def execute_tool_decisions(\n    self, decisions: List[ToolCallDecision], tools: Dict[str, Tool]\n) -&gt; List[ToolCallDecision]:\n    \"\"\"\n    Execute a series of tool decisions and capture their results.\n\n    Takes a list of validated tool selection decisions and executes each\n    tool with its specified arguments. Updates the decision objects with\n    either results or error messages from the execution.\n\n    Args:\n        decisions: List of ToolCallDecision objects containing tool\n                  selections and their arguments.\n        tools: Dictionary mapping tool names to Tool instances that\n              will be executed.\n\n    Returns:\n        The same list of decisions, updated with execution results or\n        error messages in case of failures.\n\n    Example:\n        ```python\n        # Execute multiple tool decisions\n        updated_decisions = selector.execute_tool_decisions(\n            decisions=[\n                ToolCallDecision(\n                    tool_name=\"calculator\",\n                    arguments={\"x\": 5, \"y\": 3},\n                    confidence=0.9,\n                    reasoning=\"Need to add numbers\"\n                ),\n                ToolCallDecision(\n                    tool_name=\"formatter\",\n                    arguments={\"text\": \"hello\"},\n                    confidence=0.8,\n                    reasoning=\"Need to format text\"\n                )\n            ],\n            tools={\n                \"calculator\": calculator_tool,\n                \"formatter\": formatter_tool\n            }\n        )\n\n        # Process results\n        for decision in updated_decisions:\n            if decision.error:\n                print(\n                    f\"Error in {decision.tool_name}: {decision.error}\"\n                )\n            else:\n                print(\n                    f\"Result from {decision.tool_name}: {decision.result}\"\n                )\n        ```\n\n    Notes:\n        - Each decision is executed independently\n        - Execution errors in one decision don't prevent others\n          from executing\n        - Original decision objects are modified with results/errors\n        - All execution attempts are logged for debugging\n        - Tools are executed with their provided arguments\n          without modification\n        - Results can be any type that the tools return\n        - Errors are captured as strings in the decision object\n\n    Raises:\n        KeyError: If a tool name in decisions isn't found in the tools\n                  dictionary\n        Exception: Any exception from tool execution is caught and stored\n                  in the decision's error field\n    \"\"\"\n    for decision in decisions:\n        logger.debug(f\"Executing tool '{decision.tool_name}'\")\n        logger.debug(f\"Arguments: {decision.arguments}\")\n\n        try:\n            tool = tools[decision.tool_name]\n            result = tool(**decision.arguments)\n            decision.result = result\n            logger.debug(f\"Tool execution successful: {result}\")\n        except Exception as e:\n            logger.error(\n                f\"Failed to execute tool {decision.tool_name}: {e}\",\n                exc_info=True,\n            )\n            decision.error = str(e)\n\n    return decisions\n</code></pre>"},{"location":"api/agent/tools/selector/#clientai.agent.tools.selection.ToolSelector.select_tools","title":"<code>select_tools(task, tools, context, client)</code>","text":"<p>Use LLM to select appropriate tools for a given task.</p> <p>Analyzes the task description, available tools, and current context to determine which tools would be most appropriate to use. Makes selections based on confidence thresholds, validates arguments, and provides reasoning.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Description of the task to accomplish.</p> required <code>tools</code> <code>List[Tool]</code> <p>List of available tools to choose from.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Current execution context and state information.</p> required <code>client</code> <code>Any</code> <p>LLM client for making selection decisions.</p> required <p>Returns:</p> Type Description <code>List[ToolCallDecision]</code> <p>List of ToolCallDecision objects containing selected tools,</p> <code>List[ToolCallDecision]</code> <p>arguments, confidence levels, and reasoning.</p> <p>Raises:</p> Type Description <code>StepError</code> <p>If LLM interaction fails.</p> <code>ToolError</code> <p>If tool validation fails.</p> Example <pre><code>decisions = selector.select_tools(\n    task=\"Calculate average daily sales increase\",\n    tools=[calculator, aggregator],\n    context={\"sales_data\": [100, 200, 300]},\n    client=llm_client\n)\n\nfor decision in decisions:\n    print(f\"Selected: {decision.tool_name}\")\n    print(f\"Arguments: {decision.arguments}\")\n    print(f\"Confidence: {decision.confidence}\")\n</code></pre> Source code in <code>clientai/agent/tools/selection/selector.py</code> <pre><code>def select_tools(\n    self,\n    task: str,\n    tools: List[Tool],\n    context: Dict[str, Any],\n    client: Any,\n) -&gt; List[ToolCallDecision]:\n    \"\"\"Use LLM to select appropriate tools for a given task.\n\n    Analyzes the task description, available tools, and current context\n    to determine which tools would be most appropriate to use.\n    Makes selections based on confidence thresholds,\n    validates arguments, and provides reasoning.\n\n    Args:\n        task: Description of the task to accomplish.\n        tools: List of available tools to choose from.\n        context: Current execution context and state information.\n        client: LLM client for making selection decisions.\n\n    Returns:\n        List of ToolCallDecision objects containing selected tools,\n        arguments, confidence levels, and reasoning.\n\n    Raises:\n        StepError: If LLM interaction fails.\n        ToolError: If tool validation fails.\n\n    Example:\n        ```python\n        decisions = selector.select_tools(\n            task=\"Calculate average daily sales increase\",\n            tools=[calculator, aggregator],\n            context={\"sales_data\": [100, 200, 300]},\n            client=llm_client\n        )\n\n        for decision in decisions:\n            print(f\"Selected: {decision.tool_name}\")\n            print(f\"Arguments: {decision.arguments}\")\n            print(f\"Confidence: {decision.confidence}\")\n        ```\n    \"\"\"\n    logger.debug(f\"Selecting tools for task: {task}\")\n    logger.debug(f\"Available tools: {[t.name for t in tools]}\")\n    logger.debug(f\"Context keys: {list(context.keys())}\")\n\n    if not tools:\n        logger.debug(\"No tools available, returning empty list\")\n        return []\n\n    prompt = f\"\"\"\n    You are a helpful AI that uses tools to solve problems.\n\n    Task: {task}\n\n    {self._format_context(context)}\n\n    Available Tools:\n    {self._format_tools(tools)}\n\n    Respond ONLY with a valid JSON object.\n    Do not include any comments or placeholders.\n    The JSON must follow this exact structure:\n    {{\n        \"tool_calls\": [\n            {{\n                \"tool_name\": \"name_of_tool\",\n                \"arguments\": {{\n                    \"param1\": \"value1\",\n                    \"param2\": \"value2\"\n                }},\n                \"confidence\": 0.0,\n                \"reasoning\": \"Clear explanation of why the tool was chosen\"\n            }}\n        ]\n    }}\n\n    All values must be concrete and complete.\n    Do not use placeholders or temporary values.\n    Each tool_name must exactly match one of the available tools.\n    All required parameters for the chosen tool must be provided.\n    Confidence must be a number between 0.0 and 1.0.\n    \"\"\"\n\n    logger.debug(f\"Generated tool selection prompt:\\n{prompt}\")\n\n    try:\n        logger.debug(\"Requesting tool selection from LLM\")\n        response = client.generate_text(\n            prompt,\n            model=self.model_config.name,\n            **self.model_config.get_parameters(),\n        )\n        logger.debug(f\"Raw LLM response: {response}\")\n\n        try:\n            decisions = json.loads(response)\n            logger.debug(f\"Parsed decisions: {decisions}\")\n        except json.JSONDecodeError:\n            logger.warning(\n                \"Failed to parse JSON response, attempting to extract JSON\"\n            )\n            start = response.find(\"{\")\n            end = response.rfind(\"}\") + 1\n            if start &gt;= 0 and end &gt; start:\n                json_str = response[start:end]\n                decisions = json.loads(json_str)\n                logger.debug(\n                    f\"Successfully extracted and parsed JSON: {decisions}\"\n                )\n            else:\n                logger.error(\"Could not find valid JSON in response\")\n                return []\n\n    except Exception as e:\n        logger.error(\n            f\"Failed to get or parse LLM tool selection response: {e}\"\n        )\n        return []\n\n    tool_decisions = []\n    tools_by_name = {tool.name: tool for tool in tools}\n\n    for call in decisions.get(\"tool_calls\", []):\n        tool_name = call.get(\"tool_name\")\n        if tool_name not in tools_by_name:\n            logger.warning(\n                f\"Tool '{tool_name}' not found in available tools\"\n            )\n            continue\n\n        arguments = call.get(\"arguments\", {})\n        confidence = call.get(\"confidence\", 0.0)\n        reasoning = call.get(\"reasoning\", \"No reasoning provided\")\n\n        logger.debug(f\"Processing decision for tool '{tool_name}':\")\n        logger.debug(f\"  Arguments: {arguments}\")\n        logger.debug(f\"  Confidence: {confidence}\")\n        logger.debug(f\"  Reasoning: {reasoning}\")\n\n        if confidence &lt; self.config.confidence_threshold:\n            logger.debug(\n                f\"Skipping tool {tool_name} \"\n                \"due to low confidence: {confidence}\"\n            )\n            continue\n\n        if not self._validate_tool_arguments(\n            tools_by_name[tool_name], arguments\n        ):\n            logger.warning(\n                f\"Invalid arguments for tool {tool_name}: {arguments}\"\n            )\n            continue\n\n        tool_decisions.append(\n            ToolCallDecision(\n                tool_name=tool_name,\n                arguments=arguments,\n                confidence=confidence,\n                reasoning=reasoning,\n            )\n        )\n        logger.debug(f\"Added decision for tool '{tool_name}'\")\n\n    logger.debug(\n        f\"Final tool decisions: {[d.tool_name for d in tool_decisions]}\"\n    )\n    return tool_decisions\n</code></pre>"},{"location":"api/agent/tools/tool/","title":"Tool Class API Reference","text":"<p>The <code>Tool</code> class represents a callable tool with associated metadata that can be used by agents in their workflows.</p>"},{"location":"api/agent/tools/tool/#class-definition","title":"Class Definition","text":"<p>A callable tool with metadata for use in agent workflows.</p> <p>Represents a function with associated metadata (name, description, signature) that can be used as a tool by an agent. Tools are immutable and can be called like regular functions.</p> <p>Attributes:</p> Name Type Description <code>func</code> <code>ToolCallable</code> <p>The underlying function that implements the tool's logic.</p> <code>name</code> <code>str</code> <p>The tool's display name.</p> <code>description</code> <code>str</code> <p>Human-readable description of the tool's purpose.</p> <code>_signature</code> <code>ToolSignature</code> <p>Internal cached signature information.</p> Example <p>Using the @tool decorator: <pre><code>@tool\ndef calculate(x: int, y: int) -&gt; int:\n    '''Add two numbers.'''\n    return x + y\nresult = calculate(5, 3)\nprint(result)  # Output: 8\n</code></pre></p> <p>Using @tool with parameters: <pre><code>@tool(name=\"Calculator\", description=\"Performs basic arithmetic\")\ndef add(x: int, y: int) -&gt; int:\n    return x + y\n</code></pre></p> <p>Direct creation and registration: <pre><code>def multiply(x: int, y: int) -&gt; int:\n    '''Multiply two numbers.'''\n    return x * y\ntool = Tool.create(\n    multiply,\n    name=\"Multiplier\",\n    description=\"Performs multiplication\"\n)\nagent.register_tool(tool)\n</code></pre></p> Notes <ul> <li>Tools are immutable (frozen=True dataclass)</li> <li>Tool signatures are cached for performance</li> <li>Tools can be used directly as functions</li> <li>Tool metadata is available for agent introspection</li> </ul> Source code in <code>clientai/agent/tools/base.py</code> <pre><code>@dataclass(frozen=True)\nclass Tool:\n    \"\"\"A callable tool with metadata for use in agent workflows.\n\n    Represents a function with associated metadata\n    (name, description, signature) that can be used\n    as a tool by an agent. Tools are immutable and\n    can be called like regular functions.\n\n    Attributes:\n        func: The underlying function that implements the tool's logic.\n        name: The tool's display name.\n        description: Human-readable description of the tool's purpose.\n        _signature: Internal cached signature information.\n\n    Example:\n        Using the @tool decorator:\n        ```python\n        @tool\n        def calculate(x: int, y: int) -&gt; int:\n            '''Add two numbers.'''\n            return x + y\n        result = calculate(5, 3)\n        print(result)  # Output: 8\n        ```\n\n        Using @tool with parameters:\n        ```python\n        @tool(name=\"Calculator\", description=\"Performs basic arithmetic\")\n        def add(x: int, y: int) -&gt; int:\n            return x + y\n        ```\n\n        Direct creation and registration:\n        ```python\n        def multiply(x: int, y: int) -&gt; int:\n            '''Multiply two numbers.'''\n            return x * y\n        tool = Tool.create(\n            multiply,\n            name=\"Multiplier\",\n            description=\"Performs multiplication\"\n        )\n        agent.register_tool(tool)\n        ```\n\n    Notes:\n        - Tools are immutable (frozen=True dataclass)\n        - Tool signatures are cached for performance\n        - Tools can be used directly as functions\n        - Tool metadata is available for agent introspection\n    \"\"\"\n\n    func: ToolCallable\n    name: str\n    description: str\n    _signature: ToolSignature = field(\n        default_factory=lambda: ToolSignature.empty(),  # type: ignore\n        repr=False,\n    )\n\n    @classmethod\n    def create(\n        cls,\n        func: ToolCallable,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n    ) -&gt; \"Tool\":\n        \"\"\"Create a new Tool instance from a callable.\n\n        Factory method that creates a Tool with proper signature analysis and\n        metadata extraction. Uses function's docstring as description if none\n        provided.\n\n        Args:\n            func: The function to convert into a tool.\n            name: Optional custom name for the tool. Defaults to function name.\n            description: Optional custom description. Defaults to docstring.\n\n        Returns:\n            A new Tool instance.\n\n        Raises:\n            ValueError: If function lacks required type hints\n                        or has invalid signature.\n\n        Example:\n            Basic tool creation:\n            ```python\n            def format_text(text: str, uppercase: bool = False) -&gt; str:\n                '''Format input text.'''\n                return text.upper() if uppercase else text\n            tool = Tool.create(format_text)\n            ```\n\n            Custom metadata:\n            ```python\n            tool = Tool.create(\n                format_text,\n                name=\"Formatter\",\n                description=\"Text formatting utility\"\n            )\n            ```\n        \"\"\"\n        actual_name = name or func.__name__\n        actual_description = (\n            description or getdoc(func) or \"No description available\"\n        )\n        signature = ToolSignature.from_callable(func, actual_name)\n\n        return cls(\n            func=func,\n            name=actual_name,\n            description=actual_description,\n            _signature=signature,\n        )\n\n    @property\n    def signature(self) -&gt; ToolSignature:\n        \"\"\"\n        Get the tool's signature information.\n\n        Provides access to the analyzed signature of the tool's function,\n        creating it if not already cached.\n\n        Returns:\n            Signature information for the tool.\n\n        Example:\n            ```python\n            @tool\n            def my_function(x: int, y: str) -&gt; str:\n                return f\"{y}: {x}\"\n            sig = my_function.signature\n            print(sig.parameters)  # Shows parameter information\n            ```\n        \"\"\"\n        if self._signature is None:\n            sig = ToolSignature.from_callable(self.func, self.name)\n            object.__setattr__(self, \"_signature\", sig)\n            return sig\n        return self._signature\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        Execute the tool's function with the provided arguments.\n\n        Makes the Tool instance callable, delegating to the underlying\n        function. This allows tools to be used like regular functions\n        while maintaining their metadata.\n\n        Args:\n            *args: Positional arguments to pass to the function.\n            **kwargs: Keyword arguments to pass to the function.\n\n        Returns:\n            The result of the tool's function execution.\n\n        Example:\n            Using a tool with positional arguments:\n            ```python\n            @tool\n            def calculate(x: int, y: int) -&gt; int:\n                return x + y\n            result = calculate(5, 3)\n            ```\n\n            Using a tool with keyword arguments:\n            ```python\n            result = calculate(x=5, y=3)\n            ```\n        \"\"\"\n        return self.func(*args, **kwargs)\n\n    @cached_property\n    def signature_str(self) -&gt; str:\n        \"\"\"\n        Get a string representation of the tool's signature.\n\n        Provides a cached, formatted string version of the tool's signature\n        for display purposes. This is useful for documentation and debugging.\n\n        Returns:\n            A formatted string representing the tool's signature.\n\n        Example:\n            ```python\n            @tool\n            def calculate(x: int, y: int) -&gt; int:\n                return x + y\n            print(calculate.signature_str)\n\n            # Output: \"calculate(x: int, y: int) -&gt; int\"\n            ```\n        \"\"\"\n        return self.signature.format()\n\n    def format_tool_info(self, indent: str = \"\") -&gt; str:\n        \"\"\"Format the tool's information in a standardized way for LLM prompts.\n\n        Creates a consistently formatted string representation of the tool\n        that includes its name, signature, and description in a hierarchical\n        format.\n\n        Args:\n            indent: Optional indentation prefix for each line.\n                   Useful for nested formatting. Defaults to no indentation.\n\n        Returns:\n            A formatted string containing the tool's complete information.\n\n        Example:\n            Basic formatting:\n            ```python\n            @tool(name=\"Calculator\")\n            def add(x: int, y: int) -&gt; int:\n                '''Add two numbers together.'''\n                return x + y\n            print(add.format_tool_info())\n            # Output:\n            # - Calculator\n            #   Signature: add(x: int, y: int) -&gt; int\n            #   Description: Add two numbers together\n            ```\n\n            With custom indentation:\n            ```python\n            print(add.format_tool_info(\"  \"))\n            # Output:\n            #   - Calculator\n            #     Signature: add(x: int, y: int) -&gt; int\n            #     Description: Add two numbers together\n            ```\n        \"\"\"\n        return (\n            f\"{indent}- {self.name}\\n\"\n            f\"{indent}  Signature: {self.signature_str}\\n\"\n            f\"{indent}  Description: {self.description}\"\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Get a complete string representation of the tool.\n\n        Provides a formatted string containing all relevant tool\n        information using the standardized format defined by\n        format_tool_info(). This ensures consistency between\n        string representation and prompt formatting.\n\n        Returns:\n            A formatted string containing the tool's complete information.\n\n        Example:\n            ```python\n            @tool(name=\"Calculator\")\n            def add(x: int, y: int) -&gt; int:\n                '''Add two numbers together.'''\n                return x + y\n            print(str(add))\n\n            # Output:\n            # - Calculator\n            #   Signature: add(x: int, y: int) -&gt; int\n            #   Description: Add two numbers together\n            ```\n\n        Note:\n            This method uses format_tool_info() to ensure consistency between\n            string representation and prompt formatting. The format is designed\n            to be both human-readable and suitable for LLM processing.\n        \"\"\"\n        return self.format_tool_info()\n</code></pre>"},{"location":"api/agent/tools/tool/#clientai.agent.tools.Tool.signature","title":"<code>signature: ToolSignature</code>  <code>property</code>","text":"<p>Get the tool's signature information.</p> <p>Provides access to the analyzed signature of the tool's function, creating it if not already cached.</p> <p>Returns:</p> Type Description <code>ToolSignature</code> <p>Signature information for the tool.</p> Example <pre><code>@tool\ndef my_function(x: int, y: str) -&gt; str:\n    return f\"{y}: {x}\"\nsig = my_function.signature\nprint(sig.parameters)  # Shows parameter information\n</code></pre>"},{"location":"api/agent/tools/tool/#clientai.agent.tools.Tool.signature_str","title":"<code>signature_str: str</code>  <code>cached</code> <code>property</code>","text":"<p>Get a string representation of the tool's signature.</p> <p>Provides a cached, formatted string version of the tool's signature for display purposes. This is useful for documentation and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>A formatted string representing the tool's signature.</p> Example <pre><code>@tool\ndef calculate(x: int, y: int) -&gt; int:\n    return x + y\nprint(calculate.signature_str)\n\n# Output: \"calculate(x: int, y: int) -&gt; int\"\n</code></pre>"},{"location":"api/agent/tools/tool/#clientai.agent.tools.Tool.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Execute the tool's function with the provided arguments.</p> <p>Makes the Tool instance callable, delegating to the underlying function. This allows tools to be used like regular functions while maintaining their metadata.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the tool's function execution.</p> Example <p>Using a tool with positional arguments: <pre><code>@tool\ndef calculate(x: int, y: int) -&gt; int:\n    return x + y\nresult = calculate(5, 3)\n</code></pre></p> <p>Using a tool with keyword arguments: <pre><code>result = calculate(x=5, y=3)\n</code></pre></p> Source code in <code>clientai/agent/tools/base.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Execute the tool's function with the provided arguments.\n\n    Makes the Tool instance callable, delegating to the underlying\n    function. This allows tools to be used like regular functions\n    while maintaining their metadata.\n\n    Args:\n        *args: Positional arguments to pass to the function.\n        **kwargs: Keyword arguments to pass to the function.\n\n    Returns:\n        The result of the tool's function execution.\n\n    Example:\n        Using a tool with positional arguments:\n        ```python\n        @tool\n        def calculate(x: int, y: int) -&gt; int:\n            return x + y\n        result = calculate(5, 3)\n        ```\n\n        Using a tool with keyword arguments:\n        ```python\n        result = calculate(x=5, y=3)\n        ```\n    \"\"\"\n    return self.func(*args, **kwargs)\n</code></pre>"},{"location":"api/agent/tools/tool/#clientai.agent.tools.Tool.__str__","title":"<code>__str__()</code>","text":"<p>Get a complete string representation of the tool.</p> <p>Provides a formatted string containing all relevant tool information using the standardized format defined by format_tool_info(). This ensures consistency between string representation and prompt formatting.</p> <p>Returns:</p> Type Description <code>str</code> <p>A formatted string containing the tool's complete information.</p> Example <pre><code>@tool(name=\"Calculator\")\ndef add(x: int, y: int) -&gt; int:\n    '''Add two numbers together.'''\n    return x + y\nprint(str(add))\n\n# Output:\n# - Calculator\n#   Signature: add(x: int, y: int) -&gt; int\n#   Description: Add two numbers together\n</code></pre> Note <p>This method uses format_tool_info() to ensure consistency between string representation and prompt formatting. The format is designed to be both human-readable and suitable for LLM processing.</p> Source code in <code>clientai/agent/tools/base.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Get a complete string representation of the tool.\n\n    Provides a formatted string containing all relevant tool\n    information using the standardized format defined by\n    format_tool_info(). This ensures consistency between\n    string representation and prompt formatting.\n\n    Returns:\n        A formatted string containing the tool's complete information.\n\n    Example:\n        ```python\n        @tool(name=\"Calculator\")\n        def add(x: int, y: int) -&gt; int:\n            '''Add two numbers together.'''\n            return x + y\n        print(str(add))\n\n        # Output:\n        # - Calculator\n        #   Signature: add(x: int, y: int) -&gt; int\n        #   Description: Add two numbers together\n        ```\n\n    Note:\n        This method uses format_tool_info() to ensure consistency between\n        string representation and prompt formatting. The format is designed\n        to be both human-readable and suitable for LLM processing.\n    \"\"\"\n    return self.format_tool_info()\n</code></pre>"},{"location":"api/agent/tools/tool/#clientai.agent.tools.Tool.create","title":"<code>create(func, name=None, description=None)</code>  <code>classmethod</code>","text":"<p>Create a new Tool instance from a callable.</p> <p>Factory method that creates a Tool with proper signature analysis and metadata extraction. Uses function's docstring as description if none provided.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>ToolCallable</code> <p>The function to convert into a tool.</p> required <code>name</code> <code>Optional[str]</code> <p>Optional custom name for the tool. Defaults to function name.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Optional custom description. Defaults to docstring.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tool</code> <p>A new Tool instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If function lacks required type hints         or has invalid signature.</p> Example <p>Basic tool creation: <pre><code>def format_text(text: str, uppercase: bool = False) -&gt; str:\n    '''Format input text.'''\n    return text.upper() if uppercase else text\ntool = Tool.create(format_text)\n</code></pre></p> <p>Custom metadata: <pre><code>tool = Tool.create(\n    format_text,\n    name=\"Formatter\",\n    description=\"Text formatting utility\"\n)\n</code></pre></p> Source code in <code>clientai/agent/tools/base.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    func: ToolCallable,\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n) -&gt; \"Tool\":\n    \"\"\"Create a new Tool instance from a callable.\n\n    Factory method that creates a Tool with proper signature analysis and\n    metadata extraction. Uses function's docstring as description if none\n    provided.\n\n    Args:\n        func: The function to convert into a tool.\n        name: Optional custom name for the tool. Defaults to function name.\n        description: Optional custom description. Defaults to docstring.\n\n    Returns:\n        A new Tool instance.\n\n    Raises:\n        ValueError: If function lacks required type hints\n                    or has invalid signature.\n\n    Example:\n        Basic tool creation:\n        ```python\n        def format_text(text: str, uppercase: bool = False) -&gt; str:\n            '''Format input text.'''\n            return text.upper() if uppercase else text\n        tool = Tool.create(format_text)\n        ```\n\n        Custom metadata:\n        ```python\n        tool = Tool.create(\n            format_text,\n            name=\"Formatter\",\n            description=\"Text formatting utility\"\n        )\n        ```\n    \"\"\"\n    actual_name = name or func.__name__\n    actual_description = (\n        description or getdoc(func) or \"No description available\"\n    )\n    signature = ToolSignature.from_callable(func, actual_name)\n\n    return cls(\n        func=func,\n        name=actual_name,\n        description=actual_description,\n        _signature=signature,\n    )\n</code></pre>"},{"location":"api/agent/tools/tool/#clientai.agent.tools.Tool.format_tool_info","title":"<code>format_tool_info(indent='')</code>","text":"<p>Format the tool's information in a standardized way for LLM prompts.</p> <p>Creates a consistently formatted string representation of the tool that includes its name, signature, and description in a hierarchical format.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <code>str</code> <p>Optional indentation prefix for each line.    Useful for nested formatting. Defaults to no indentation.</p> <code>''</code> <p>Returns:</p> Type Description <code>str</code> <p>A formatted string containing the tool's complete information.</p> Example <p>Basic formatting: <pre><code>@tool(name=\"Calculator\")\ndef add(x: int, y: int) -&gt; int:\n    '''Add two numbers together.'''\n    return x + y\nprint(add.format_tool_info())\n# Output:\n# - Calculator\n#   Signature: add(x: int, y: int) -&gt; int\n#   Description: Add two numbers together\n</code></pre></p> <p>With custom indentation: <pre><code>print(add.format_tool_info(\"  \"))\n# Output:\n#   - Calculator\n#     Signature: add(x: int, y: int) -&gt; int\n#     Description: Add two numbers together\n</code></pre></p> Source code in <code>clientai/agent/tools/base.py</code> <pre><code>def format_tool_info(self, indent: str = \"\") -&gt; str:\n    \"\"\"Format the tool's information in a standardized way for LLM prompts.\n\n    Creates a consistently formatted string representation of the tool\n    that includes its name, signature, and description in a hierarchical\n    format.\n\n    Args:\n        indent: Optional indentation prefix for each line.\n               Useful for nested formatting. Defaults to no indentation.\n\n    Returns:\n        A formatted string containing the tool's complete information.\n\n    Example:\n        Basic formatting:\n        ```python\n        @tool(name=\"Calculator\")\n        def add(x: int, y: int) -&gt; int:\n            '''Add two numbers together.'''\n            return x + y\n        print(add.format_tool_info())\n        # Output:\n        # - Calculator\n        #   Signature: add(x: int, y: int) -&gt; int\n        #   Description: Add two numbers together\n        ```\n\n        With custom indentation:\n        ```python\n        print(add.format_tool_info(\"  \"))\n        # Output:\n        #   - Calculator\n        #     Signature: add(x: int, y: int) -&gt; int\n        #     Description: Add two numbers together\n        ```\n    \"\"\"\n    return (\n        f\"{indent}- {self.name}\\n\"\n        f\"{indent}  Signature: {self.signature_str}\\n\"\n        f\"{indent}  Description: {self.description}\"\n    )\n</code></pre>"},{"location":"api/client/ai_provider/","title":"AIProvider Class API Reference","text":"<p>The <code>AIProvider</code> class is an abstract base class that defines the interface for all AI provider implementations in ClientAI. It ensures consistency across different providers.</p>"},{"location":"api/client/ai_provider/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for AI providers.</p> Source code in <code>clientai/ai_provider.py</code> <pre><code>class AIProvider(ABC):\n    \"\"\"\n    Abstract base class for AI providers.\n    \"\"\"\n\n    @abstractmethod\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        system_prompt: Optional[str] = None,\n        return_full_response: bool = False,\n        stream: bool = False,\n        json_output: bool = False,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; GenericResponse:\n        \"\"\"\n        Generate text based on a given prompt.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the AI model to use.\n            system_prompt: Optional system prompt to guide model behavior.\n            return_full_response: If True, return the full response object\n                                  instead of just the generated text.\n            stream: If True, return an iterator for streaming responses.\n            json_output: If True, format the response as valid JSON.\n                        Each provider uses its native JSON support mechanism.\n            temperature: Optional temperature value controlling randomness.\n                        Usually between 0.0 and 2.0, with lower values making\n                        the output more focused and deterministic, and higher\n                        values making it more creative and variable.\n            top_p: Optional nucleus sampling parameter controlling diversity.\n                  Usually between 0.0 and 1.0, with lower values making the\n                  output more focused on likely tokens, and higher values\n                  allowing more diverse selections.\n            **kwargs: Additional keyword arguments specific to\n                      the provider's API.\n\n        Returns:\n            GenericResponse:\n                The generated text response, full response object,\n                or an iterator for streaming responses.\n\n        Note:\n            When json_output is True:\n            - OpenAI/Groq use response_format={\"type\": \"json_object\"}\n            - Replicate adds output=\"json\" to input parameters\n            - Ollama uses format=\"json\" parameter\n\n            Temperature ranges:\n            - OpenAI: 0.0 to 2.0 (default: 1.0)\n            - Ollama: 0.0 to 2.0 (default: 0.8)\n            - Replicate: Model-dependent\n            - Groq: 0.0 to 2.0 (default: 1.0)\n\n            Top-p ranges:\n            - OpenAI: 0.0 to 1.0 (default: 1.0)\n            - Ollama: 0.0 to 1.0 (default: 0.9)\n            - Replicate: Model-dependent\n            - Groq: 0.0 to 1.0 (default: 1.0)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        system_prompt: Optional[str] = None,\n        return_full_response: bool = False,\n        stream: bool = False,\n        json_output: bool = False,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; GenericResponse:\n        \"\"\"\n        Engage in a chat conversation.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the AI model to use.\n            system_prompt: Optional system prompt to guide model behavior.\n            return_full_response: If True, return the full response object\n                                  instead of just the chat content.\n            stream: If True, return an iterator for streaming responses.\n            json_output: If True, format the response as valid JSON.\n                        Each provider uses its native JSON support mechanism.\n            temperature: Optional temperature value controlling randomness.\n                        Usually between 0.0 and 2.0, with lower values making\n                        the output more focused and deterministic, and higher\n                        values making it more creative and variable.\n            top_p: Optional nucleus sampling parameter controlling diversity.\n                  Usually between 0.0 and 1.0, with lower values making the\n                  output more focused on likely tokens, and higher values\n                  allowing more diverse selections.\n            **kwargs: Additional keyword arguments specific to\n                      the provider's API.\n\n        Returns:\n            GenericResponse:\n                The chat response, either as a string, a dictionary,\n                or an iterator for streaming responses.\n\n        Note:\n            When json_output is True:\n            - OpenAI/Groq use response_format={\"type\": \"json_object\"}\n            - Replicate adds output=\"json\" to input parameters\n            - Ollama uses format=\"json\" parameter\n\n            Temperature ranges:\n            - OpenAI: 0.0 to 2.0 (default: 1.0)\n            - Ollama: 0.0 to 2.0 (default: 0.8)\n            - Replicate: Model-dependent\n            - Groq: 0.0 to 2.0 (default: 1.0)\n\n            Top-p ranges:\n            - OpenAI: 0.0 to 1.0 (default: 1.0)\n            - Ollama: 0.0 to 1.0 (default: 0.9)\n            - Replicate: Model-dependent\n            - Groq: 0.0 to 1.0 (default: 1.0)\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/client/ai_provider/#clientai.ai_provider.AIProvider.chat","title":"<code>chat(messages, model, system_prompt=None, return_full_response=False, stream=False, json_output=False, temperature=None, top_p=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Engage in a chat conversation.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to guide model behavior.</p> <code>None</code> <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object                   instead of just the chat content.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses.</p> <code>False</code> <code>json_output</code> <code>bool</code> <p>If True, format the response as valid JSON.         Each provider uses its native JSON support mechanism.</p> <code>False</code> <code>temperature</code> <code>Optional[float]</code> <p>Optional temperature value controlling randomness.         Usually between 0.0 and 2.0, with lower values making         the output more focused and deterministic, and higher         values making it more creative and variable.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Optional nucleus sampling parameter controlling diversity.   Usually between 0.0 and 1.0, with lower values making the   output more focused on likely tokens, and higher values   allowing more diverse selections.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to       the provider's API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GenericResponse</code> <code>GenericResponse</code> <p>The chat response, either as a string, a dictionary, or an iterator for streaming responses.</p> Note <p>When json_output is True: - OpenAI/Groq use response_format={\"type\": \"json_object\"} - Replicate adds output=\"json\" to input parameters - Ollama uses format=\"json\" parameter</p> <p>Temperature ranges: - OpenAI: 0.0 to 2.0 (default: 1.0) - Ollama: 0.0 to 2.0 (default: 0.8) - Replicate: Model-dependent - Groq: 0.0 to 2.0 (default: 1.0)</p> <p>Top-p ranges: - OpenAI: 0.0 to 1.0 (default: 1.0) - Ollama: 0.0 to 1.0 (default: 0.9) - Replicate: Model-dependent - Groq: 0.0 to 1.0 (default: 1.0)</p> Source code in <code>clientai/ai_provider.py</code> <pre><code>@abstractmethod\ndef chat(\n    self,\n    messages: List[Message],\n    model: str,\n    system_prompt: Optional[str] = None,\n    return_full_response: bool = False,\n    stream: bool = False,\n    json_output: bool = False,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; GenericResponse:\n    \"\"\"\n    Engage in a chat conversation.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the AI model to use.\n        system_prompt: Optional system prompt to guide model behavior.\n        return_full_response: If True, return the full response object\n                              instead of just the chat content.\n        stream: If True, return an iterator for streaming responses.\n        json_output: If True, format the response as valid JSON.\n                    Each provider uses its native JSON support mechanism.\n        temperature: Optional temperature value controlling randomness.\n                    Usually between 0.0 and 2.0, with lower values making\n                    the output more focused and deterministic, and higher\n                    values making it more creative and variable.\n        top_p: Optional nucleus sampling parameter controlling diversity.\n              Usually between 0.0 and 1.0, with lower values making the\n              output more focused on likely tokens, and higher values\n              allowing more diverse selections.\n        **kwargs: Additional keyword arguments specific to\n                  the provider's API.\n\n    Returns:\n        GenericResponse:\n            The chat response, either as a string, a dictionary,\n            or an iterator for streaming responses.\n\n    Note:\n        When json_output is True:\n        - OpenAI/Groq use response_format={\"type\": \"json_object\"}\n        - Replicate adds output=\"json\" to input parameters\n        - Ollama uses format=\"json\" parameter\n\n        Temperature ranges:\n        - OpenAI: 0.0 to 2.0 (default: 1.0)\n        - Ollama: 0.0 to 2.0 (default: 0.8)\n        - Replicate: Model-dependent\n        - Groq: 0.0 to 2.0 (default: 1.0)\n\n        Top-p ranges:\n        - OpenAI: 0.0 to 1.0 (default: 1.0)\n        - Ollama: 0.0 to 1.0 (default: 0.9)\n        - Replicate: Model-dependent\n        - Groq: 0.0 to 1.0 (default: 1.0)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/client/ai_provider/#clientai.ai_provider.AIProvider.generate_text","title":"<code>generate_text(prompt, model, system_prompt=None, return_full_response=False, stream=False, json_output=False, temperature=None, top_p=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate text based on a given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to guide model behavior.</p> <code>None</code> <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object                   instead of just the generated text.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses.</p> <code>False</code> <code>json_output</code> <code>bool</code> <p>If True, format the response as valid JSON.         Each provider uses its native JSON support mechanism.</p> <code>False</code> <code>temperature</code> <code>Optional[float]</code> <p>Optional temperature value controlling randomness.         Usually between 0.0 and 2.0, with lower values making         the output more focused and deterministic, and higher         values making it more creative and variable.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Optional nucleus sampling parameter controlling diversity.   Usually between 0.0 and 1.0, with lower values making the   output more focused on likely tokens, and higher values   allowing more diverse selections.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to       the provider's API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GenericResponse</code> <code>GenericResponse</code> <p>The generated text response, full response object, or an iterator for streaming responses.</p> Note <p>When json_output is True: - OpenAI/Groq use response_format={\"type\": \"json_object\"} - Replicate adds output=\"json\" to input parameters - Ollama uses format=\"json\" parameter</p> <p>Temperature ranges: - OpenAI: 0.0 to 2.0 (default: 1.0) - Ollama: 0.0 to 2.0 (default: 0.8) - Replicate: Model-dependent - Groq: 0.0 to 2.0 (default: 1.0)</p> <p>Top-p ranges: - OpenAI: 0.0 to 1.0 (default: 1.0) - Ollama: 0.0 to 1.0 (default: 0.9) - Replicate: Model-dependent - Groq: 0.0 to 1.0 (default: 1.0)</p> Source code in <code>clientai/ai_provider.py</code> <pre><code>@abstractmethod\ndef generate_text(\n    self,\n    prompt: str,\n    model: str,\n    system_prompt: Optional[str] = None,\n    return_full_response: bool = False,\n    stream: bool = False,\n    json_output: bool = False,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; GenericResponse:\n    \"\"\"\n    Generate text based on a given prompt.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the AI model to use.\n        system_prompt: Optional system prompt to guide model behavior.\n        return_full_response: If True, return the full response object\n                              instead of just the generated text.\n        stream: If True, return an iterator for streaming responses.\n        json_output: If True, format the response as valid JSON.\n                    Each provider uses its native JSON support mechanism.\n        temperature: Optional temperature value controlling randomness.\n                    Usually between 0.0 and 2.0, with lower values making\n                    the output more focused and deterministic, and higher\n                    values making it more creative and variable.\n        top_p: Optional nucleus sampling parameter controlling diversity.\n              Usually between 0.0 and 1.0, with lower values making the\n              output more focused on likely tokens, and higher values\n              allowing more diverse selections.\n        **kwargs: Additional keyword arguments specific to\n                  the provider's API.\n\n    Returns:\n        GenericResponse:\n            The generated text response, full response object,\n            or an iterator for streaming responses.\n\n    Note:\n        When json_output is True:\n        - OpenAI/Groq use response_format={\"type\": \"json_object\"}\n        - Replicate adds output=\"json\" to input parameters\n        - Ollama uses format=\"json\" parameter\n\n        Temperature ranges:\n        - OpenAI: 0.0 to 2.0 (default: 1.0)\n        - Ollama: 0.0 to 2.0 (default: 0.8)\n        - Replicate: Model-dependent\n        - Groq: 0.0 to 2.0 (default: 1.0)\n\n        Top-p ranges:\n        - OpenAI: 0.0 to 1.0 (default: 1.0)\n        - Ollama: 0.0 to 1.0 (default: 0.9)\n        - Replicate: Model-dependent\n        - Groq: 0.0 to 1.0 (default: 1.0)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/client/clientai/","title":"ClientAI Class API Reference","text":"<p>The <code>ClientAI</code> class is the primary interface for interacting with various AI providers in a unified manner. It provides methods for text generation and chat functionality across different AI services.</p>"},{"location":"api/client/clientai/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>Generic[P, T, S]</code></p> <p>A unified client for interacting with a single AI provider (OpenAI, Replicate, Ollama, or Groq).</p> <p>This class provides a consistent interface for common AI operations such as text generation and chat for the chosen AI provider.</p> <p>Type Parameters: P: The type of the AI provider. T: The type of the full response for non-streaming operations. S: The type of each chunk in streaming operations.</p> <p>Attributes:</p> Name Type Description <code>provider</code> <p>The initialized AI provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>The name of the AI provider to use            ('openai', 'replicate', 'ollama', or 'groq').</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to guide the model's behavior            across all interactions.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>Optional default temperature value for all interactions.         Controls randomness in the output (usually 0.0-2.0).</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Optional default top-p value for all interactions.    Controls diversity via nucleus sampling (usually 0.0-1.0).</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Provider-specific initialization parameters.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported provider name is given.</p> <code>ImportError</code> <p>If the specified provider is not installed.</p> Example <p>Initialize with OpenAI: <pre><code>ai = ClientAI('openai', api_key=\"your-openai-key\")\n</code></pre></p> <p>Initialize with custom generation parameters: <pre><code>ai = ClientAI(\n    'openai',\n    api_key=\"your-openai-key\",\n    temperature=0.8,\n    top_p=0.9\n)\n</code></pre></p> Source code in <code>clientai/client_ai.py</code> <pre><code>class ClientAI(Generic[P, T, S]):\n    \"\"\"\n    A unified client for interacting with a single AI provider\n    (OpenAI, Replicate, Ollama, or Groq).\n\n    This class provides a consistent interface for common\n    AI operations such as text generation and chat\n    for the chosen AI provider.\n\n    Type Parameters:\n    P: The type of the AI provider.\n    T: The type of the full response for non-streaming operations.\n    S: The type of each chunk in streaming operations.\n\n    Attributes:\n        provider: The initialized AI provider.\n\n    Args:\n        provider_name: The name of the AI provider to use\n                       ('openai', 'replicate', 'ollama', or 'groq').\n        system_prompt: Optional system prompt to guide the model's behavior\n                       across all interactions.\n        temperature: Optional default temperature value for all interactions.\n                    Controls randomness in the output (usually 0.0-2.0).\n        top_p: Optional default top-p value for all interactions.\n               Controls diversity via nucleus sampling (usually 0.0-1.0).\n        **kwargs (Any): Provider-specific initialization parameters.\n\n    Raises:\n        ValueError: If an unsupported provider name is given.\n        ImportError: If the specified provider is not installed.\n\n    Example:\n        Initialize with OpenAI:\n        ```python\n        ai = ClientAI('openai', api_key=\"your-openai-key\")\n        ```\n\n        Initialize with custom generation parameters:\n        ```python\n        ai = ClientAI(\n            'openai',\n            api_key=\"your-openai-key\",\n            temperature=0.8,\n            top_p=0.9\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        provider_name: str,\n        system_prompt: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        **kwargs,\n    ):\n        prov_name = provider_name\n        if prov_name not in [\"openai\", \"replicate\", \"ollama\", \"groq\"]:\n            raise ValueError(f\"Unsupported provider: {prov_name}\")\n\n        if (\n            prov_name == \"openai\"\n            and not OPENAI_INSTALLED\n            or prov_name == \"replicate\"\n            and not REPLICATE_INSTALLED\n            or prov_name == \"ollama\"\n            and not OLLAMA_INSTALLED\n            or prov_name == \"groq\"\n            and not GROQ_INSTALLED\n        ):\n            raise ImportError(\n                f\"The {prov_name} provider is not installed. \"\n                f\"Please install it with 'pip install clientai[{prov_name}]'.\"\n            )\n\n        self.system_prompt = system_prompt\n        self.temperature = temperature\n        self.top_p = top_p\n\n        try:\n            provider_module = import_module(\n                f\".{prov_name}.provider\", package=\"clientai\"\n            )\n            provider_class = getattr(provider_module, \"Provider\")\n            if prov_name in [\"openai\", \"replicate\", \"groq\"]:\n                self.provider = cast(\n                    P, provider_class(api_key=kwargs.get(\"api_key\"))\n                )\n            elif prov_name == \"ollama\":\n                self.provider = cast(\n                    P, provider_class(host=kwargs.get(\"host\"))\n                )\n        except ImportError as e:\n            raise ImportError(\n                f\"Error importing {prov_name} provider module: {str(e)}\"\n            ) from e\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        system_prompt: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; AIGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt\n        using the specified AI model and provider.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the AI model to use.\n            system_prompt: Optional system prompt to override the default one.\n                           If None, uses the one specified in initialization.\n            temperature: Optional temperature value to override the default.\n                        Controls randomness (usually 0.0-2.0).\n            top_p: Optional top-p value to override the default.\n                  Controls diversity via nucleus sampling (usually 0.0-1.0).\n            return_full_response: If True, returns the full structured\n                                  response. If False, returns only the\n                                  generated text.\n            stream: If True, returns an iterator for streaming responses.\n            **kwargs: Additional keyword arguments specific to\n                      the chosen provider's API.\n\n        Returns:\n            AIGenericResponse:\n                The generated text response, full response structure,\n                or an iterator for streaming responses.\n\n        Example:\n            Generate text with default settings:\n            ```python\n            response = ai.generate_text(\n                \"Tell me a joke\",\n                model=\"gpt-3.5-turbo\",\n            )\n            ```\n\n            Generate creative text with high temperature:\n            ```python\n            response = ai.generate_text(\n                \"Write a story about space\",\n                model=\"gpt-3.5-turbo\",\n                temperature=0.8,\n                top_p=0.9\n            )\n            ```\n        \"\"\"\n        effective_system_prompt = system_prompt or self.system_prompt\n        effective_temperature = (\n            temperature if temperature is not None else self.temperature\n        )\n        effective_top_p = top_p if top_p is not None else self.top_p\n\n        return self.provider.generate_text(\n            prompt,\n            model,\n            system_prompt=effective_system_prompt,\n            temperature=effective_temperature,\n            top_p=effective_top_p,\n            return_full_response=return_full_response,\n            stream=stream,\n            **kwargs,\n        )\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        system_prompt: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; AIGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using\n        the specified AI model and provider.\n\n        Args:\n            messages: A list of message dictionaries, each\n                      containing 'role' and 'content'.\n            model: The name or identifier of the AI model to use.\n            system_prompt: Optional system prompt to override the default one.\n                           If None, uses the one specified in initialization.\n            temperature: Optional temperature value to override the default.\n                        Controls randomness (usually 0.0-2.0).\n            top_p: Optional top-p value to override the default.\n                  Controls diversity via nucleus sampling (usually 0.0-1.0).\n            return_full_response: If True, returns the full structured\n                                  response. If False, returns the\n                                  assistant's message.\n            stream: If True, returns an iterator for streaming responses.\n            **kwargs: Additional keyword arguments specific to\n                      the chosen provider's API.\n\n        Returns:\n            AIGenericResponse:\n                The chat response, full response structure,\n                or an iterator for streaming responses.\n\n        Example:\n            Chat with default settings:\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is AI?\"}\n            ]\n            response = ai.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n            )\n            ```\n\n            Creative chat with custom temperature:\n            ```python\n            response = ai.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n                temperature=0.8,\n                top_p=0.9\n            )\n            ```\n        \"\"\"\n        effective_system_prompt = system_prompt or self.system_prompt\n        effective_temperature = (\n            temperature if temperature is not None else self.temperature\n        )\n        effective_top_p = top_p if top_p is not None else self.top_p\n\n        return self.provider.chat(\n            messages,\n            model,\n            system_prompt=effective_system_prompt,\n            temperature=effective_temperature,\n            top_p=effective_top_p,\n            return_full_response=return_full_response,\n            stream=stream,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/client/clientai/#clientai.ClientAI.chat","title":"<code>chat(messages, model, system_prompt=None, temperature=None, top_p=None, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Engage in a chat conversation using the specified AI model and provider.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each       containing 'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to override the default one.            If None, uses the one specified in initialization.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>Optional temperature value to override the default.         Controls randomness (usually 0.0-2.0).</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Optional top-p value to override the default.   Controls diversity via nucleus sampling (usually 0.0-1.0).</p> <code>None</code> <code>return_full_response</code> <code>bool</code> <p>If True, returns the full structured                   response. If False, returns the                   assistant's message.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, returns an iterator for streaming responses.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to       the chosen provider's API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AIGenericResponse</code> <code>AIGenericResponse</code> <p>The chat response, full response structure, or an iterator for streaming responses.</p> Example <p>Chat with default settings: <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is AI?\"}\n]\nresponse = ai.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n)\n</code></pre></p> <p>Creative chat with custom temperature: <pre><code>response = ai.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    temperature=0.8,\n    top_p=0.9\n)\n</code></pre></p> Source code in <code>clientai/client_ai.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    system_prompt: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; AIGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using\n    the specified AI model and provider.\n\n    Args:\n        messages: A list of message dictionaries, each\n                  containing 'role' and 'content'.\n        model: The name or identifier of the AI model to use.\n        system_prompt: Optional system prompt to override the default one.\n                       If None, uses the one specified in initialization.\n        temperature: Optional temperature value to override the default.\n                    Controls randomness (usually 0.0-2.0).\n        top_p: Optional top-p value to override the default.\n              Controls diversity via nucleus sampling (usually 0.0-1.0).\n        return_full_response: If True, returns the full structured\n                              response. If False, returns the\n                              assistant's message.\n        stream: If True, returns an iterator for streaming responses.\n        **kwargs: Additional keyword arguments specific to\n                  the chosen provider's API.\n\n    Returns:\n        AIGenericResponse:\n            The chat response, full response structure,\n            or an iterator for streaming responses.\n\n    Example:\n        Chat with default settings:\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is AI?\"}\n        ]\n        response = ai.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n        )\n        ```\n\n        Creative chat with custom temperature:\n        ```python\n        response = ai.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n            temperature=0.8,\n            top_p=0.9\n        )\n        ```\n    \"\"\"\n    effective_system_prompt = system_prompt or self.system_prompt\n    effective_temperature = (\n        temperature if temperature is not None else self.temperature\n    )\n    effective_top_p = top_p if top_p is not None else self.top_p\n\n    return self.provider.chat(\n        messages,\n        model,\n        system_prompt=effective_system_prompt,\n        temperature=effective_temperature,\n        top_p=effective_top_p,\n        return_full_response=return_full_response,\n        stream=stream,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/client/clientai/#clientai.ClientAI.generate_text","title":"<code>generate_text(prompt, model, system_prompt=None, temperature=None, top_p=None, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Generate text based on a given prompt using the specified AI model and provider.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to override the default one.            If None, uses the one specified in initialization.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>Optional temperature value to override the default.         Controls randomness (usually 0.0-2.0).</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Optional top-p value to override the default.   Controls diversity via nucleus sampling (usually 0.0-1.0).</p> <code>None</code> <code>return_full_response</code> <code>bool</code> <p>If True, returns the full structured                   response. If False, returns only the                   generated text.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, returns an iterator for streaming responses.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to       the chosen provider's API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AIGenericResponse</code> <code>AIGenericResponse</code> <p>The generated text response, full response structure, or an iterator for streaming responses.</p> Example <p>Generate text with default settings: <pre><code>response = ai.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n)\n</code></pre></p> <p>Generate creative text with high temperature: <pre><code>response = ai.generate_text(\n    \"Write a story about space\",\n    model=\"gpt-3.5-turbo\",\n    temperature=0.8,\n    top_p=0.9\n)\n</code></pre></p> Source code in <code>clientai/client_ai.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    system_prompt: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; AIGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt\n    using the specified AI model and provider.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the AI model to use.\n        system_prompt: Optional system prompt to override the default one.\n                       If None, uses the one specified in initialization.\n        temperature: Optional temperature value to override the default.\n                    Controls randomness (usually 0.0-2.0).\n        top_p: Optional top-p value to override the default.\n              Controls diversity via nucleus sampling (usually 0.0-1.0).\n        return_full_response: If True, returns the full structured\n                              response. If False, returns only the\n                              generated text.\n        stream: If True, returns an iterator for streaming responses.\n        **kwargs: Additional keyword arguments specific to\n                  the chosen provider's API.\n\n    Returns:\n        AIGenericResponse:\n            The generated text response, full response structure,\n            or an iterator for streaming responses.\n\n    Example:\n        Generate text with default settings:\n        ```python\n        response = ai.generate_text(\n            \"Tell me a joke\",\n            model=\"gpt-3.5-turbo\",\n        )\n        ```\n\n        Generate creative text with high temperature:\n        ```python\n        response = ai.generate_text(\n            \"Write a story about space\",\n            model=\"gpt-3.5-turbo\",\n            temperature=0.8,\n            top_p=0.9\n        )\n        ```\n    \"\"\"\n    effective_system_prompt = system_prompt or self.system_prompt\n    effective_temperature = (\n        temperature if temperature is not None else self.temperature\n    )\n    effective_top_p = top_p if top_p is not None else self.top_p\n\n    return self.provider.generate_text(\n        prompt,\n        model,\n        system_prompt=effective_system_prompt,\n        temperature=effective_temperature,\n        top_p=effective_top_p,\n        return_full_response=return_full_response,\n        stream=stream,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/client/ollama_manager/ollama_manager/","title":"OllamaManager Class API Reference","text":"<p>The <code>OllamaManager</code> class is a utility class that manages the lifecycle of a local Ollama server instance. It handles server process startup, monitoring, and shutdown while respecting platform-specific requirements and custom configurations. The manager supports configurable GPU acceleration, CPU thread allocation, and memory limits through <code>OllamaServerConfig</code>. It provides both context manager and manual management interfaces for controlling the server process.</p>"},{"location":"api/client/ollama_manager/ollama_manager/#class-definition","title":"Class Definition","text":"<p>Manages the Ollama server process and configuration.</p> <p>This class provides methods to start, stop, and manage the lifecycle of an Ollama server instance with configurable parameters.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>The server configuration used by the manager.</p> <code>_process</code> <code>Optional[Popen[str]]</code> <p>The underlying server process.</p> <code>_platform_info</code> <p>Information about the current platform.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[OllamaServerConfig]</code> <p>Optional server configuration. If None, uses defaults.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required system dependencies are not installed.</p> Example <p>Basic usage with defaults: <pre><code>with OllamaManager() as manager:\n    # Server is running with default configuration\n    pass  # Server automatically stops when exiting context\n</code></pre></p> <p>Custom configuration: <pre><code>config = OllamaServerConfig(\n    gpu_layers=35,\n    gpu_memory_fraction=0.8,\n    cpu_threads=8\n)\nmanager = OllamaManager(config)\nmanager.start()\n# ... use the server ...\nmanager.stop()\n</code></pre></p> Source code in <code>clientai/ollama/manager/core.py</code> <pre><code>class OllamaManager:\n    \"\"\"\n    Manages the Ollama server process and configuration.\n\n    This class provides methods to start, stop, and manage the lifecycle\n    of an Ollama server instance with configurable parameters.\n\n    Attributes:\n        config: The server configuration used by the manager.\n        _process: The underlying server process.\n        _platform_info: Information about the current platform.\n\n    Args:\n        config: Optional server configuration. If None, uses defaults.\n\n    Raises:\n        ImportError: If required system dependencies are not installed.\n\n    Example:\n        Basic usage with defaults:\n        ```python\n        with OllamaManager() as manager:\n            # Server is running with default configuration\n            pass  # Server automatically stops when exiting context\n        ```\n\n        Custom configuration:\n        ```python\n        config = OllamaServerConfig(\n            gpu_layers=35,\n            gpu_memory_fraction=0.8,\n            cpu_threads=8\n        )\n        manager = OllamaManager(config)\n        manager.start()\n        # ... use the server ...\n        manager.stop()\n        ```\n    \"\"\"\n\n    def __init__(self, config: Optional[OllamaServerConfig] = None) -&gt; None:\n        \"\"\"\n        Initialize the Ollama manager.\n\n        Args:\n            config: Optional server configuration. If None, uses defaults.\n        \"\"\"\n        self.config = config or OllamaServerConfig()\n        self._process: Optional[subprocess.Popen[str]] = None\n        self._platform_info = PlatformInfo()\n\n    def start(self) -&gt; None:\n        \"\"\"\n        Start the Ollama server using the configured parameters.\n\n        This method initializes and starts the Ollama server process,\n        waiting for it to become healthy before returning.\n\n        Raises:\n            ServerStartupError: If the server fails to start\n            ServerTimeoutError: If the server doesn't respond within timeout\n            ExecutableNotFoundError: If the Ollama executable is not found\n            ResourceError: If there are insufficient system resources\n\n        Example:\n            Start with default configuration:\n            ```python\n            manager = OllamaManager()\n            manager.start()\n            ```\n\n            Start with custom configuration:\n            ```python\n            config = OllamaServerConfig(gpu_layers=35)\n            manager = OllamaManager(config)\n            manager.start()\n            ```\n        \"\"\"\n        if self._process is not None:\n            raise ServerStartupError(\"Ollama server is already running\")\n\n        logging.info(\n            f\"Starting Ollama server on {self.config.host}:{self.config.port}\"\n        )\n\n        try:\n            popen_kwargs: Dict[str, Any] = {\n                \"stdout\": subprocess.PIPE,\n                \"stderr\": subprocess.PIPE,\n                \"text\": True,\n                \"env\": self._platform_info.get_environment(self.config),\n            }\n\n            if self._platform_info.platform == Platform.WINDOWS:\n                if sys.platform == \"win32\":\n                    popen_kwargs[\"creationflags\"] = subprocess.CREATE_NO_WINDOW\n\n            self._process = subprocess.Popen(\n                self._platform_info.get_server_command(self.config),\n                **popen_kwargs,\n            )\n\n        except FileNotFoundError as e:\n            raise_ollama_error(\n                ExecutableNotFoundError,\n                \"Ollama executable not found. Ensure Ollama is installed.\",\n                e,\n            )\n        except MemoryError as e:\n            raise_ollama_error(\n                ResourceError, \"Insufficient memory to start Ollama server\", e\n            )\n        except Exception as e:\n            raise_ollama_error(\n                ServerStartupError,\n                f\"Failed to start Ollama server: {str(e)}\",\n                e,\n            )\n\n        try:\n            self._wait_for_server()\n        except Exception as e:\n            self.stop()\n            raise e\n\n    def stop(self) -&gt; None:\n        \"\"\"\n        Stop the running Ollama server instance.\n\n        This method terminates the Ollama server process if it's running.\n        It will wait for the process to complete before returning.\n\n        Example:\n            Stop a running server:\n            ```python\n            manager = OllamaManager()\n            manager.start()\n            # ... use the server ...\n            manager.stop()\n            ```\n\n            Using context manager (automatic stop):\n            ```python\n            with OllamaManager() as manager:\n                # ... use the server ...\n                pass  # Server stops automatically\n            ```\n        \"\"\"\n        if self._process is not None:\n            try:\n                self._process.terminate()\n                self._process.wait()\n            finally:\n                self._process = None\n                logging.info(\"Ollama server stopped\")\n\n    def _check_server_health(self) -&gt; bool:\n        \"\"\"\n        Check if the server is responding to health checks.\n\n        This method attempts to connect to the server and verify\n        its health status.\n\n        Returns:\n            bool: True if server is healthy, False otherwise\n\n        Note:\n            This is an internal method used by the manager to verify\n            server status during startup.\n        \"\"\"\n        try:\n            url = urlparse(self.config.base_url)\n            conn = http.client.HTTPConnection(\n                url.hostname or self.config.host,\n                url.port or self.config.port,\n                timeout=5,\n            )\n            try:\n                conn.request(\"GET\", \"/\")\n                response = conn.getresponse()\n                return response.status == 200\n            finally:\n                conn.close()\n        except (http.client.HTTPException, ConnectionRefusedError, OSError):\n            return False\n\n    def _wait_for_server(self) -&gt; None:\n        \"\"\"\n        Wait for the server to become ready and responsive.\n\n        This method polls the server until it responds successfully or\n        times out. It checks both process health and server responsiveness.\n\n        Raises:\n            ServerStartupError: If the server process terminates unexpectedly\n            ServerTimeoutError: If the server doesn't respond within timeout\n\n        Note:\n            This is an internal method used during the server startup process.\n        \"\"\"\n        start_time = time.time()\n\n        while time.time() - start_time &lt; self.config.timeout:\n            process = cast(subprocess.Popen[str], self._process)\n\n            if process.poll() is not None:\n                stdout, stderr = process.communicate()\n                error_msg = (\n                    f\"Ollama process terminated unexpectedly.\\n\"\n                    f\"Exit code: {process.returncode}\\n\"\n                    f\"stdout: {stdout}\\n\"\n                    f\"stderr: {stderr}\"\n                )\n                self._process = None\n                raise ServerStartupError(error_msg)\n\n            if self._check_server_health():\n                logging.info(\"Ollama server is ready\")\n                return\n\n            time.sleep(self.config.check_interval)\n\n        raise ServerTimeoutError(\n            f\"Ollama server did not start within {self.config.timeout} seconds\"\n        )\n\n    def __enter__(self) -&gt; \"OllamaManager\":\n        \"\"\"Context manager entry point that starts the server.\"\"\"\n        self.start()\n        return self\n\n    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None:\n        \"\"\"Context manager exit point that ensures the server is stopped.\"\"\"\n        self.stop()\n</code></pre>"},{"location":"api/client/ollama_manager/ollama_manager/#clientai.ollama.OllamaManager.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry point that starts the server.</p> Source code in <code>clientai/ollama/manager/core.py</code> <pre><code>def __enter__(self) -&gt; \"OllamaManager\":\n    \"\"\"Context manager entry point that starts the server.\"\"\"\n    self.start()\n    return self\n</code></pre>"},{"location":"api/client/ollama_manager/ollama_manager/#clientai.ollama.OllamaManager.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit point that ensures the server is stopped.</p> Source code in <code>clientai/ollama/manager/core.py</code> <pre><code>def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None:\n    \"\"\"Context manager exit point that ensures the server is stopped.\"\"\"\n    self.stop()\n</code></pre>"},{"location":"api/client/ollama_manager/ollama_manager/#clientai.ollama.OllamaManager.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the Ollama manager.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[OllamaServerConfig]</code> <p>Optional server configuration. If None, uses defaults.</p> <code>None</code> Source code in <code>clientai/ollama/manager/core.py</code> <pre><code>def __init__(self, config: Optional[OllamaServerConfig] = None) -&gt; None:\n    \"\"\"\n    Initialize the Ollama manager.\n\n    Args:\n        config: Optional server configuration. If None, uses defaults.\n    \"\"\"\n    self.config = config or OllamaServerConfig()\n    self._process: Optional[subprocess.Popen[str]] = None\n    self._platform_info = PlatformInfo()\n</code></pre>"},{"location":"api/client/ollama_manager/ollama_manager/#clientai.ollama.OllamaManager.start","title":"<code>start()</code>","text":"<p>Start the Ollama server using the configured parameters.</p> <p>This method initializes and starts the Ollama server process, waiting for it to become healthy before returning.</p> <p>Raises:</p> Type Description <code>ServerStartupError</code> <p>If the server fails to start</p> <code>ServerTimeoutError</code> <p>If the server doesn't respond within timeout</p> <code>ExecutableNotFoundError</code> <p>If the Ollama executable is not found</p> <code>ResourceError</code> <p>If there are insufficient system resources</p> Example <p>Start with default configuration: <pre><code>manager = OllamaManager()\nmanager.start()\n</code></pre></p> <p>Start with custom configuration: <pre><code>config = OllamaServerConfig(gpu_layers=35)\nmanager = OllamaManager(config)\nmanager.start()\n</code></pre></p> Source code in <code>clientai/ollama/manager/core.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"\n    Start the Ollama server using the configured parameters.\n\n    This method initializes and starts the Ollama server process,\n    waiting for it to become healthy before returning.\n\n    Raises:\n        ServerStartupError: If the server fails to start\n        ServerTimeoutError: If the server doesn't respond within timeout\n        ExecutableNotFoundError: If the Ollama executable is not found\n        ResourceError: If there are insufficient system resources\n\n    Example:\n        Start with default configuration:\n        ```python\n        manager = OllamaManager()\n        manager.start()\n        ```\n\n        Start with custom configuration:\n        ```python\n        config = OllamaServerConfig(gpu_layers=35)\n        manager = OllamaManager(config)\n        manager.start()\n        ```\n    \"\"\"\n    if self._process is not None:\n        raise ServerStartupError(\"Ollama server is already running\")\n\n    logging.info(\n        f\"Starting Ollama server on {self.config.host}:{self.config.port}\"\n    )\n\n    try:\n        popen_kwargs: Dict[str, Any] = {\n            \"stdout\": subprocess.PIPE,\n            \"stderr\": subprocess.PIPE,\n            \"text\": True,\n            \"env\": self._platform_info.get_environment(self.config),\n        }\n\n        if self._platform_info.platform == Platform.WINDOWS:\n            if sys.platform == \"win32\":\n                popen_kwargs[\"creationflags\"] = subprocess.CREATE_NO_WINDOW\n\n        self._process = subprocess.Popen(\n            self._platform_info.get_server_command(self.config),\n            **popen_kwargs,\n        )\n\n    except FileNotFoundError as e:\n        raise_ollama_error(\n            ExecutableNotFoundError,\n            \"Ollama executable not found. Ensure Ollama is installed.\",\n            e,\n        )\n    except MemoryError as e:\n        raise_ollama_error(\n            ResourceError, \"Insufficient memory to start Ollama server\", e\n        )\n    except Exception as e:\n        raise_ollama_error(\n            ServerStartupError,\n            f\"Failed to start Ollama server: {str(e)}\",\n            e,\n        )\n\n    try:\n        self._wait_for_server()\n    except Exception as e:\n        self.stop()\n        raise e\n</code></pre>"},{"location":"api/client/ollama_manager/ollama_manager/#clientai.ollama.OllamaManager.stop","title":"<code>stop()</code>","text":"<p>Stop the running Ollama server instance.</p> <p>This method terminates the Ollama server process if it's running. It will wait for the process to complete before returning.</p> Example <p>Stop a running server: <pre><code>manager = OllamaManager()\nmanager.start()\n# ... use the server ...\nmanager.stop()\n</code></pre></p> <p>Using context manager (automatic stop): <pre><code>with OllamaManager() as manager:\n    # ... use the server ...\n    pass  # Server stops automatically\n</code></pre></p> Source code in <code>clientai/ollama/manager/core.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"\n    Stop the running Ollama server instance.\n\n    This method terminates the Ollama server process if it's running.\n    It will wait for the process to complete before returning.\n\n    Example:\n        Stop a running server:\n        ```python\n        manager = OllamaManager()\n        manager.start()\n        # ... use the server ...\n        manager.stop()\n        ```\n\n        Using context manager (automatic stop):\n        ```python\n        with OllamaManager() as manager:\n            # ... use the server ...\n            pass  # Server stops automatically\n        ```\n    \"\"\"\n    if self._process is not None:\n        try:\n            self._process.terminate()\n            self._process.wait()\n        finally:\n            self._process = None\n            logging.info(\"Ollama server stopped\")\n</code></pre>"},{"location":"api/client/ollama_manager/ollama_server_config/","title":"OllamaServerConfig Class API Reference","text":"<p>The <code>OllamaServerConfig</code> class is a configuration container that defines the runtime parameters for an Ollama server instance. It allows users to specify network settings (host/port), hardware utilization options (GPU layers, CPU threads, memory limits), and environment variables. The class provides sensible defaults while allowing fine-grained control over server behavior through optional configuration parameters.</p>"},{"location":"api/client/ollama_manager/ollama_server_config/#class-definition","title":"Class Definition","text":"<p>Configuration settings for Ollama server.</p> <p>Attributes:</p> Name Type Description <code>host</code> <code>str</code> <p>Hostname to bind the server to</p> <code>port</code> <code>int</code> <p>Port number to listen on</p> <code>timeout</code> <code>int</code> <p>Maximum time in seconds to wait for server startup</p> <code>check_interval</code> <code>float</code> <p>Time in seconds between server readiness checks</p> <code>gpu_layers</code> <code>Optional[int]</code> <p>Number of layers to run on GPU</p> <code>compute_unit</code> <code>Optional[str]</code> <p>Compute device to use ('cpu', 'gpu', 'auto')</p> <code>cpu_threads</code> <code>Optional[int]</code> <p>Number of CPU threads to use</p> <code>memory_limit</code> <code>Optional[str]</code> <p>Memory limit for the server           (format: number + GiB/MiB, e.g., \"8GiB\")</p> <code>gpu_memory_fraction</code> <code>Optional[float]</code> <p>Fraction of GPU memory to use (0.0-1.0)</p> <code>gpu_devices</code> <code>Optional[Union[List[int], int]]</code> <p>GPU device IDs to use</p> <code>env_vars</code> <code>Dict[str, str]</code> <p>Additional environment variables</p> <code>extra_args</code> <code>List[str]</code> <p>Additional command line arguments</p> Source code in <code>clientai/ollama/manager/config.py</code> <pre><code>@dataclass\nclass OllamaServerConfig:\n    \"\"\"\n    Configuration settings for Ollama server.\n\n    Attributes:\n        host: Hostname to bind the server to\n        port: Port number to listen on\n        timeout: Maximum time in seconds to wait for server startup\n        check_interval: Time in seconds between server readiness checks\n        gpu_layers: Number of layers to run on GPU\n        compute_unit: Compute device to use ('cpu', 'gpu', 'auto')\n        cpu_threads: Number of CPU threads to use\n        memory_limit: Memory limit for the server\n                      (format: number + GiB/MiB, e.g., \"8GiB\")\n        gpu_memory_fraction: Fraction of GPU memory to use (0.0-1.0)\n        gpu_devices: GPU device IDs to use\n        env_vars: Additional environment variables\n        extra_args: Additional command line arguments\n    \"\"\"\n\n    host: str = \"127.0.0.1\"\n    port: int = 11434\n    timeout: int = 30\n    check_interval: float = 1.0\n    gpu_layers: Optional[int] = None\n    compute_unit: Optional[str] = None\n    cpu_threads: Optional[int] = None\n    memory_limit: Optional[str] = None\n    gpu_memory_fraction: Optional[float] = None\n    gpu_devices: Optional[Union[List[int], int]] = None\n    env_vars: Dict[str, str] = field(default_factory=dict)\n    extra_args: List[str] = field(default_factory=list)\n\n    def _validate_host(self) -&gt; None:\n        \"\"\"Validate the host address.\"\"\"\n        if not self.host:\n            raise ValueError(\"Host cannot be empty\")\n\n        try:\n            ipaddress.ip_address(self.host)\n        except ValueError:\n            if not re.match(r\"^[a-zA-Z0-9-]+(\\.[a-zA-Z0-9-]+)*$\", self.host):\n                raise ValueError(f\"Invalid host: {self.host}\")\n\n    def _validate_port(self) -&gt; None:\n        \"\"\"Validate the port number.\"\"\"\n        if not 1 &lt;= self.port &lt;= 65535:\n            raise ValueError(\n                f\"Port must be between 1 and 65535, got {self.port}\"\n            )\n\n    def _validate_timeout_and_interval(self) -&gt; None:\n        \"\"\"Validate timeout and check interval.\"\"\"\n        if self.timeout &lt;= 0:\n            raise ValueError(\"Timeout must be positive\")\n        if self.check_interval &lt;= 0:\n            raise ValueError(\"Check interval must be positive\")\n        if self.check_interval &gt; self.timeout:\n            raise ValueError(\"Check interval cannot be greater than timeout\")\n\n    def _validate_gpu_settings(self) -&gt; None:\n        \"\"\"Validate GPU-related settings.\"\"\"\n        if self.gpu_layers is not None:\n            if not isinstance(self.gpu_layers, int) or self.gpu_layers &lt; 0:\n                raise ValueError(\"gpu_layers must be a non-negative integer\")\n\n        if self.gpu_memory_fraction is not None:\n            if not 0.0 &lt;= self.gpu_memory_fraction &lt;= 1.0:\n                raise ValueError(\n                    \"gpu_memory_fraction must be between 0.0 and 1.0\"\n                )\n\n        if self.gpu_devices is not None:\n            if isinstance(self.gpu_devices, int):\n                if self.gpu_devices &lt; 0:\n                    raise ValueError(\"GPU device ID must be non-negative\")\n            elif isinstance(self.gpu_devices, list):\n                if not all(\n                    isinstance(d, int) and d &gt;= 0 for d in self.gpu_devices\n                ):\n                    raise ValueError(\n                        \"All GPU device IDs must be non-negative integers\"\n                    )\n                if len(self.gpu_devices) != len(set(self.gpu_devices)):\n                    raise ValueError(\n                        \"Duplicate GPU device IDs are not allowed\"\n                    )\n            else:\n                raise ValueError(\n                    \"gpu_devices must be an integer or list of integers\"\n                )\n\n    def _validate_compute_unit(self) -&gt; None:\n        \"\"\"Validate compute unit setting.\"\"\"\n        if self.compute_unit and self.compute_unit not in [\n            \"cpu\",\n            \"gpu\",\n            \"auto\",\n        ]:\n            raise ValueError(\n                \"compute_unit must be one of: 'cpu', 'gpu', 'auto'\"\n            )\n\n    def _validate_cpu_threads(self) -&gt; None:\n        \"\"\"Validate CPU threads setting.\"\"\"\n        if self.cpu_threads is not None:\n            if not isinstance(self.cpu_threads, int) or self.cpu_threads &lt;= 0:\n                raise ValueError(\"cpu_threads must be a positive integer\")\n\n    def _validate_memory_limit(self) -&gt; None:\n        \"\"\"Validate memory limit format.\"\"\"\n        if self.memory_limit is not None:\n            pattern = r\"^\\d+(\\.\\d+)?[MGT]iB$\"\n            if not re.match(pattern, self.memory_limit):\n                raise ValueError(\n                    \"memory_limit must be in format: NUMBER + UNIT, \"\n                    \"where UNIT is MiB, GiB, or TiB (e.g., '8GiB')\"\n                )\n\n            match = re.match(r\"^\\d+(\\.\\d+)?\", self.memory_limit)\n            if match is None:\n                raise ValueError(\"Invalid memory_limit format\")\n\n            value = float(match.group())\n            unit = self.memory_limit[-3:]\n\n            if unit == \"MiB\" and value &lt; 100:\n                raise ValueError(\"memory_limit in MiB must be at least 100\")\n            elif unit == \"GiB\" and value &lt; 0.1:\n                raise ValueError(\"memory_limit in GiB must be at least 0.1\")\n            elif unit == \"TiB\" and value &lt; 0.001:\n                raise ValueError(\"memory_limit in TiB must be at least 0.001\")\n\n    def _validate_env_vars(self) -&gt; None:\n        \"\"\"Validate environment variables.\"\"\"\n        if not all(\n            isinstance(k, str) and isinstance(v, str)\n            for k, v in self.env_vars.items()\n        ):\n            raise ValueError(\"All environment variables must be strings\")\n\n    def _validate_extra_args(self) -&gt; None:\n        \"\"\"Validate extra arguments.\"\"\"\n        if not all(isinstance(arg, str) for arg in self.extra_args):\n            raise ValueError(\"All extra arguments must be strings\")\n\n    def __post_init__(self):\n        \"\"\"Validate all configuration after initialization.\"\"\"\n        self._validate_host()\n        self._validate_port()\n        self._validate_timeout_and_interval()\n        self._validate_gpu_settings()\n        self._validate_compute_unit()\n        self._validate_cpu_threads()\n        self._validate_memory_limit()\n        self._validate_env_vars()\n        self._validate_extra_args()\n\n    @property\n    def base_url(self) -&gt; str:\n        \"\"\"Get the base URL for the Ollama server.\"\"\"\n        return f\"http://{self.host}:{self.port}\"\n</code></pre> <p>rendering:      show_if_no_docstring: true</p>"},{"location":"api/client/ollama_manager/ollama_server_config/#clientai.ollama.OllamaServerConfig.base_url","title":"<code>base_url: str</code>  <code>property</code>","text":"<p>Get the base URL for the Ollama server.</p>"},{"location":"api/client/ollama_manager/ollama_server_config/#clientai.ollama.OllamaServerConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate all configuration after initialization.</p> Source code in <code>clientai/ollama/manager/config.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate all configuration after initialization.\"\"\"\n    self._validate_host()\n    self._validate_port()\n    self._validate_timeout_and_interval()\n    self._validate_gpu_settings()\n    self._validate_compute_unit()\n    self._validate_cpu_threads()\n    self._validate_memory_limit()\n    self._validate_env_vars()\n    self._validate_extra_args()\n</code></pre>"},{"location":"api/client/specific_providers/groq_provider/","title":"Groq Provider API Reference","text":"<p>The <code>GroqProvider</code> class implements the <code>AIProvider</code> interface for the Groq service. It provides methods for text generation and chat functionality using Groq's models.</p>"},{"location":"api/client/specific_providers/groq_provider/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>AIProvider</code></p> <p>Groq-specific implementation of the AIProvider abstract base class.</p> <p>This class provides methods to interact with Groq's models for text generation and chat functionality.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>GroqClientProtocol</code> <p>The Groq client used for making API calls.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for authenticating with Groq.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the Groq package is not installed.</p> Example <p>Initialize the Groq provider: <pre><code>provider = Provider(api_key=\"your-groq-api-key\")\n</code></pre></p> Source code in <code>clientai/groq/provider.py</code> <pre><code>class Provider(AIProvider):\n    \"\"\"\n    Groq-specific implementation of the AIProvider abstract base class.\n\n    This class provides methods to interact with Groq's models for\n    text generation and chat functionality.\n\n    Attributes:\n        client: The Groq client used for making API calls.\n\n    Args:\n        api_key: The API key for authenticating with Groq.\n\n    Raises:\n        ImportError: If the Groq package is not installed.\n\n    Example:\n        Initialize the Groq provider:\n        ```python\n        provider = Provider(api_key=\"your-groq-api-key\")\n        ```\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        if not GROQ_INSTALLED or Client is None:\n            raise ImportError(\n                \"The groq package is not installed. \"\n                \"Please install it with 'pip install clientai[groq]'.\"\n            )\n        self.client: GroqClientProtocol = cast(\n            GroqClientProtocol, Client(api_key=api_key)\n        )\n\n    def _validate_temperature(self, temperature: Optional[float]) -&gt; None:\n        \"\"\"Validate the temperature parameter.\"\"\"\n        if temperature is not None:\n            if not isinstance(temperature, (int, float)):  # noqa: UP038\n                raise InvalidRequestError(\n                    \"Temperature must be a number between 0 and 2\"\n                )\n            if temperature &lt; 0 or temperature &gt; 2:\n                raise InvalidRequestError(\n                    f\"Temperature must be between 0 and 2, got {temperature}\"\n                )\n\n    def _validate_top_p(self, top_p: Optional[float]) -&gt; None:\n        \"\"\"Validate the top_p parameter.\"\"\"\n        if top_p is not None:\n            if not isinstance(top_p, (int, float)):  # noqa: UP038\n                raise InvalidRequestError(\n                    \"Top-p must be a number between 0 and 1\"\n                )\n            if top_p &lt; 0 or top_p &gt; 1:\n                raise InvalidRequestError(\n                    f\"Top-p must be between 0 and 1, got {top_p}\"\n                )\n\n    def _stream_response(\n        self,\n        stream: Iterator[GroqStreamResponse],\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, GroqStreamResponse]]:\n        \"\"\"\n        Process the streaming response from Groq API.\n\n        Args:\n            stream: The stream of responses from Groq API.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, GroqStreamResponse]: Processed content or\n                                            full response objects.\n        \"\"\"\n        for chunk in stream:\n            if return_full_response:\n                yield chunk\n            else:\n                content = chunk.choices[0].delta.content\n                if content:\n                    yield content\n\n    def _map_exception_to_clientai_error(self, e: Exception) -&gt; ClientAIError:\n        \"\"\"\n        Maps a Groq exception to the appropriate ClientAI exception.\n\n        Args:\n            e (Exception): The exception caught during the API call.\n\n        Returns:\n            ClientAIError: An instance of the appropriate ClientAI exception.\n        \"\"\"\n        error_message = str(e)\n\n        if isinstance(e, (GroqAuthenticationError, PermissionDeniedError)):  # noqa: UP038\n            return AuthenticationError(\n                error_message,\n                status_code=getattr(e, \"status_code\", 401),\n                original_error=e,\n            )\n        elif isinstance(e, GroqRateLimitError):\n            return RateLimitError(\n                error_message, status_code=429, original_error=e\n            )\n        elif isinstance(e, NotFoundError):\n            return ModelError(error_message, status_code=404, original_error=e)\n        elif isinstance(  # noqa: UP038\n            e, (BadRequestError, UnprocessableEntityError, ConflictError)\n        ):\n            return InvalidRequestError(\n                error_message,\n                status_code=getattr(e, \"status_code\", 400),\n                original_error=e,\n            )\n        elif isinstance(e, APITimeoutError):\n            return TimeoutError(\n                error_message, status_code=408, original_error=e\n            )\n        elif isinstance(e, InternalServerError):\n            return APIError(\n                error_message,\n                status_code=getattr(e, \"status_code\", 500),\n                original_error=e,\n            )\n        elif isinstance(e, APIStatusError):\n            status = getattr(e, \"status_code\", 500)\n            if status &gt;= 500:\n                return APIError(\n                    error_message, status_code=status, original_error=e\n                )\n            return InvalidRequestError(\n                error_message, status_code=status, original_error=e\n            )\n\n        return ClientAIError(error_message, status_code=500, original_error=e)\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        system_prompt: Optional[str] = None,\n        return_full_response: bool = False,\n        stream: bool = False,\n        json_output: bool = False,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; GroqGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt using a specified Groq model.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the Groq model to use.\n            system_prompt: Optional system prompt to guide model behavior.\n                           If provided, will be added as a system message\n                           before the prompt.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            json_output: If True, format the response as valid JSON using\n                Groq's native JSON mode (beta). Note that this is incompatible\n                with streaming and stop sequences. Will return a 400 error with\n                code \"json_validate_failed\" if JSON generation fails. Defaults\n                to False.\n            temperature: Optional temperature value (0.0-2.0).\n                         Controls randomness in generation.\n                         Lower values make the output more focused\n                         and deterministic, higher values make it\n                         more creative.\n            top_p: Optional nucleus sampling parameter (0.0-1.0).\n                   Controls diversity by limiting cumulative probability\n                   in token selection.\n            **kwargs: Additional keyword arguments to pass to the Groq API.\n\n        Returns:\n            GroqGenericResponse: The generated text, full response object,\n            or an iterator for streaming responses.\n\n        Raises:\n            InvalidRequestError: If json_output and stream are both True.\n            ClientAIError: If an error occurs during the API call.\n\n        Example:\n            Generate text (text only):\n            ```python\n            response = provider.generate_text(\n                \"Explain quantum computing\",\n                model=\"llama3-8b-8192\",\n            )\n            print(response)\n            ```\n\n            Generate text (full response):\n            ```python\n            response = provider.generate_text(\n                \"Explain quantum computing\",\n                model=\"llama3-8b-8192\",\n                return_full_response=True\n            )\n            print(response.choices[0].message.content)\n            ```\n\n            Generate JSON output:\n            ```python\n            response = provider.generate_text(\n                '''Create a user profile with:\n                {\n                    \"name\": \"A random name\",\n                    \"age\": \"A random age between 20-80\",\n                    \"occupation\": \"A random occupation\"\n                }''',\n                model=\"llama3-8b-8192\",\n                json_output=True\n            )\n            print(response)  # Will be valid JSON\n            ```\n        \"\"\"\n        try:\n            self._validate_temperature(temperature)\n            self._validate_top_p(top_p)\n\n            messages: List[Optional[Message]] = []\n            if system_prompt:\n                messages.append({\"role\": \"system\", \"content\": system_prompt})\n            messages.append({\"role\": \"user\", \"content\": prompt})\n\n            completion_kwargs: dict[str, Any] = {\n                \"model\": model,\n                \"messages\": messages,\n                \"stream\": stream,\n            }\n            if json_output:\n                completion_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n            if temperature is not None:\n                completion_kwargs[\"temperature\"] = temperature\n            if top_p is not None:\n                completion_kwargs[\"top_p\"] = top_p\n            completion_kwargs.update(kwargs)\n\n            response = self.client.chat.completions.create(**completion_kwargs)\n\n            if stream:\n                return cast(\n                    GroqGenericResponse,\n                    self._stream_response(\n                        cast(Iterator[GroqStreamResponse], response),\n                        return_full_response,\n                    ),\n                )\n            else:\n                response = cast(GroqResponse, response)\n                if return_full_response:\n                    return response\n                else:\n                    return cast(str, response.choices[0].message.content)\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        system_prompt: Optional[str] = None,\n        return_full_response: bool = False,\n        stream: bool = False,\n        json_output: bool = False,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; GroqGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using a specified Groq model.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the Groq model to use.\n            system_prompt: Optional system prompt to guide model behavior.\n                           If provided, will be inserted at the start of\n                           the conversation.\n            return_full_response: If True, return the full response object.\n                If False, return only the chat content.\n            stream: If True, return an iterator for streaming responses.\n            json_output: If True, format the response as valid JSON using\n                Groq's native JSON mode (beta). Note that this is incompatible\n                with streaming and stop sequences. Will return a 400 error with\n                code \"json_validate_failed\" if JSON generation fails.\n                Defaults to False.\n            temperature: Optional temperature value (0.0-2.0).\n                         Controls randomness in generation.\n                         Lower values make the output more focused\n                         and deterministic, higher values make it\n                         more creative.\n            top_p: Optional nucleus sampling parameter (0.0-1.0).\n                   Controls diversity by limiting cumulative probability\n                   in token selection.\n            **kwargs: Additional keyword arguments to pass to the Groq API.\n\n        Returns:\n            GroqGenericResponse: The chat response, full response object,\n            or an iterator for streaming responses.\n\n        Raises:\n            InvalidRequestError: If json_output and stream are both True.\n            ClientAIError: If an error occurs during the API call.\n\n        Example:\n            Chat (message content only):\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is quantum computing?\"},\n                {\"role\": \"assistant\", \"content\": \"Quantum computing uses...\"},\n                {\"role\": \"user\", \"content\": \"What are its applications?\"}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"llama3-8b-8192\",\n            )\n            print(response)\n            ```\n\n            Chat (full response):\n            ```python\n            response = provider.chat(\n                messages,\n                model=\"llama3-8b-8192\",\n                return_full_response=True\n            )\n            print(response.choices[0].message.content)\n            ```\n\n            Chat with JSON output:\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": '''Generate a user profile with:\n                {\n                    \"name\": \"A random name\",\n                    \"age\": \"A random age between 20-80\",\n                    \"occupation\": \"A random occupation\"\n                }'''}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"llama3-8b-8192\",\n                json_output=True\n            )\n            print(response)  # Will be valid JSON\n            ```\n        \"\"\"\n        try:\n            self._validate_temperature(temperature)\n            self._validate_top_p(top_p)\n\n            chat_messages = messages.copy()\n            if system_prompt:\n                chat_messages.insert(\n                    0, {\"role\": \"system\", \"content\": system_prompt}\n                )\n\n            completion_kwargs: dict[str, Any] = {\n                \"model\": model,\n                \"messages\": chat_messages,\n                \"stream\": stream,\n            }\n            if json_output:\n                completion_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n            if temperature is not None:\n                completion_kwargs[\"temperature\"] = temperature\n            if top_p is not None:\n                completion_kwargs[\"top_p\"] = top_p\n            completion_kwargs.update(kwargs)\n\n            response = self.client.chat.completions.create(**completion_kwargs)\n\n            if stream:\n                return cast(\n                    GroqGenericResponse,\n                    self._stream_response(\n                        cast(Iterator[GroqStreamResponse], response),\n                        return_full_response,\n                    ),\n                )\n            else:\n                response = cast(GroqResponse, response)\n                if return_full_response:\n                    return response\n                else:\n                    return cast(str, response.choices[0].message.content)\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/client/specific_providers/groq_provider/#clientai.groq.Provider.chat","title":"<code>chat(messages, model, system_prompt=None, return_full_response=False, stream=False, json_output=False, temperature=None, top_p=None, **kwargs)</code>","text":"<p>Engage in a chat conversation using a specified Groq model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Groq model to use.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to guide model behavior.            If provided, will be inserted at the start of            the conversation.</p> <code>None</code> <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the chat content.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses.</p> <code>False</code> <code>json_output</code> <code>bool</code> <p>If True, format the response as valid JSON using Groq's native JSON mode (beta). Note that this is incompatible with streaming and stop sequences. Will return a 400 error with code \"json_validate_failed\" if JSON generation fails. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Optional[float]</code> <p>Optional temperature value (0.0-2.0).          Controls randomness in generation.          Lower values make the output more focused          and deterministic, higher values make it          more creative.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Optional nucleus sampling parameter (0.0-1.0).    Controls diversity by limiting cumulative probability    in token selection.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the Groq API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GroqGenericResponse</code> <code>GroqGenericResponse</code> <p>The chat response, full response object,</p> <code>GroqGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Raises:</p> Type Description <code>InvalidRequestError</code> <p>If json_output and stream are both True.</p> <code>ClientAIError</code> <p>If an error occurs during the API call.</p> Example <p>Chat (message content only): <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is quantum computing?\"},\n    {\"role\": \"assistant\", \"content\": \"Quantum computing uses...\"},\n    {\"role\": \"user\", \"content\": \"What are its applications?\"}\n]\nresponse = provider.chat(\n    messages,\n    model=\"llama3-8b-8192\",\n)\nprint(response)\n</code></pre></p> <p>Chat (full response): <pre><code>response = provider.chat(\n    messages,\n    model=\"llama3-8b-8192\",\n    return_full_response=True\n)\nprint(response.choices[0].message.content)\n</code></pre></p> <p>Chat with JSON output: <pre><code>messages = [\n    {\"role\": \"user\", \"content\": '''Generate a user profile with:\n    {\n        \"name\": \"A random name\",\n        \"age\": \"A random age between 20-80\",\n        \"occupation\": \"A random occupation\"\n    }'''}\n]\nresponse = provider.chat(\n    messages,\n    model=\"llama3-8b-8192\",\n    json_output=True\n)\nprint(response)  # Will be valid JSON\n</code></pre></p> Source code in <code>clientai/groq/provider.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    system_prompt: Optional[str] = None,\n    return_full_response: bool = False,\n    stream: bool = False,\n    json_output: bool = False,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; GroqGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using a specified Groq model.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the Groq model to use.\n        system_prompt: Optional system prompt to guide model behavior.\n                       If provided, will be inserted at the start of\n                       the conversation.\n        return_full_response: If True, return the full response object.\n            If False, return only the chat content.\n        stream: If True, return an iterator for streaming responses.\n        json_output: If True, format the response as valid JSON using\n            Groq's native JSON mode (beta). Note that this is incompatible\n            with streaming and stop sequences. Will return a 400 error with\n            code \"json_validate_failed\" if JSON generation fails.\n            Defaults to False.\n        temperature: Optional temperature value (0.0-2.0).\n                     Controls randomness in generation.\n                     Lower values make the output more focused\n                     and deterministic, higher values make it\n                     more creative.\n        top_p: Optional nucleus sampling parameter (0.0-1.0).\n               Controls diversity by limiting cumulative probability\n               in token selection.\n        **kwargs: Additional keyword arguments to pass to the Groq API.\n\n    Returns:\n        GroqGenericResponse: The chat response, full response object,\n        or an iterator for streaming responses.\n\n    Raises:\n        InvalidRequestError: If json_output and stream are both True.\n        ClientAIError: If an error occurs during the API call.\n\n    Example:\n        Chat (message content only):\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is quantum computing?\"},\n            {\"role\": \"assistant\", \"content\": \"Quantum computing uses...\"},\n            {\"role\": \"user\", \"content\": \"What are its applications?\"}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"llama3-8b-8192\",\n        )\n        print(response)\n        ```\n\n        Chat (full response):\n        ```python\n        response = provider.chat(\n            messages,\n            model=\"llama3-8b-8192\",\n            return_full_response=True\n        )\n        print(response.choices[0].message.content)\n        ```\n\n        Chat with JSON output:\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": '''Generate a user profile with:\n            {\n                \"name\": \"A random name\",\n                \"age\": \"A random age between 20-80\",\n                \"occupation\": \"A random occupation\"\n            }'''}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"llama3-8b-8192\",\n            json_output=True\n        )\n        print(response)  # Will be valid JSON\n        ```\n    \"\"\"\n    try:\n        self._validate_temperature(temperature)\n        self._validate_top_p(top_p)\n\n        chat_messages = messages.copy()\n        if system_prompt:\n            chat_messages.insert(\n                0, {\"role\": \"system\", \"content\": system_prompt}\n            )\n\n        completion_kwargs: dict[str, Any] = {\n            \"model\": model,\n            \"messages\": chat_messages,\n            \"stream\": stream,\n        }\n        if json_output:\n            completion_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n        if temperature is not None:\n            completion_kwargs[\"temperature\"] = temperature\n        if top_p is not None:\n            completion_kwargs[\"top_p\"] = top_p\n        completion_kwargs.update(kwargs)\n\n        response = self.client.chat.completions.create(**completion_kwargs)\n\n        if stream:\n            return cast(\n                GroqGenericResponse,\n                self._stream_response(\n                    cast(Iterator[GroqStreamResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(GroqResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return cast(str, response.choices[0].message.content)\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/client/specific_providers/groq_provider/#clientai.groq.Provider.generate_text","title":"<code>generate_text(prompt, model, system_prompt=None, return_full_response=False, stream=False, json_output=False, temperature=None, top_p=None, **kwargs)</code>","text":"<p>Generate text based on a given prompt using a specified Groq model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Groq model to use.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to guide model behavior.            If provided, will be added as a system message            before the prompt.</p> <code>None</code> <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>json_output</code> <code>bool</code> <p>If True, format the response as valid JSON using Groq's native JSON mode (beta). Note that this is incompatible with streaming and stop sequences. Will return a 400 error with code \"json_validate_failed\" if JSON generation fails. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Optional[float]</code> <p>Optional temperature value (0.0-2.0).          Controls randomness in generation.          Lower values make the output more focused          and deterministic, higher values make it          more creative.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Optional nucleus sampling parameter (0.0-1.0).    Controls diversity by limiting cumulative probability    in token selection.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the Groq API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GroqGenericResponse</code> <code>GroqGenericResponse</code> <p>The generated text, full response object,</p> <code>GroqGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Raises:</p> Type Description <code>InvalidRequestError</code> <p>If json_output and stream are both True.</p> <code>ClientAIError</code> <p>If an error occurs during the API call.</p> Example <p>Generate text (text only): <pre><code>response = provider.generate_text(\n    \"Explain quantum computing\",\n    model=\"llama3-8b-8192\",\n)\nprint(response)\n</code></pre></p> <p>Generate text (full response): <pre><code>response = provider.generate_text(\n    \"Explain quantum computing\",\n    model=\"llama3-8b-8192\",\n    return_full_response=True\n)\nprint(response.choices[0].message.content)\n</code></pre></p> <p>Generate JSON output: <pre><code>response = provider.generate_text(\n    '''Create a user profile with:\n    {\n        \"name\": \"A random name\",\n        \"age\": \"A random age between 20-80\",\n        \"occupation\": \"A random occupation\"\n    }''',\n    model=\"llama3-8b-8192\",\n    json_output=True\n)\nprint(response)  # Will be valid JSON\n</code></pre></p> Source code in <code>clientai/groq/provider.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    system_prompt: Optional[str] = None,\n    return_full_response: bool = False,\n    stream: bool = False,\n    json_output: bool = False,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; GroqGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt using a specified Groq model.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the Groq model to use.\n        system_prompt: Optional system prompt to guide model behavior.\n                       If provided, will be added as a system message\n                       before the prompt.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        json_output: If True, format the response as valid JSON using\n            Groq's native JSON mode (beta). Note that this is incompatible\n            with streaming and stop sequences. Will return a 400 error with\n            code \"json_validate_failed\" if JSON generation fails. Defaults\n            to False.\n        temperature: Optional temperature value (0.0-2.0).\n                     Controls randomness in generation.\n                     Lower values make the output more focused\n                     and deterministic, higher values make it\n                     more creative.\n        top_p: Optional nucleus sampling parameter (0.0-1.0).\n               Controls diversity by limiting cumulative probability\n               in token selection.\n        **kwargs: Additional keyword arguments to pass to the Groq API.\n\n    Returns:\n        GroqGenericResponse: The generated text, full response object,\n        or an iterator for streaming responses.\n\n    Raises:\n        InvalidRequestError: If json_output and stream are both True.\n        ClientAIError: If an error occurs during the API call.\n\n    Example:\n        Generate text (text only):\n        ```python\n        response = provider.generate_text(\n            \"Explain quantum computing\",\n            model=\"llama3-8b-8192\",\n        )\n        print(response)\n        ```\n\n        Generate text (full response):\n        ```python\n        response = provider.generate_text(\n            \"Explain quantum computing\",\n            model=\"llama3-8b-8192\",\n            return_full_response=True\n        )\n        print(response.choices[0].message.content)\n        ```\n\n        Generate JSON output:\n        ```python\n        response = provider.generate_text(\n            '''Create a user profile with:\n            {\n                \"name\": \"A random name\",\n                \"age\": \"A random age between 20-80\",\n                \"occupation\": \"A random occupation\"\n            }''',\n            model=\"llama3-8b-8192\",\n            json_output=True\n        )\n        print(response)  # Will be valid JSON\n        ```\n    \"\"\"\n    try:\n        self._validate_temperature(temperature)\n        self._validate_top_p(top_p)\n\n        messages: List[Optional[Message]] = []\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n        messages.append({\"role\": \"user\", \"content\": prompt})\n\n        completion_kwargs: dict[str, Any] = {\n            \"model\": model,\n            \"messages\": messages,\n            \"stream\": stream,\n        }\n        if json_output:\n            completion_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n        if temperature is not None:\n            completion_kwargs[\"temperature\"] = temperature\n        if top_p is not None:\n            completion_kwargs[\"top_p\"] = top_p\n        completion_kwargs.update(kwargs)\n\n        response = self.client.chat.completions.create(**completion_kwargs)\n\n        if stream:\n            return cast(\n                GroqGenericResponse,\n                self._stream_response(\n                    cast(Iterator[GroqStreamResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(GroqResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return cast(str, response.choices[0].message.content)\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/client/specific_providers/ollama_provider/","title":"Ollama Provider API Reference","text":"<p>The <code>OllamaProvider</code> class implements the <code>AIProvider</code> interface for the Ollama service. It provides methods for text generation and chat functionality using locally hosted models through Ollama.</p>"},{"location":"api/client/specific_providers/ollama_provider/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>AIProvider</code></p> <p>Ollama-specific implementation of the AIProvider abstract base class.</p> <p>This class provides methods to interact with Ollama's models for text generation and chat functionality.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>OllamaClientProtocol</code> <p>The Ollama client used for making API calls.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>Optional[str]</code> <p>The host address for the Ollama server. If not provided, the default Ollama client will be used.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the Ollama package is not installed.</p> Example <p>Initialize the Ollama provider: <pre><code>provider = Provider(host=\"http://localhost:11434\")\n</code></pre></p> Source code in <code>clientai/ollama/provider.py</code> <pre><code>class Provider(AIProvider):\n    \"\"\"\n    Ollama-specific implementation of the AIProvider abstract base class.\n\n    This class provides methods to interact with Ollama's models for\n    text generation and chat functionality.\n\n    Attributes:\n        client: The Ollama client used for making API calls.\n\n    Args:\n        host: The host address for the Ollama server.\n            If not provided, the default Ollama client will be used.\n\n    Raises:\n        ImportError: If the Ollama package is not installed.\n\n    Example:\n        Initialize the Ollama provider:\n        ```python\n        provider = Provider(host=\"http://localhost:11434\")\n        ```\n    \"\"\"\n\n    def __init__(self, host: Optional[str] = None):\n        if not OLLAMA_INSTALLED or Client is None:\n            raise ImportError(\n                \"The ollama package is not installed. \"\n                \"Please install it with 'pip install clientai[ollama]'.\"\n            )\n        self.client: OllamaClientProtocol = cast(\n            OllamaClientProtocol, Client(host=host) if host else ollama\n        )\n\n    def _prepare_options(\n        self,\n        json_output: bool = False,\n        system_prompt: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Prepare the options dictionary for Ollama API calls.\n\n        Args:\n            json_output: If True, set format to \"json\"\n            system_prompt: Optional system prompt\n            temperature: Optional temperature value\n            top_p: Optional top-p value\n            **kwargs: Additional options to include\n\n        Returns:\n            Dict[str, Any]: The prepared options dictionary\n        \"\"\"\n        options: Dict[str, Any] = {}\n\n        if json_output:\n            options[\"format\"] = \"json\"\n        if system_prompt:\n            options[\"system\"] = system_prompt\n        if temperature is not None:\n            options[\"temperature\"] = temperature\n        if top_p is not None:\n            options[\"top_p\"] = top_p\n\n        options.update(kwargs)\n\n        return options\n\n    def _validate_temperature(self, temperature: Optional[float]) -&gt; None:\n        \"\"\"[previous implementation remains the same]\"\"\"\n        if temperature is not None:\n            if not isinstance(temperature, (int, float)):  # noqa: UP038\n                raise InvalidRequestError(\n                    \"Temperature must be a number between 0 and 2\"\n                )\n            if temperature &lt; 0 or temperature &gt; 2:\n                raise InvalidRequestError(\n                    f\"Temperature must be between 0 and 2, got {temperature}\"\n                )\n\n    def _validate_top_p(self, top_p: Optional[float]) -&gt; None:\n        \"\"\"[previous implementation remains the same]\"\"\"\n        if top_p is not None:\n            if not isinstance(top_p, (int, float)):  # noqa: UP038\n                raise InvalidRequestError(\n                    \"Top-p must be a number between 0 and 1\"\n                )\n            if top_p &lt; 0 or top_p &gt; 1:\n                raise InvalidRequestError(\n                    f\"Top-p must be between 0 and 1, got {top_p}\"\n                )\n\n    def _stream_generate_response(\n        self,\n        stream: Iterator[OllamaStreamResponse],\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, OllamaStreamResponse]]:\n        \"\"\"\n        Process the streaming response from Ollama API for text generation.\n\n        Args:\n            stream: The stream of responses from Ollama API.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, OllamaStreamResponse]: Processed content or\n                                              full response objects.\n        \"\"\"\n        for chunk in stream:\n            if return_full_response:\n                yield chunk\n            else:\n                yield chunk[\"response\"]\n\n    def _stream_chat_response(\n        self,\n        stream: Iterator[OllamaChatResponse],\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, OllamaChatResponse]]:\n        \"\"\"\n        Process the streaming response from Ollama API for chat.\n\n        Args:\n            stream: The stream of responses from Ollama API.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, OllamaChatResponse]: Processed content or\n                                            full response objects.\n        \"\"\"\n        for chunk in stream:\n            if return_full_response:\n                yield chunk\n            else:\n                yield chunk[\"message\"][\"content\"]\n\n    def _map_exception_to_clientai_error(self, e: Exception) -&gt; ClientAIError:\n        \"\"\"\n        Maps an Ollama exception to the appropriate ClientAI exception.\n\n        Args:\n            e (Exception): The exception caught during the API call.\n\n        Returns:\n            ClientAIError: An instance of the appropriate ClientAI exception.\n        \"\"\"\n        message = str(e)\n\n        if isinstance(e, ollama.RequestError):\n            if \"authentication\" in message.lower():\n                return AuthenticationError(\n                    message, status_code=401, original_error=e\n                )\n            elif \"rate limit\" in message.lower():\n                return RateLimitError(\n                    message, status_code=429, original_error=e\n                )\n            elif \"not found\" in message.lower():\n                return ModelError(message, status_code=404, original_error=e)\n            else:\n                return InvalidRequestError(\n                    message, status_code=400, original_error=e\n                )\n        elif isinstance(e, ollama.ResponseError):\n            if \"timeout\" in message.lower() or \"timed out\" in message.lower():\n                return TimeoutError(message, status_code=408, original_error=e)\n            else:\n                return APIError(message, status_code=500, original_error=e)\n        else:\n            return ClientAIError(message, status_code=500, original_error=e)\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        system_prompt: Optional[str] = None,\n        return_full_response: bool = False,\n        stream: bool = False,\n        json_output: bool = False,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; OllamaGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt using a specified Ollama model.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the Ollama model to use.\n            system_prompt: Optional system prompt to guide model behavior.\n                           Uses Ollama's native system parameter.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text.\n            stream: If True, return an iterator for streaming responses.\n            json_output: If True, set format=\"json\" to get JSON-formatted\n                responses using Ollama's native JSON support. The prompt\n                should specify the desired JSON structure.\n            temperature: Optional temperature value for generation (0.0-2.0).\n                Controls randomness in the output.\n            top_p: Optional top-p value for nucleus sampling (0.0-1.0).\n                Controls diversity of the output.\n            **kwargs: Additional keyword arguments to pass to the Ollama API.\n\n        Returns:\n            OllamaGenericResponse: The generated text, full response object,\n            or an iterator for streaming responses.\n\n        Example:\n            Generate text (text only):\n            ```python\n            response = provider.generate_text(\n                \"Explain machine learning\",\n                model=\"llama2\",\n            )\n            print(response)\n            ```\n\n            Generate creative text with high temperature:\n            ```python\n            response = provider.generate_text(\n                \"Write a story about a space adventure\",\n                model=\"llama2\",\n                temperature=0.8,\n                top_p=0.9\n            )\n            print(response)\n            ```\n\n            Generate JSON output:\n            ```python\n            response = provider.generate_text(\n                '''Create a user profile with:\n                {\n                    \"name\": \"A random name\",\n                    \"age\": \"A random age between 20-80\",\n                    \"occupation\": \"A random occupation\"\n                }''',\n                model=\"llama2\",\n                json_output=True\n            )\n            print(response)  # Will be JSON formatted\n            ```\n        \"\"\"\n        try:\n            self._validate_temperature(temperature)\n            self._validate_top_p(top_p)\n\n            options = self._prepare_options(\n                json_output=json_output,\n                system_prompt=system_prompt,\n                temperature=temperature,\n                top_p=top_p,\n                **kwargs,\n            )\n\n            response = self.client.generate(\n                model=model,\n                prompt=prompt,\n                stream=stream,\n                options=options,\n            )\n\n            if stream:\n                return cast(\n                    OllamaGenericResponse,\n                    self._stream_generate_response(\n                        cast(Iterator[OllamaStreamResponse], response),\n                        return_full_response,\n                    ),\n                )\n            else:\n                response = cast(OllamaResponse, response)\n                if return_full_response:\n                    return response\n                else:\n                    return response[\"response\"]\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        system_prompt: Optional[str] = None,\n        return_full_response: bool = False,\n        stream: bool = False,\n        json_output: bool = False,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; OllamaGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using a specified Ollama model.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the Ollama model to use.\n            system_prompt: Optional system prompt to guide model behavior.\n                           If provided, will be inserted at the start of the\n                           conversation.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text.\n            stream: If True, return an iterator for streaming responses.\n            json_output: If True, set format=\"json\" to get JSON-formatted\n                responses using Ollama's native JSON support. The messages\n                should specify the desired JSON structure.\n            temperature: Optional temperature value for generation (0.0-2.0).\n                Controls randomness in the output.\n            top_p: Optional top-p value for nucleus sampling (0.0-1.0).\n                Controls diversity of the output.\n            **kwargs: Additional keyword arguments to pass to the Ollama API.\n\n        Returns:\n            OllamaGenericResponse: The chat response, full response object,\n            or an iterator for streaming responses.\n\n        Example:\n            Chat with default settings:\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n                {\"role\": \"assistant\", \"content\": \"Machine learning is...\"},\n                {\"role\": \"user\", \"content\": \"Give me some examples\"}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"llama2\",\n            )\n            print(response)\n            ```\n\n            Creative chat with high temperature:\n            ```python\n            response = provider.chat(\n                messages,\n                model=\"llama2\",\n                temperature=0.8,\n                top_p=0.9\n            )\n            print(response)\n            ```\n\n            Chat with JSON output:\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": '''Create a user profile with:\n                {\n                    \"name\": \"A random name\",\n                    \"age\": \"A random age between 20-80\",\n                    \"occupation\": \"A random occupation\"\n                }'''}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"llama2\",\n                json_output=True\n            )\n            print(response)  # Will be JSON formatted\n            ```\n        \"\"\"\n        try:\n            self._validate_temperature(temperature)\n            self._validate_top_p(top_p)\n\n            chat_messages = messages.copy()\n            if system_prompt:\n                chat_messages.insert(\n                    0, {\"role\": \"system\", \"content\": system_prompt}\n                )\n\n            options = self._prepare_options(\n                json_output=json_output,\n                temperature=temperature,\n                top_p=top_p,\n                **kwargs,\n            )\n\n            response = self.client.chat(\n                model=model,\n                messages=chat_messages,\n                stream=stream,\n                options=options,\n            )\n\n            if stream:\n                return cast(\n                    OllamaGenericResponse,\n                    self._stream_chat_response(\n                        cast(Iterator[OllamaChatResponse], response),\n                        return_full_response,\n                    ),\n                )\n            else:\n                response = cast(OllamaChatResponse, response)\n                if return_full_response:\n                    return response\n                else:\n                    return response[\"message\"][\"content\"]\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/client/specific_providers/ollama_provider/#clientai.ollama.Provider.chat","title":"<code>chat(messages, model, system_prompt=None, return_full_response=False, stream=False, json_output=False, temperature=None, top_p=None, **kwargs)</code>","text":"<p>Engage in a chat conversation using a specified Ollama model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Ollama model to use.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to guide model behavior.            If provided, will be inserted at the start of the            conversation.</p> <code>None</code> <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses.</p> <code>False</code> <code>json_output</code> <code>bool</code> <p>If True, set format=\"json\" to get JSON-formatted responses using Ollama's native JSON support. The messages should specify the desired JSON structure.</p> <code>False</code> <code>temperature</code> <code>Optional[float]</code> <p>Optional temperature value for generation (0.0-2.0). Controls randomness in the output.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Optional top-p value for nucleus sampling (0.0-1.0). Controls diversity of the output.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the Ollama API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OllamaGenericResponse</code> <code>OllamaGenericResponse</code> <p>The chat response, full response object,</p> <code>OllamaGenericResponse</code> <p>or an iterator for streaming responses.</p> Example <p>Chat with default settings: <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n    {\"role\": \"assistant\", \"content\": \"Machine learning is...\"},\n    {\"role\": \"user\", \"content\": \"Give me some examples\"}\n]\nresponse = provider.chat(\n    messages,\n    model=\"llama2\",\n)\nprint(response)\n</code></pre></p> <p>Creative chat with high temperature: <pre><code>response = provider.chat(\n    messages,\n    model=\"llama2\",\n    temperature=0.8,\n    top_p=0.9\n)\nprint(response)\n</code></pre></p> <p>Chat with JSON output: <pre><code>messages = [\n    {\"role\": \"user\", \"content\": '''Create a user profile with:\n    {\n        \"name\": \"A random name\",\n        \"age\": \"A random age between 20-80\",\n        \"occupation\": \"A random occupation\"\n    }'''}\n]\nresponse = provider.chat(\n    messages,\n    model=\"llama2\",\n    json_output=True\n)\nprint(response)  # Will be JSON formatted\n</code></pre></p> Source code in <code>clientai/ollama/provider.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    system_prompt: Optional[str] = None,\n    return_full_response: bool = False,\n    stream: bool = False,\n    json_output: bool = False,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; OllamaGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using a specified Ollama model.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the Ollama model to use.\n        system_prompt: Optional system prompt to guide model behavior.\n                       If provided, will be inserted at the start of the\n                       conversation.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text.\n        stream: If True, return an iterator for streaming responses.\n        json_output: If True, set format=\"json\" to get JSON-formatted\n            responses using Ollama's native JSON support. The messages\n            should specify the desired JSON structure.\n        temperature: Optional temperature value for generation (0.0-2.0).\n            Controls randomness in the output.\n        top_p: Optional top-p value for nucleus sampling (0.0-1.0).\n            Controls diversity of the output.\n        **kwargs: Additional keyword arguments to pass to the Ollama API.\n\n    Returns:\n        OllamaGenericResponse: The chat response, full response object,\n        or an iterator for streaming responses.\n\n    Example:\n        Chat with default settings:\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n            {\"role\": \"assistant\", \"content\": \"Machine learning is...\"},\n            {\"role\": \"user\", \"content\": \"Give me some examples\"}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"llama2\",\n        )\n        print(response)\n        ```\n\n        Creative chat with high temperature:\n        ```python\n        response = provider.chat(\n            messages,\n            model=\"llama2\",\n            temperature=0.8,\n            top_p=0.9\n        )\n        print(response)\n        ```\n\n        Chat with JSON output:\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": '''Create a user profile with:\n            {\n                \"name\": \"A random name\",\n                \"age\": \"A random age between 20-80\",\n                \"occupation\": \"A random occupation\"\n            }'''}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"llama2\",\n            json_output=True\n        )\n        print(response)  # Will be JSON formatted\n        ```\n    \"\"\"\n    try:\n        self._validate_temperature(temperature)\n        self._validate_top_p(top_p)\n\n        chat_messages = messages.copy()\n        if system_prompt:\n            chat_messages.insert(\n                0, {\"role\": \"system\", \"content\": system_prompt}\n            )\n\n        options = self._prepare_options(\n            json_output=json_output,\n            temperature=temperature,\n            top_p=top_p,\n            **kwargs,\n        )\n\n        response = self.client.chat(\n            model=model,\n            messages=chat_messages,\n            stream=stream,\n            options=options,\n        )\n\n        if stream:\n            return cast(\n                OllamaGenericResponse,\n                self._stream_chat_response(\n                    cast(Iterator[OllamaChatResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(OllamaChatResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return response[\"message\"][\"content\"]\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/client/specific_providers/ollama_provider/#clientai.ollama.Provider.generate_text","title":"<code>generate_text(prompt, model, system_prompt=None, return_full_response=False, stream=False, json_output=False, temperature=None, top_p=None, **kwargs)</code>","text":"<p>Generate text based on a given prompt using a specified Ollama model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Ollama model to use.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to guide model behavior.            Uses Ollama's native system parameter.</p> <code>None</code> <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses.</p> <code>False</code> <code>json_output</code> <code>bool</code> <p>If True, set format=\"json\" to get JSON-formatted responses using Ollama's native JSON support. The prompt should specify the desired JSON structure.</p> <code>False</code> <code>temperature</code> <code>Optional[float]</code> <p>Optional temperature value for generation (0.0-2.0). Controls randomness in the output.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Optional top-p value for nucleus sampling (0.0-1.0). Controls diversity of the output.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the Ollama API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OllamaGenericResponse</code> <code>OllamaGenericResponse</code> <p>The generated text, full response object,</p> <code>OllamaGenericResponse</code> <p>or an iterator for streaming responses.</p> Example <p>Generate text (text only): <pre><code>response = provider.generate_text(\n    \"Explain machine learning\",\n    model=\"llama2\",\n)\nprint(response)\n</code></pre></p> <p>Generate creative text with high temperature: <pre><code>response = provider.generate_text(\n    \"Write a story about a space adventure\",\n    model=\"llama2\",\n    temperature=0.8,\n    top_p=0.9\n)\nprint(response)\n</code></pre></p> <p>Generate JSON output: <pre><code>response = provider.generate_text(\n    '''Create a user profile with:\n    {\n        \"name\": \"A random name\",\n        \"age\": \"A random age between 20-80\",\n        \"occupation\": \"A random occupation\"\n    }''',\n    model=\"llama2\",\n    json_output=True\n)\nprint(response)  # Will be JSON formatted\n</code></pre></p> Source code in <code>clientai/ollama/provider.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    system_prompt: Optional[str] = None,\n    return_full_response: bool = False,\n    stream: bool = False,\n    json_output: bool = False,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; OllamaGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt using a specified Ollama model.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the Ollama model to use.\n        system_prompt: Optional system prompt to guide model behavior.\n                       Uses Ollama's native system parameter.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text.\n        stream: If True, return an iterator for streaming responses.\n        json_output: If True, set format=\"json\" to get JSON-formatted\n            responses using Ollama's native JSON support. The prompt\n            should specify the desired JSON structure.\n        temperature: Optional temperature value for generation (0.0-2.0).\n            Controls randomness in the output.\n        top_p: Optional top-p value for nucleus sampling (0.0-1.0).\n            Controls diversity of the output.\n        **kwargs: Additional keyword arguments to pass to the Ollama API.\n\n    Returns:\n        OllamaGenericResponse: The generated text, full response object,\n        or an iterator for streaming responses.\n\n    Example:\n        Generate text (text only):\n        ```python\n        response = provider.generate_text(\n            \"Explain machine learning\",\n            model=\"llama2\",\n        )\n        print(response)\n        ```\n\n        Generate creative text with high temperature:\n        ```python\n        response = provider.generate_text(\n            \"Write a story about a space adventure\",\n            model=\"llama2\",\n            temperature=0.8,\n            top_p=0.9\n        )\n        print(response)\n        ```\n\n        Generate JSON output:\n        ```python\n        response = provider.generate_text(\n            '''Create a user profile with:\n            {\n                \"name\": \"A random name\",\n                \"age\": \"A random age between 20-80\",\n                \"occupation\": \"A random occupation\"\n            }''',\n            model=\"llama2\",\n            json_output=True\n        )\n        print(response)  # Will be JSON formatted\n        ```\n    \"\"\"\n    try:\n        self._validate_temperature(temperature)\n        self._validate_top_p(top_p)\n\n        options = self._prepare_options(\n            json_output=json_output,\n            system_prompt=system_prompt,\n            temperature=temperature,\n            top_p=top_p,\n            **kwargs,\n        )\n\n        response = self.client.generate(\n            model=model,\n            prompt=prompt,\n            stream=stream,\n            options=options,\n        )\n\n        if stream:\n            return cast(\n                OllamaGenericResponse,\n                self._stream_generate_response(\n                    cast(Iterator[OllamaStreamResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(OllamaResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return response[\"response\"]\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/client/specific_providers/openai_provider/","title":"OpenAI Provider API Reference","text":"<p>The <code>OpenAIProvider</code> class implements the <code>AIProvider</code> interface for the OpenAI service. It provides methods for text generation and chat functionality using OpenAI's models.</p>"},{"location":"api/client/specific_providers/openai_provider/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>AIProvider</code></p> <p>OpenAI-specific implementation of the AIProvider abstract base class.</p> <p>This class provides methods to interact with OpenAI's models for text generation and chat functionality.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>OpenAIClientProtocol</code> <p>The OpenAI client used for making API calls.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for authenticating with OpenAI.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the OpenAI package is not installed.</p> Example <p>Initialize the OpenAI provider: <pre><code>provider = Provider(api_key=\"your-openai-api-key\")\n</code></pre></p> Source code in <code>clientai/openai/provider.py</code> <pre><code>class Provider(AIProvider):\n    \"\"\"\n    OpenAI-specific implementation of the AIProvider abstract base class.\n\n    This class provides methods to interact with OpenAI's\n    models for text generation and chat functionality.\n\n    Attributes:\n        client: The OpenAI client used for making API calls.\n\n    Args:\n        api_key: The API key for authenticating with OpenAI.\n\n    Raises:\n        ImportError: If the OpenAI package is not installed.\n\n    Example:\n        Initialize the OpenAI provider:\n        ```python\n        provider = Provider(api_key=\"your-openai-api-key\")\n        ```\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        if not OPENAI_INSTALLED or Client is None:\n            raise ImportError(\n                \"The openai package is not installed. \"\n                \"Please install it with 'pip install clientai[openai]'.\"\n            )\n        self.client: OpenAIClientProtocol = cast(\n            OpenAIClientProtocol, Client(api_key=api_key)\n        )\n\n    def _validate_temperature(self, temperature: Optional[float]) -&gt; None:\n        \"\"\"Validate the temperature parameter.\"\"\"\n        if temperature is not None:\n            if not isinstance(temperature, (int, float)):  # noqa: UP038\n                raise InvalidRequestError(\n                    \"Temperature must be a number between 0 and 2\"\n                )\n            if temperature &lt; 0 or temperature &gt; 2:\n                raise InvalidRequestError(\n                    f\"Temperature must be between 0 and 2, got {temperature}\"\n                )\n\n    def _validate_top_p(self, top_p: Optional[float]) -&gt; None:\n        \"\"\"Validate the top_p parameter.\"\"\"\n        if top_p is not None:\n            if not isinstance(top_p, (int, float)):  # noqa: UP038\n                raise InvalidRequestError(\n                    \"Top-p must be a number between 0 and 1\"\n                )\n            if top_p &lt; 0 or top_p &gt; 1:\n                raise InvalidRequestError(\n                    f\"Top-p must be between 0 and 1, got {top_p}\"\n                )\n\n    def _stream_response(\n        self,\n        stream: Iterator[OpenAIStreamResponse],\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, OpenAIStreamResponse]]:\n        \"\"\"\n        Process the streaming response from OpenAI API.\n\n        Args:\n            stream: The stream of responses from OpenAI API.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, OpenAIStreamResponse]: Processed content or full\n                                              response objects.\n        \"\"\"\n        for chunk in stream:\n            if return_full_response:\n                yield chunk\n            else:\n                content = chunk.choices[0].delta.content\n                if content:\n                    yield content\n\n    def _map_exception_to_clientai_error(self, e: Exception) -&gt; ClientAIError:\n        \"\"\"\n        Maps an OpenAI exception to the appropriate ClientAI exception.\n\n        Args:\n            e (Exception): The exception caught during the API call.\n\n        Raises:\n            ClientAIError: An instance of the appropriate ClientAI exception.\n        \"\"\"\n        error_message = str(e)\n        status_code = None\n\n        if hasattr(e, \"status_code\"):\n            status_code = e.status_code\n        else:\n            try:\n                status_code = int(\n                    error_message.split(\"Error code: \")[1].split(\" -\")[0]\n                )\n            except (IndexError, ValueError):\n                pass\n\n        if (\n            isinstance(e, OpenAIAuthenticationError)\n            or \"incorrect api key\" in error_message.lower()\n        ):\n            return AuthenticationError(\n                error_message, status_code, original_error=e\n            )\n        elif (\n            isinstance(e, openai.OpenAIError)\n            or \"error code:\" in error_message.lower()\n        ):\n            if status_code == 429 or \"rate limit\" in error_message.lower():\n                return RateLimitError(\n                    error_message, status_code, original_error=e\n                )\n            elif status_code == 404 or \"not found\" in error_message.lower():\n                return ModelError(error_message, status_code, original_error=e)\n            elif status_code == 400 or \"invalid\" in error_message.lower():\n                return InvalidRequestError(\n                    error_message, status_code, original_error=e\n                )\n            elif status_code == 408 or \"timeout\" in error_message.lower():\n                return TimeoutError(\n                    error_message, status_code, original_error=e\n                )\n            elif status_code and status_code &gt;= 500:\n                return APIError(error_message, status_code, original_error=e)\n\n        return ClientAIError(error_message, status_code, original_error=e)\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        system_prompt: Optional[str] = None,\n        return_full_response: bool = False,\n        stream: bool = False,\n        json_output: bool = False,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; OpenAIGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt using a specified OpenAI model.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the OpenAI model to use.\n            system_prompt: Optional system prompt to guide model behavior.\n                           If provided, will be added as a system message\n                           before the prompt.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            json_output: If True, format the response as valid JSON using\n                OpenAI's native JSON mode. The prompt should specify the\n                desired JSON structure. Defaults to False.\n            temperature: Optional temperature value (0.0-2.0).\n                         Controls randomness in generation.\n                         Lower values make the output more focused\n                         and deterministic, higher values make it\n                         more creative.\n            top_p: Optional nucleus sampling parameter (0.0-1.0).\n                   Controls diversity by limiting cumulative probability\n                   in token selection.\n            **kwargs: Additional keyword arguments to pass to the OpenAI API.\n\n        Returns:\n            OpenAIGenericResponse: The generated text, full response object,\n            or an iterator for streaming responses.\n\n        Raises:\n            ClientAIError: If an error occurs during the API call.\n\n        Example:\n            Generate text (text only):\n            ```python\n            response = provider.generate_text(\n                \"Explain the theory of relativity\",\n                model=\"gpt-3.5-turbo\",\n            )\n            print(response)\n            ```\n\n            Generate text (full response):\n            ```python\n            response = provider.generate_text(\n                \"Explain the theory of relativity\",\n                model=\"gpt-3.5-turbo\",\n                return_full_response=True\n            )\n            print(response.choices[0].message.content)\n            ```\n\n            Generate text (streaming):\n            ```python\n            for chunk in provider.generate_text(\n                \"Explain the theory of relativity\",\n                model=\"gpt-3.5-turbo\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n\n            Generate JSON output:\n            ```python\n            response = provider.generate_text(\n                '''Generate a user profile with the following structure:\n                {\n                    \"name\": \"A random name\",\n                    \"age\": \"A random age between 20-80\",\n                    \"occupation\": \"A random occupation\"\n                }''',\n                model=\"gpt-3.5-turbo\",\n                json_output=True\n            )\n            print(response)  # Will be valid JSON\n            ```\n        \"\"\"\n        try:\n            self._validate_temperature(temperature)\n            self._validate_top_p(top_p)\n\n            messages = []\n            if system_prompt:\n                messages.append({\"role\": \"system\", \"content\": system_prompt})\n            messages.append({\"role\": \"user\", \"content\": prompt})\n\n            completion_kwargs = {\n                \"model\": model,\n                \"messages\": messages,\n                \"stream\": stream,\n            }\n            if json_output:\n                completion_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n            if temperature is not None:\n                completion_kwargs[\"temperature\"] = temperature\n            if top_p is not None:\n                completion_kwargs[\"top_p\"] = top_p\n            completion_kwargs.update(kwargs)\n\n            response = self.client.chat.completions.create(**completion_kwargs)\n\n            if stream:\n                return cast(\n                    OpenAIGenericResponse,\n                    self._stream_response(\n                        cast(Iterator[OpenAIStreamResponse], response),\n                        return_full_response,\n                    ),\n                )\n            else:\n                response = cast(OpenAIResponse, response)\n                if return_full_response:\n                    return response\n                else:\n                    return response.choices[0].message.content\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        system_prompt: Optional[str] = None,\n        return_full_response: bool = False,\n        stream: bool = False,\n        json_output: bool = False,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; OpenAIGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using a specified OpenAI model.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the OpenAI model to use.\n            system_prompt: Optional system prompt to guide model behavior.\n                           If provided, will be inserted at the start of the\n                           conversation.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            json_output: If True, format the response as valid JSON using\n                OpenAI's native JSON mode. The messages should specify the\n                desired JSON structure. Defaults to False.\n            temperature: Optional temperature value (0.0-2.0).\n                         Controls randomness in generation.\n                         Lower values make the output more focused\n                         and deterministic, higher values make it\n                         more creative.\n            top_p: Optional nucleus sampling parameter (0.0-1.0).\n                   Controls diversity by limiting cumulative probability\n                   in token selection.\n            **kwargs: Additional keyword arguments to pass to the OpenAI API.\n\n        Returns:\n            OpenAIGenericResponse: The chat response, full response object,\n            or an iterator for streaming responses.\n\n        Raises:\n            ClientAIError: If an error occurs during the API call.\n\n        Example:\n            Chat (message content only):\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n                {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n                {\"role\": \"user\", \"content\": \"What is its population?\"}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n            )\n            print(response)\n            ```\n\n            Chat (full response):\n            ```python\n            response = provider.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n                return_full_response=True\n            )\n            print(response.choices[0].message.content)\n            ```\n\n            Chat (streaming):\n            ```python\n            for chunk in provider.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n\n            Chat with JSON output:\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": '''Generate a user profile with:\n                {\n                    \"name\": \"A random name\",\n                    \"age\": \"A random age between 20-80\",\n                    \"occupation\": \"A random occupation\"\n                }'''}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n                json_output=True\n            )\n            print(response)  # Will be valid JSON\n            ```\n        \"\"\"\n        try:\n            self._validate_temperature(temperature)\n            self._validate_top_p(top_p)\n\n            chat_messages = messages.copy()\n            if system_prompt:\n                chat_messages.insert(\n                    0, {\"role\": \"system\", \"content\": system_prompt}\n                )\n\n            completion_kwargs = {\n                \"model\": model,\n                \"messages\": chat_messages,\n                \"stream\": stream,\n            }\n            if json_output:\n                completion_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n            if temperature is not None:\n                completion_kwargs[\"temperature\"] = temperature\n            if top_p is not None:\n                completion_kwargs[\"top_p\"] = top_p\n            completion_kwargs.update(kwargs)\n\n            response = self.client.chat.completions.create(**completion_kwargs)\n\n            if stream:\n                return cast(\n                    OpenAIGenericResponse,\n                    self._stream_response(\n                        cast(Iterator[OpenAIStreamResponse], response),\n                        return_full_response,\n                    ),\n                )\n            else:\n                response = cast(OpenAIResponse, response)\n                if return_full_response:\n                    return response\n                else:\n                    return response.choices[0].message.content\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/client/specific_providers/openai_provider/#clientai.openai.Provider.chat","title":"<code>chat(messages, model, system_prompt=None, return_full_response=False, stream=False, json_output=False, temperature=None, top_p=None, **kwargs)</code>","text":"<p>Engage in a chat conversation using a specified OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the OpenAI model to use.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to guide model behavior.            If provided, will be inserted at the start of the            conversation.</p> <code>None</code> <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>json_output</code> <code>bool</code> <p>If True, format the response as valid JSON using OpenAI's native JSON mode. The messages should specify the desired JSON structure. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Optional[float]</code> <p>Optional temperature value (0.0-2.0).          Controls randomness in generation.          Lower values make the output more focused          and deterministic, higher values make it          more creative.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Optional nucleus sampling parameter (0.0-1.0).    Controls diversity by limiting cumulative probability    in token selection.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OpenAIGenericResponse</code> <code>OpenAIGenericResponse</code> <p>The chat response, full response object,</p> <code>OpenAIGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Raises:</p> Type Description <code>ClientAIError</code> <p>If an error occurs during the API call.</p> Example <p>Chat (message content only): <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\nresponse = provider.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n)\nprint(response)\n</code></pre></p> <p>Chat (full response): <pre><code>response = provider.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    return_full_response=True\n)\nprint(response.choices[0].message.content)\n</code></pre></p> <p>Chat (streaming): <pre><code>for chunk in provider.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> <p>Chat with JSON output: <pre><code>messages = [\n    {\"role\": \"user\", \"content\": '''Generate a user profile with:\n    {\n        \"name\": \"A random name\",\n        \"age\": \"A random age between 20-80\",\n        \"occupation\": \"A random occupation\"\n    }'''}\n]\nresponse = provider.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    json_output=True\n)\nprint(response)  # Will be valid JSON\n</code></pre></p> Source code in <code>clientai/openai/provider.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    system_prompt: Optional[str] = None,\n    return_full_response: bool = False,\n    stream: bool = False,\n    json_output: bool = False,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; OpenAIGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using a specified OpenAI model.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the OpenAI model to use.\n        system_prompt: Optional system prompt to guide model behavior.\n                       If provided, will be inserted at the start of the\n                       conversation.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        json_output: If True, format the response as valid JSON using\n            OpenAI's native JSON mode. The messages should specify the\n            desired JSON structure. Defaults to False.\n        temperature: Optional temperature value (0.0-2.0).\n                     Controls randomness in generation.\n                     Lower values make the output more focused\n                     and deterministic, higher values make it\n                     more creative.\n        top_p: Optional nucleus sampling parameter (0.0-1.0).\n               Controls diversity by limiting cumulative probability\n               in token selection.\n        **kwargs: Additional keyword arguments to pass to the OpenAI API.\n\n    Returns:\n        OpenAIGenericResponse: The chat response, full response object,\n        or an iterator for streaming responses.\n\n    Raises:\n        ClientAIError: If an error occurs during the API call.\n\n    Example:\n        Chat (message content only):\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n            {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n            {\"role\": \"user\", \"content\": \"What is its population?\"}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n        )\n        print(response)\n        ```\n\n        Chat (full response):\n        ```python\n        response = provider.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n            return_full_response=True\n        )\n        print(response.choices[0].message.content)\n        ```\n\n        Chat (streaming):\n        ```python\n        for chunk in provider.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n\n        Chat with JSON output:\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": '''Generate a user profile with:\n            {\n                \"name\": \"A random name\",\n                \"age\": \"A random age between 20-80\",\n                \"occupation\": \"A random occupation\"\n            }'''}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n            json_output=True\n        )\n        print(response)  # Will be valid JSON\n        ```\n    \"\"\"\n    try:\n        self._validate_temperature(temperature)\n        self._validate_top_p(top_p)\n\n        chat_messages = messages.copy()\n        if system_prompt:\n            chat_messages.insert(\n                0, {\"role\": \"system\", \"content\": system_prompt}\n            )\n\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": chat_messages,\n            \"stream\": stream,\n        }\n        if json_output:\n            completion_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n        if temperature is not None:\n            completion_kwargs[\"temperature\"] = temperature\n        if top_p is not None:\n            completion_kwargs[\"top_p\"] = top_p\n        completion_kwargs.update(kwargs)\n\n        response = self.client.chat.completions.create(**completion_kwargs)\n\n        if stream:\n            return cast(\n                OpenAIGenericResponse,\n                self._stream_response(\n                    cast(Iterator[OpenAIStreamResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(OpenAIResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return response.choices[0].message.content\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/client/specific_providers/openai_provider/#clientai.openai.Provider.generate_text","title":"<code>generate_text(prompt, model, system_prompt=None, return_full_response=False, stream=False, json_output=False, temperature=None, top_p=None, **kwargs)</code>","text":"<p>Generate text based on a given prompt using a specified OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the OpenAI model to use.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to guide model behavior.            If provided, will be added as a system message            before the prompt.</p> <code>None</code> <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>json_output</code> <code>bool</code> <p>If True, format the response as valid JSON using OpenAI's native JSON mode. The prompt should specify the desired JSON structure. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Optional[float]</code> <p>Optional temperature value (0.0-2.0).          Controls randomness in generation.          Lower values make the output more focused          and deterministic, higher values make it          more creative.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Optional nucleus sampling parameter (0.0-1.0).    Controls diversity by limiting cumulative probability    in token selection.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OpenAIGenericResponse</code> <code>OpenAIGenericResponse</code> <p>The generated text, full response object,</p> <code>OpenAIGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Raises:</p> Type Description <code>ClientAIError</code> <p>If an error occurs during the API call.</p> Example <p>Generate text (text only): <pre><code>response = provider.generate_text(\n    \"Explain the theory of relativity\",\n    model=\"gpt-3.5-turbo\",\n)\nprint(response)\n</code></pre></p> <p>Generate text (full response): <pre><code>response = provider.generate_text(\n    \"Explain the theory of relativity\",\n    model=\"gpt-3.5-turbo\",\n    return_full_response=True\n)\nprint(response.choices[0].message.content)\n</code></pre></p> <p>Generate text (streaming): <pre><code>for chunk in provider.generate_text(\n    \"Explain the theory of relativity\",\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> <p>Generate JSON output: <pre><code>response = provider.generate_text(\n    '''Generate a user profile with the following structure:\n    {\n        \"name\": \"A random name\",\n        \"age\": \"A random age between 20-80\",\n        \"occupation\": \"A random occupation\"\n    }''',\n    model=\"gpt-3.5-turbo\",\n    json_output=True\n)\nprint(response)  # Will be valid JSON\n</code></pre></p> Source code in <code>clientai/openai/provider.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    system_prompt: Optional[str] = None,\n    return_full_response: bool = False,\n    stream: bool = False,\n    json_output: bool = False,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; OpenAIGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt using a specified OpenAI model.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the OpenAI model to use.\n        system_prompt: Optional system prompt to guide model behavior.\n                       If provided, will be added as a system message\n                       before the prompt.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        json_output: If True, format the response as valid JSON using\n            OpenAI's native JSON mode. The prompt should specify the\n            desired JSON structure. Defaults to False.\n        temperature: Optional temperature value (0.0-2.0).\n                     Controls randomness in generation.\n                     Lower values make the output more focused\n                     and deterministic, higher values make it\n                     more creative.\n        top_p: Optional nucleus sampling parameter (0.0-1.0).\n               Controls diversity by limiting cumulative probability\n               in token selection.\n        **kwargs: Additional keyword arguments to pass to the OpenAI API.\n\n    Returns:\n        OpenAIGenericResponse: The generated text, full response object,\n        or an iterator for streaming responses.\n\n    Raises:\n        ClientAIError: If an error occurs during the API call.\n\n    Example:\n        Generate text (text only):\n        ```python\n        response = provider.generate_text(\n            \"Explain the theory of relativity\",\n            model=\"gpt-3.5-turbo\",\n        )\n        print(response)\n        ```\n\n        Generate text (full response):\n        ```python\n        response = provider.generate_text(\n            \"Explain the theory of relativity\",\n            model=\"gpt-3.5-turbo\",\n            return_full_response=True\n        )\n        print(response.choices[0].message.content)\n        ```\n\n        Generate text (streaming):\n        ```python\n        for chunk in provider.generate_text(\n            \"Explain the theory of relativity\",\n            model=\"gpt-3.5-turbo\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n\n        Generate JSON output:\n        ```python\n        response = provider.generate_text(\n            '''Generate a user profile with the following structure:\n            {\n                \"name\": \"A random name\",\n                \"age\": \"A random age between 20-80\",\n                \"occupation\": \"A random occupation\"\n            }''',\n            model=\"gpt-3.5-turbo\",\n            json_output=True\n        )\n        print(response)  # Will be valid JSON\n        ```\n    \"\"\"\n    try:\n        self._validate_temperature(temperature)\n        self._validate_top_p(top_p)\n\n        messages = []\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n        messages.append({\"role\": \"user\", \"content\": prompt})\n\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"stream\": stream,\n        }\n        if json_output:\n            completion_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n        if temperature is not None:\n            completion_kwargs[\"temperature\"] = temperature\n        if top_p is not None:\n            completion_kwargs[\"top_p\"] = top_p\n        completion_kwargs.update(kwargs)\n\n        response = self.client.chat.completions.create(**completion_kwargs)\n\n        if stream:\n            return cast(\n                OpenAIGenericResponse,\n                self._stream_response(\n                    cast(Iterator[OpenAIStreamResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(OpenAIResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return response.choices[0].message.content\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/client/specific_providers/replicate_provider/","title":"Replicate Provider API Reference","text":"<p>The <code>ReplicateProvider</code> class implements the <code>AIProvider</code> interface for the Replicate service. It provides methods for text generation and chat functionality using models hosted on Replicate.</p>"},{"location":"api/client/specific_providers/replicate_provider/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>AIProvider</code></p> <p>Replicate-specific implementation of the AIProvider abstract base class.</p> <p>This class provides methods to interact with Replicate's AI models for text generation and chat functionality.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>ReplicateClientProtocol</code> <p>The Replicate client used for making API calls.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for authenticating with Replicate.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the Replicate package is not installed.</p> Example <p>Initialize the Replicate provider: <pre><code>provider = Provider(api_key=\"your-replicate-api-key\")\n</code></pre></p> Source code in <code>clientai/replicate/provider.py</code> <pre><code>class Provider(AIProvider):\n    \"\"\"\n    Replicate-specific implementation of the AIProvider abstract base class.\n\n    This class provides methods to interact with Replicate's AI models for\n    text generation and chat functionality.\n\n    Attributes:\n        client: The Replicate client used for making API calls.\n\n    Args:\n        api_key: The API key for authenticating with Replicate.\n\n    Raises:\n        ImportError: If the Replicate package is not installed.\n\n    Example:\n        Initialize the Replicate provider:\n        ```python\n        provider = Provider(api_key=\"your-replicate-api-key\")\n        ```\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        if not REPLICATE_INSTALLED or Client is None:\n            raise ImportError(\n                \"The replicate package is not installed. \"\n                \"Please install it with 'pip install clientai[replicate]'.\"\n            )\n        self.client: ReplicateClientProtocol = Client(api_token=api_key)\n\n    def _validate_temperature(self, temperature: Optional[float]) -&gt; None:\n        \"\"\"Validate the temperature parameter.\"\"\"\n        if temperature is not None:\n            if not isinstance(temperature, (int, float)):  # noqa: UP038\n                raise InvalidRequestError(\"Temperature must be a number\")\n\n    def _validate_top_p(self, top_p: Optional[float]) -&gt; None:\n        \"\"\"Validate the top_p parameter.\"\"\"\n        if top_p is not None:\n            if not isinstance(top_p, (int, float)):  # noqa: UP038\n                raise InvalidRequestError(\n                    \"Top-p must be a number between 0 and 1\"\n                )\n            if top_p &lt; 0 or top_p &gt; 1:\n                raise InvalidRequestError(\n                    f\"Top-p must be between 0 and 1, got {top_p}\"\n                )\n\n    def _process_output(self, output: Any) -&gt; str:\n        \"\"\"\n        Process the output from Replicate API into a string format.\n\n        Args:\n            output: The raw output from Replicate API.\n\n        Returns:\n            str: The processed output as a string.\n        \"\"\"\n        if isinstance(output, List):\n            return \"\".join(str(item) for item in output)\n        elif isinstance(output, str):\n            return output\n        else:\n            return str(output)\n\n    def _wait_for_prediction(\n        self, prediction_id: str, max_wait_time: int = 300\n    ) -&gt; ReplicatePredictionProtocol:\n        \"\"\"\n        Wait for a prediction to complete or fail.\n\n        Args:\n            prediction_id: The ID of the prediction to wait for.\n            max_wait_time: Maximum time to wait in seconds. Defaults to 300.\n\n        Returns:\n            ReplicatePredictionProtocol: The completed prediction.\n\n        Raises:\n            TimeoutError: If the prediction doesn't complete within\n                          the max_wait_time.\n            APIError: If the prediction fails.\n        \"\"\"\n        start_time = time.time()\n        while time.time() - start_time &lt; max_wait_time:\n            prediction = self.client.predictions.get(prediction_id)\n            if prediction.status == \"succeeded\":\n                return prediction\n            elif prediction.status == \"failed\":\n                raise self._map_exception_to_clientai_error(\n                    Exception(f\"Prediction failed: {prediction.error}\")\n                )\n            time.sleep(1)\n\n        raise self._map_exception_to_clientai_error(\n            Exception(\"Prediction timed out\"), status_code=408\n        )\n\n    def _stream_response(\n        self,\n        prediction: ReplicatePredictionProtocol,\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, ReplicateStreamResponse]]:\n        \"\"\"\n        Stream the response from a prediction.\n\n        Args:\n            prediction: The prediction to stream.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, ReplicateStreamResponse]: Processed output or\n                                                 full response objects.\n        \"\"\"\n        metadata = cast(ReplicateStreamResponse, prediction.__dict__.copy())\n        for event in prediction.stream():\n            if return_full_response:\n                metadata[\"output\"] = self._process_output(event)\n                yield metadata\n            else:\n                yield self._process_output(event)\n\n    def _map_exception_to_clientai_error(\n        self, e: Exception, status_code: Optional[int] = None\n    ) -&gt; ClientAIError:\n        \"\"\"\n        Maps a Replicate exception to the appropriate ClientAI exception.\n\n        Args:\n            e (Exception): The exception caught during the API call.\n            status_code (int, optional): The HTTP status code, if available.\n\n        Returns:\n            ClientAIError: An instance of the appropriate ClientAI exception.\n        \"\"\"\n        error_message = str(e)\n        status_code = status_code or getattr(e, \"status_code\", None)\n\n        if (\n            \"authentication\" in error_message.lower()\n            or \"unauthorized\" in error_message.lower()\n        ):\n            return AuthenticationError(\n                error_message, status_code, original_error=e\n            )\n        elif \"rate limit\" in error_message.lower():\n            return RateLimitError(error_message, status_code, original_error=e)\n        elif \"not found\" in error_message.lower():\n            return ModelError(error_message, status_code, original_error=e)\n        elif \"invalid\" in error_message.lower():\n            return InvalidRequestError(\n                error_message, status_code, original_error=e\n            )\n        elif \"timeout\" in error_message.lower() or status_code == 408:\n            return TimeoutError(error_message, status_code, original_error=e)\n        elif status_code == 400:\n            return InvalidRequestError(\n                error_message, status_code, original_error=e\n            )\n        else:\n            return APIError(error_message, status_code, original_error=e)\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        system_prompt: Optional[str] = None,\n        return_full_response: bool = False,\n        stream: bool = False,\n        json_output: bool = False,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; ReplicateGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt\n        using a specified Replicate model.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the Replicate model to use.\n            system_prompt: Optional system prompt to guide model behavior.\n                      If provided, will be added as a system message before\n                      the prompt.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text.\n            stream: If True, return an iterator for streaming responses.\n            json_output: If True, set output=\"json\" in the input parameters\n                to get JSON-formatted responses. The prompt should specify\n                the desired JSON structure.\n            **kwargs: Additional keyword arguments to pass\n                      to the Replicate API.\n\n        Returns:\n            ReplicateGenericResponse: The generated text, full response object,\n            or an iterator for streaming responses.\n\n        Example:\n            Generate text (text only):\n            ```python\n            response = provider.generate_text(\n                \"Explain quantum computing\",\n                model=\"meta/llama-2-70b-chat:latest\",\n            )\n            print(response)\n            ```\n\n            Generate text (full response):\n            ```python\n            response = provider.generate_text(\n                \"Explain quantum computing\",\n                model=\"meta/llama-2-70b-chat:latest\",\n                return_full_response=True\n            )\n            print(response[\"output\"])\n            ```\n\n            Generate JSON output:\n            ```python\n            response = provider.generate_text(\n                '''Create a user profile with:\n                {\n                    \"name\": \"A random name\",\n                    \"age\": \"A random age between 20-80\",\n                    \"occupation\": \"A random occupation\"\n                }''',\n                model=\"meta/llama-2-70b-chat:latest\",\n                json_output=True\n            )\n            print(response)  # Will be JSON formatted\n            ```\n\n            Generate text (streaming):\n            ```python\n            for chunk in provider.generate_text(\n                \"Explain quantum computing\",\n                model=\"meta/llama-2-70b-chat:latest\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        try:\n            self._validate_temperature(temperature)\n            self._validate_top_p(top_p)\n\n            formatted_prompt = \"\"\n            if system_prompt:\n                formatted_prompt = f\"&lt;system&gt;{system_prompt}&lt;/system&gt;\\n\"\n            formatted_prompt += f\"&lt;user&gt;{prompt}&lt;/user&gt;\\n&lt;assistant&gt;\"\n\n            input_params = {\"prompt\": formatted_prompt}\n            if json_output:\n                input_params[\"output\"] = \"json\"\n            if temperature is not None:\n                input_params[\"temperature\"] = temperature  # type: ignore\n            if top_p is not None:\n                input_params[\"top_p\"] = top_p  # type: ignore\n\n            prediction = self.client.predictions.create(\n                model=model,\n                input=input_params,\n                stream=stream,\n                **kwargs,\n            )\n\n            if stream:\n                return self._stream_response(prediction, return_full_response)\n            else:\n                completed_prediction = self._wait_for_prediction(prediction.id)\n                if return_full_response:\n                    response = cast(\n                        ReplicateResponse, completed_prediction.__dict__.copy()\n                    )\n                    response[\"output\"] = self._process_output(\n                        completed_prediction.output\n                    )\n                    return response\n                else:\n                    return self._process_output(completed_prediction.output)\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        system_prompt: Optional[str] = None,\n        return_full_response: bool = False,\n        stream: bool = False,\n        json_output: bool = False,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; ReplicateGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using a specified Replicate model.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the Replicate model to use.\n            system_prompt: Optional system prompt to guide model behavior.\n                      If provided, will be inserted at the start of the\n                      conversation.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text.\n            stream: If True, return an iterator for streaming responses.\n            json_output: If True, set output=\"json\" in the input parameters\n                to get JSON-formatted responses. The messages should specify\n                the desired JSON structure.\n            **kwargs: Additional keyword arguments to pass\n                      to the Replicate API.\n\n        Returns:\n            ReplicateGenericResponse: The chat response, full response object,\n            or an iterator for streaming responses.\n\n        Examples:\n            Chat (message content only):\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is quantum computing?\"},\n                {\"role\": \"assistant\", \"content\": \"Quantum computing uses...\"},\n                {\"role\": \"user\", \"content\": \"What are its applications?\"}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"meta/llama-2-70b-chat:latest\",\n            )\n            print(response)\n            ```\n\n            Chat with JSON output:\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": '''Create a user profile with:\n                {\n                    \"name\": \"A random name\",\n                    \"age\": \"A random age between 20-80\",\n                    \"occupation\": \"A random occupation\"\n                }'''}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"meta/llama-2-70b-chat:latest\",\n                json_output=True\n            )\n            print(response)  # Will be JSON formatted\n            ```\n\n            Chat (streaming):\n            ```python\n            for chunk in provider.chat(\n                messages,\n                model=\"meta/llama-2-70b-chat:latest\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        try:\n            self._validate_temperature(temperature)\n            self._validate_top_p(top_p)\n\n            chat_messages = messages.copy()\n            if system_prompt:\n                chat_messages.insert(\n                    0, {\"role\": \"system\", \"content\": system_prompt}\n                )\n\n            prompt = \"\\n\".join(\n                [\n                    f\"&lt;{m['role']}&gt;{m['content']}&lt;/{m['role']}&gt;\"\n                    for m in chat_messages\n                ]\n            )\n            prompt += \"\\n&lt;assistant&gt;\"\n\n            input_params = {\"prompt\": prompt}\n            if json_output:\n                input_params[\"output\"] = \"json\"\n            if temperature is not None:\n                input_params[\"temperature\"] = temperature  # type: ignore\n            if top_p is not None:\n                input_params[\"top_p\"] = top_p  # type: ignore\n\n            prediction = self.client.predictions.create(\n                model=model,\n                input=input_params,\n                stream=stream,\n                **kwargs,\n            )\n\n            if stream:\n                return self._stream_response(prediction, return_full_response)\n            else:\n                completed_prediction = self._wait_for_prediction(prediction.id)\n                if return_full_response:\n                    response = cast(\n                        ReplicateResponse, completed_prediction.__dict__.copy()\n                    )\n                    response[\"output\"] = self._process_output(\n                        completed_prediction.output\n                    )\n                    return response\n                else:\n                    return self._process_output(completed_prediction.output)\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/client/specific_providers/replicate_provider/#clientai.replicate.Provider.chat","title":"<code>chat(messages, model, system_prompt=None, return_full_response=False, stream=False, json_output=False, temperature=None, top_p=None, **kwargs)</code>","text":"<p>Engage in a chat conversation using a specified Replicate model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Replicate model to use.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to guide model behavior.       If provided, will be inserted at the start of the       conversation.</p> <code>None</code> <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses.</p> <code>False</code> <code>json_output</code> <code>bool</code> <p>If True, set output=\"json\" in the input parameters to get JSON-formatted responses. The messages should specify the desired JSON structure.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass       to the Replicate API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ReplicateGenericResponse</code> <code>ReplicateGenericResponse</code> <p>The chat response, full response object,</p> <code>ReplicateGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Examples:</p> <p>Chat (message content only): <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is quantum computing?\"},\n    {\"role\": \"assistant\", \"content\": \"Quantum computing uses...\"},\n    {\"role\": \"user\", \"content\": \"What are its applications?\"}\n]\nresponse = provider.chat(\n    messages,\n    model=\"meta/llama-2-70b-chat:latest\",\n)\nprint(response)\n</code></pre></p> <p>Chat with JSON output: <pre><code>messages = [\n    {\"role\": \"user\", \"content\": '''Create a user profile with:\n    {\n        \"name\": \"A random name\",\n        \"age\": \"A random age between 20-80\",\n        \"occupation\": \"A random occupation\"\n    }'''}\n]\nresponse = provider.chat(\n    messages,\n    model=\"meta/llama-2-70b-chat:latest\",\n    json_output=True\n)\nprint(response)  # Will be JSON formatted\n</code></pre></p> <p>Chat (streaming): <pre><code>for chunk in provider.chat(\n    messages,\n    model=\"meta/llama-2-70b-chat:latest\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/replicate/provider.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    system_prompt: Optional[str] = None,\n    return_full_response: bool = False,\n    stream: bool = False,\n    json_output: bool = False,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; ReplicateGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using a specified Replicate model.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the Replicate model to use.\n        system_prompt: Optional system prompt to guide model behavior.\n                  If provided, will be inserted at the start of the\n                  conversation.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text.\n        stream: If True, return an iterator for streaming responses.\n        json_output: If True, set output=\"json\" in the input parameters\n            to get JSON-formatted responses. The messages should specify\n            the desired JSON structure.\n        **kwargs: Additional keyword arguments to pass\n                  to the Replicate API.\n\n    Returns:\n        ReplicateGenericResponse: The chat response, full response object,\n        or an iterator for streaming responses.\n\n    Examples:\n        Chat (message content only):\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is quantum computing?\"},\n            {\"role\": \"assistant\", \"content\": \"Quantum computing uses...\"},\n            {\"role\": \"user\", \"content\": \"What are its applications?\"}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"meta/llama-2-70b-chat:latest\",\n        )\n        print(response)\n        ```\n\n        Chat with JSON output:\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": '''Create a user profile with:\n            {\n                \"name\": \"A random name\",\n                \"age\": \"A random age between 20-80\",\n                \"occupation\": \"A random occupation\"\n            }'''}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"meta/llama-2-70b-chat:latest\",\n            json_output=True\n        )\n        print(response)  # Will be JSON formatted\n        ```\n\n        Chat (streaming):\n        ```python\n        for chunk in provider.chat(\n            messages,\n            model=\"meta/llama-2-70b-chat:latest\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    try:\n        self._validate_temperature(temperature)\n        self._validate_top_p(top_p)\n\n        chat_messages = messages.copy()\n        if system_prompt:\n            chat_messages.insert(\n                0, {\"role\": \"system\", \"content\": system_prompt}\n            )\n\n        prompt = \"\\n\".join(\n            [\n                f\"&lt;{m['role']}&gt;{m['content']}&lt;/{m['role']}&gt;\"\n                for m in chat_messages\n            ]\n        )\n        prompt += \"\\n&lt;assistant&gt;\"\n\n        input_params = {\"prompt\": prompt}\n        if json_output:\n            input_params[\"output\"] = \"json\"\n        if temperature is not None:\n            input_params[\"temperature\"] = temperature  # type: ignore\n        if top_p is not None:\n            input_params[\"top_p\"] = top_p  # type: ignore\n\n        prediction = self.client.predictions.create(\n            model=model,\n            input=input_params,\n            stream=stream,\n            **kwargs,\n        )\n\n        if stream:\n            return self._stream_response(prediction, return_full_response)\n        else:\n            completed_prediction = self._wait_for_prediction(prediction.id)\n            if return_full_response:\n                response = cast(\n                    ReplicateResponse, completed_prediction.__dict__.copy()\n                )\n                response[\"output\"] = self._process_output(\n                    completed_prediction.output\n                )\n                return response\n            else:\n                return self._process_output(completed_prediction.output)\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/client/specific_providers/replicate_provider/#clientai.replicate.Provider.generate_text","title":"<code>generate_text(prompt, model, system_prompt=None, return_full_response=False, stream=False, json_output=False, temperature=None, top_p=None, **kwargs)</code>","text":"<p>Generate text based on a given prompt using a specified Replicate model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Replicate model to use.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to guide model behavior.       If provided, will be added as a system message before       the prompt.</p> <code>None</code> <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses.</p> <code>False</code> <code>json_output</code> <code>bool</code> <p>If True, set output=\"json\" in the input parameters to get JSON-formatted responses. The prompt should specify the desired JSON structure.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass       to the Replicate API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ReplicateGenericResponse</code> <code>ReplicateGenericResponse</code> <p>The generated text, full response object,</p> <code>ReplicateGenericResponse</code> <p>or an iterator for streaming responses.</p> Example <p>Generate text (text only): <pre><code>response = provider.generate_text(\n    \"Explain quantum computing\",\n    model=\"meta/llama-2-70b-chat:latest\",\n)\nprint(response)\n</code></pre></p> <p>Generate text (full response): <pre><code>response = provider.generate_text(\n    \"Explain quantum computing\",\n    model=\"meta/llama-2-70b-chat:latest\",\n    return_full_response=True\n)\nprint(response[\"output\"])\n</code></pre></p> <p>Generate JSON output: <pre><code>response = provider.generate_text(\n    '''Create a user profile with:\n    {\n        \"name\": \"A random name\",\n        \"age\": \"A random age between 20-80\",\n        \"occupation\": \"A random occupation\"\n    }''',\n    model=\"meta/llama-2-70b-chat:latest\",\n    json_output=True\n)\nprint(response)  # Will be JSON formatted\n</code></pre></p> <p>Generate text (streaming): <pre><code>for chunk in provider.generate_text(\n    \"Explain quantum computing\",\n    model=\"meta/llama-2-70b-chat:latest\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/replicate/provider.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    system_prompt: Optional[str] = None,\n    return_full_response: bool = False,\n    stream: bool = False,\n    json_output: bool = False,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; ReplicateGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt\n    using a specified Replicate model.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the Replicate model to use.\n        system_prompt: Optional system prompt to guide model behavior.\n                  If provided, will be added as a system message before\n                  the prompt.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text.\n        stream: If True, return an iterator for streaming responses.\n        json_output: If True, set output=\"json\" in the input parameters\n            to get JSON-formatted responses. The prompt should specify\n            the desired JSON structure.\n        **kwargs: Additional keyword arguments to pass\n                  to the Replicate API.\n\n    Returns:\n        ReplicateGenericResponse: The generated text, full response object,\n        or an iterator for streaming responses.\n\n    Example:\n        Generate text (text only):\n        ```python\n        response = provider.generate_text(\n            \"Explain quantum computing\",\n            model=\"meta/llama-2-70b-chat:latest\",\n        )\n        print(response)\n        ```\n\n        Generate text (full response):\n        ```python\n        response = provider.generate_text(\n            \"Explain quantum computing\",\n            model=\"meta/llama-2-70b-chat:latest\",\n            return_full_response=True\n        )\n        print(response[\"output\"])\n        ```\n\n        Generate JSON output:\n        ```python\n        response = provider.generate_text(\n            '''Create a user profile with:\n            {\n                \"name\": \"A random name\",\n                \"age\": \"A random age between 20-80\",\n                \"occupation\": \"A random occupation\"\n            }''',\n            model=\"meta/llama-2-70b-chat:latest\",\n            json_output=True\n        )\n        print(response)  # Will be JSON formatted\n        ```\n\n        Generate text (streaming):\n        ```python\n        for chunk in provider.generate_text(\n            \"Explain quantum computing\",\n            model=\"meta/llama-2-70b-chat:latest\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    try:\n        self._validate_temperature(temperature)\n        self._validate_top_p(top_p)\n\n        formatted_prompt = \"\"\n        if system_prompt:\n            formatted_prompt = f\"&lt;system&gt;{system_prompt}&lt;/system&gt;\\n\"\n        formatted_prompt += f\"&lt;user&gt;{prompt}&lt;/user&gt;\\n&lt;assistant&gt;\"\n\n        input_params = {\"prompt\": formatted_prompt}\n        if json_output:\n            input_params[\"output\"] = \"json\"\n        if temperature is not None:\n            input_params[\"temperature\"] = temperature  # type: ignore\n        if top_p is not None:\n            input_params[\"top_p\"] = top_p  # type: ignore\n\n        prediction = self.client.predictions.create(\n            model=model,\n            input=input_params,\n            stream=stream,\n            **kwargs,\n        )\n\n        if stream:\n            return self._stream_response(prediction, return_full_response)\n        else:\n            completed_prediction = self._wait_for_prediction(prediction.id)\n            if return_full_response:\n                response = cast(\n                    ReplicateResponse, completed_prediction.__dict__.copy()\n                )\n                response[\"output\"] = self._process_output(\n                    completed_prediction.output\n                )\n                return response\n            else:\n                return self._process_output(completed_prediction.output)\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"community/CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"community/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"community/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"community/CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"community/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"community/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at igor.magalhaes.r@gmail.com. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"community/CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"community/CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"community/CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"community/CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"community/CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"community/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"community/CONTRIBUTING/","title":"Contributing to ClientAI","text":"<p>Thank you for your interest in contributing to ClientAI! This guide is meant to make it easy for you to get started.</p>"},{"location":"community/CONTRIBUTING/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":""},{"location":"community/CONTRIBUTING/#cloning-the-repository","title":"Cloning the Repository","text":"<p>Start by forking and cloning the ClientAI repository:</p> <pre><code>git clone https://github.com/YOUR-GITHUB-USERNAME/clientai.git\n</code></pre>"},{"location":"community/CONTRIBUTING/#using-poetry-for-dependency-management","title":"Using Poetry for Dependency Management","text":"<p>ClientAI uses Poetry for managing dependencies. If you don't have Poetry installed, follow the instructions on the official Poetry website.</p> <p>Once Poetry is installed, navigate to the cloned repository and install the dependencies: <pre><code>cd clientai\npoetry install\n</code></pre></p>"},{"location":"community/CONTRIBUTING/#activating-the-virtual-environment","title":"Activating the Virtual Environment","text":"<p>Poetry creates a virtual environment for your project. Activate it using:</p> <pre><code>poetry shell\n</code></pre>"},{"location":"community/CONTRIBUTING/#making-contributions","title":"Making Contributions","text":""},{"location":"community/CONTRIBUTING/#coding-standards","title":"Coding Standards","text":"<ul> <li>Follow PEP 8 guidelines.</li> <li>Write meaningful tests for new features or bug fixes.</li> </ul>"},{"location":"community/CONTRIBUTING/#testing-with-pytest","title":"Testing with Pytest","text":"<p>ClientAI uses pytest for testing. Run tests using: <pre><code>poetry run pytest\n</code></pre></p>"},{"location":"community/CONTRIBUTING/#linting","title":"Linting","text":"<p>Use mypy for type checking: <pre><code>mypy clientai\n</code></pre></p> <p>Use ruff for style: <pre><code>ruff check --fix\nruff format\n</code></pre></p> <p>Ensure your code passes linting before submitting.</p>"},{"location":"community/CONTRIBUTING/#submitting-your-contributions","title":"Submitting Your Contributions","text":""},{"location":"community/CONTRIBUTING/#creating-a-pull-request","title":"Creating a Pull Request","text":"<p>After making your changes:</p> <ul> <li>Push your changes to your fork.</li> <li>Open a pull request with a clear description of your changes.</li> <li>Update the README.md if necessary.</li> </ul>"},{"location":"community/CONTRIBUTING/#code-reviews","title":"Code Reviews","text":"<ul> <li>Address any feedback from code reviews.</li> <li>Once approved, your contributions will be merged into the main branch.</li> </ul>"},{"location":"community/CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Please adhere to our Code of Conduct to maintain a welcoming and inclusive environment.</p> <p>Thank you for contributing to ClientAI\ud83d\ude80</p>"},{"location":"community/LICENSE/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2024 Igor Benav</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"community/overview/","title":"Community Overview","text":"<p>Welcome to the project's community hub. Here, you'll find essential resources and guidelines that are crucial for contributing to and participating in the project. Please take the time to familiarize yourself with the following documents:</p>"},{"location":"community/overview/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Contributing</li> <li>Code of Conduct</li> <li>License</li> </ul>"},{"location":"community/overview/#contributing","title":"Contributing","text":"<p>View the Contributing Guidelines</p> <p>Interested in contributing to the project? Great! The contributing guidelines will provide you with all the information you need to get started. This includes how to submit issues, propose changes, and the process for submitting pull requests.</p>"},{"location":"community/overview/#code-of-conduct","title":"Code of Conduct","text":"<p>View the Code of Conduct</p> <p>The Code of Conduct outlines the standards and behaviors expected of our community members. It's crucial to ensure a welcoming and inclusive environment for everyone. Please take the time to read and adhere to these guidelines.</p>"},{"location":"community/overview/#license","title":"License","text":"<p>View the License</p> <p>The license document outlines the terms under which our project can be used, modified, and distributed. Understanding the licensing is important for both users and contributors of the project.</p> <p>Thank you for being a part of our community and for contributing to our project's success!</p>"},{"location":"community/showcase_submission/","title":"Submit Your Project","text":"<p>Share Your Work</p> <p>Have you built something with ClientAI? We'd love to feature it in our showcase!</p>"},{"location":"community/showcase_submission/#project-categories","title":"Project Categories","text":"<p>What You Can Submit</p> <ul> <li>Tutorials: Step-by-step guides teaching others how to build with ClientAI</li> <li>Open Source Projects: Libraries, tools, or applications others can use and learn from</li> <li>Applications: Web apps, desktop tools, or services built with ClientAI</li> <li>Commercial Services: Products or services powered by ClientAI</li> </ul>"},{"location":"community/showcase_submission/#how-to-submit","title":"How to Submit","text":"<p>Submission Steps</p> <ol> <li>Create a new issue using our Showcase Submission Template</li> <li>Fill in the relevant information for your project type</li> <li>We'll review your submission and add it to the showcase!</li> </ol>"},{"location":"community/showcase_submission/#requirements-by-category","title":"Requirements by Category","text":"<p>What We Look For</p> <p>For Tutorials:</p> <ul> <li>Clear step-by-step instructions</li> <li>Working code examples</li> <li>Explanation of concepts used</li> </ul> <p>For Open Source Projects:</p> <ul> <li>Public repository</li> <li>Basic documentation</li> <li>Installation/usage instructions</li> </ul> <p>For Applications/Services:</p> <ul> <li>Public demo or screenshots</li> <li>Description of ClientAI features used</li> <li>Link to live service (if applicable)</li> </ul>"},{"location":"community/showcase_submission/#need-help","title":"Need Help?","text":"<p>Questions?</p> <p>Need help with your submission? We're here to help!</p> <ul> <li>Open a discussion on GitHub</li> </ul>"},{"location":"examples/overview/","title":"Examples Overview","text":"<p>Welcome to the Examples section of the ClientAI documentation. We provide both complete example applications and core usage patterns to help you get started.</p>"},{"location":"examples/overview/#example-applications","title":"Example Applications","text":""},{"location":"examples/overview/#client-based-examples","title":"Client-Based Examples","text":"<ol> <li> <p>Simple Q&amp;A Bot: Basic question-answering bot showing provider initialization, prompt handling, and core text generation/chat methods.</p> </li> <li> <p>Multi-Provider Translator: Translation comparator demonstrating simultaneous usage of multiple providers, configurations, and response handling.</p> </li> <li> <p>AI Dungeon Master: Text-based RPG orchestrating multiple providers for game state management and dynamic narrative generation.</p> </li> </ol>"},{"location":"examples/overview/#agent-based-examples","title":"Agent-Based Examples","text":"<ol> <li> <p>Simple Q&amp;A Bot: Q&amp;A Bot implementation with Agent, introducing basic agent features.</p> </li> <li> <p>Task Planner: Basic agent that breaks down goals into steps, introducing create_agent and simple tool creation.</p> </li> <li> <p>Writing Assistant: Multi-step writing improvement agent showcasing workflow steps with think/act/synthesize, decorator configurations, and tool integration.</p> </li> <li> <p>Code Analyzer: Code analysis assistant showcasing custom workflows.</p> </li> </ol>"},{"location":"examples/overview/#core-usage-patterns","title":"Core Usage Patterns","text":""},{"location":"examples/overview/#working-with-providers","title":"Working with Providers","text":"<pre><code>from clientai import ClientAI\n\n# Initialize with your preferred provider\nclient = ClientAI('openai', api_key=\"your-openai-key\")\n# Or: ClientAI('groq', api_key=\"your-groq-key\")\n# Or: ClientAI('replicate', api_key=\"your-replicate-key\")\n# Or: ClientAI('ollama', host=\"your-ollama-host\")\n\n# Basic text generation\nresponse = client.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n)\nprint(response)\n\n# Chat functionality\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"Paris.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\n\nresponse = client.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n)\nprint(response)\n</code></pre>"},{"location":"examples/overview/#working-with-agents","title":"Working with Agents","text":""},{"location":"examples/overview/#quick-start-agent","title":"Quick-Start Agent","text":"<pre><code>from clientai import client\nfrom clientai.agent import create_agent, tool\n\n@tool(name=\"add\", description=\"Add two numbers together\")\ndef add(x: int, y: int) -&gt; int:\n    return x + y\n\n@tool(name=\"multiply\")\ndef multiply(x: int, y: int) -&gt; int:\n    \"\"\"Multiply two numbers and return their product.\"\"\"\n    return x * y\n\n# Create a simple calculator agent\ncalculator = create_agent(\n    client=client(\"groq\", api_key=\"your-groq-key\"),\n    role=\"calculator\", \n    system_prompt=\"You are a helpful calculator assistant.\",\n    model=\"llama-3.2-3b-preview\",\n    tools=[add, multiply]\n)\n\nresult = calculator.run(\"What is 5 plus 3, then multiplied by 2?\")\nprint(result)\n</code></pre>"},{"location":"examples/overview/#custom-workflow-agent","title":"Custom Workflow Agent","text":"<pre><code>from clientai import Agent, think, act, tool\n\n@tool(name=\"calculator\")\ndef calculate_average(numbers: list[float]) -&gt; float:\n    \"\"\"Calculate the arithmetic mean of a list of numbers.\"\"\"\n    return sum(numbers) / len(numbers)\n\nclass DataAnalyzer(Agent):\n    @think(\"analyze\")\n    def analyze_data(self, input_data: str) -&gt; str:\n        \"\"\"Analyze sales data by calculating key metrics.\"\"\"\n        return f\"\"\"\n            Please analyze these sales figures:\n\n            {input_data}\n\n            Calculate the average using the calculator tool\n            and identify the trend.\n            \"\"\"\n\n    @act\n    def summarize(self, analysis: str) -&gt; str:\n        \"\"\"Create a brief summary of the analysis.\"\"\"\n        return \"\"\"\n            Create a brief summary that includes:\n            1. The average sales figure\n            2. Whether sales are trending up or down\n            3. One key recommendation\n            \"\"\"\n\n# Initialize with the tool\nanalyzer = DataAnalyzer(\n    client=client(\"replicate\", api_key=\"your-replicate-key\"),\n    default_model=\"meta/meta-llama-3-70b-instruct\",\n    tool_confidence=0.8,\n    tools=[calculate_average]\n)\n\nresult = analyzer.run(\"Monthly sales: [1000, 1200, 950, 1100]\")\nprint(result)\n</code></pre>"},{"location":"examples/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Handle API Keys Securely: Never hardcode API keys in your source code</li> <li>Use Type Hints: Take advantage of ClientAI's type system for better IDE support</li> <li>Implement Error Handling: Add appropriate try/catch blocks for API calls</li> <li>Monitor Usage: Keep track of API calls and token usage across providers</li> </ol>"},{"location":"examples/overview/#contributing","title":"Contributing","text":"<p>Have you built something interesting with ClientAI? We'd love to feature it! Check our Contributing Guidelines for information on how to submit your examples.</p>"},{"location":"examples/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Usage Guide for detailed documentation</li> <li>Review the API Reference for complete API details</li> <li>Join our community to share your experiences and get help</li> </ul>"},{"location":"examples/agent/code_analyzer/","title":"Building a Code Analysis Assistant with ClientAI","text":"<p>In this tutorial, we're going to build a code analysis assistant that can help developers improve their Python code. Our assistant will analyze code structure, identify potential issues, and suggest improvements. We'll use ClientAI's framework along with local AI models to create something that's both powerful and practical.</p>"},{"location":"examples/agent/code_analyzer/#understanding-the-foundation","title":"Understanding the Foundation","text":"<p>Let's start with what we're actually building. A code analyzer needs to look at several aspects of code: its structure, complexity, style, and documentation. To do this effectively, we'll use Python's abstract syntax tree (AST) module to parse and analyze code programmatically.</p> <p>First, we need a way to represent our analysis results. Here's how we structure this:</p> <pre><code>@dataclass\nclass CodeAnalysisResult:\n    \"\"\"Results from code analysis.\"\"\"\n    complexity: int\n    functions: List[str]\n    classes: List[str]\n    imports: List[str]\n    issues: List[str]\n</code></pre> <p>This dataclass gives us a clean way to organize our findings. The complexity score helps us understand how intricate the code is, while the other fields track the various components we find in the code.</p> <p>Now let's write the core analysis function that examines Python code:</p> <pre><code>def analyze_python_code_original(code: str) -&gt; CodeAnalysisResult:\n    \"\"\"Analyze Python code structure and complexity.\"\"\"\n    try:\n        tree = ast.parse(code)\n        functions = []\n        classes = []\n        imports = []\n        complexity = 0\n\n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef):\n                functions.append(node.name)\n                complexity += sum(1 for _ in ast.walk(node) \n                                if isinstance(_, (ast.If, ast.For, ast.While)))\n            elif isinstance(node, ast.ClassDef):\n                classes.append(node.name)\n            elif isinstance(node, (ast.Import, ast.ImportFrom)):\n                for name in node.names:\n                    imports.append(name.name)\n\n        return CodeAnalysisResult(\n            complexity=complexity,\n            functions=functions,\n            classes=classes,\n            imports=imports,\n            issues=[],\n        )\n    except Exception as e:\n        return CodeAnalysisResult(\n            complexity=0, functions=[], classes=[], imports=[], issues=[str(e)]\n        )\n</code></pre> <p>This function does the heavy lifting of our analysis. When we pass it a string of Python code, it uses the AST module to parse the code into a tree structure that we can examine. We walk through this tree looking for different types of nodes that represent functions, classes, and imports.</p> <p>For each function we find, we also calculate its complexity. We do this by counting control structures like if statements, for loops, and while loops. This gives us a rough measure of how complex the function is - more control structures generally mean more complex code that might need simplification.</p> <p>Next, we need to look at code style. Python has some well-established style conventions, and we want to check if code follows them:</p> <pre><code>def check_style_issues(code: str) -&gt; str:\n    \"\"\"Check Python code style issues.\"\"\"\n    issues = []\n\n    for i, line in enumerate(code.split(\"\\n\"), 1):\n        if len(line.strip()) &gt; 88:\n            issues.append(f\"Line {i} exceeds 88 characters\")\n\n    function_pattern = r\"def\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\(\"\n    for match in re.finditer(function_pattern, code):\n        name = match.group(1)\n        if not name.islower():\n            issues.append(f\"Function '{name}' should use snake_case\")\n\n    return json.dumps({\"issues\": issues})\n</code></pre> <p>This style checker looks at a couple of key aspects of Python style. First, it checks line length - lines that are too long can be hard to read and understand. Second, it looks at function naming conventions. In Python, we typically use snake_case for function names (like <code>calculate_total</code> rather than <code>calculateTotal</code>).</p> <p>Now that we have our core analysis functions, let's integrate them with ClientAI. Here's how we package them as tools for the AI to use:</p> <pre><code>def create_review_tools() -&gt; List[ToolConfig]:\n    \"\"\"Create tool configurations for the assistant.\"\"\"\n    return [\n        ToolConfig(\n            tool=analyze_python_code,\n            name=\"code_analyzer\", \n            description=(\n                \"Analyze Python code structure and complexity. \"\n                \"Expects a 'code' parameter with the Python code as a string.\"\n            ),\n            scopes=[\"observe\"],\n        ),\n        ToolConfig(\n            tool=check_style_issues,\n            name=\"style_checker\",\n            description=(\n                \"Check Python code style issues. \"\n                \"Expects a 'code' parameter with the Python code as a string.\"\n            ),\n            scopes=[\"observe\"],\n        ),\n    ]\n</code></pre> <p>The <code>ToolConfig</code> wrapper provides metadata that helps the AI understand and use our tools effectively:</p> <ul> <li><code>name</code>: A unique identifier the AI uses to reference the tool</li> <li><code>description</code>: Helps the AI understand what the tool does and how to use it</li> <li><code>scopes</code>: Controls when the tool can be used in the workflow - in this case, during observation steps when the AI is gathering information</li> </ul> <p>Now let's look at how we build our assistant that uses these tools:</p> <pre><code>class CodeReviewAssistant(Agent):\n    \"\"\"Code review assistant implementation.\"\"\"\n\n    @observe(\n        name=\"analyze_structure\",\n        description=\"Analyze code structure and style\",\n        stream=True,\n    )\n    def analyze_structure(self, code: str) -&gt; str:\n        \"\"\"First step: Analyze code structure.\"\"\"\n        self.context.state[\"code_to_analyze\"] = code\n\n        return \"\"\"\n        Please analyze this Python code structure and style:\n\n        Use the code_analyzer and style_checker tools to evaluate:\n        1. Code complexity and structure metrics\n        2. Style compliance issues\n        3. Function and class organization\n        4. Import usage patterns\n        \"\"\"\n</code></pre> <p>Our assistant inherits from ClientAI's Agent class, which provides the framework for creating AI-powered tools. The <code>@observe</code> decorator marks this method as an observation step - a step where we gather information about the code we're analyzing.</p> <p>Inside the method, we store the code in the assistant's context. This makes it available to our tools and other steps in the process. Then we return a prompt that tells the AI what we want it to analyze.</p> <p>Let's add a step for suggesting improvements:</p> <pre><code>    @think(\n        name=\"suggest_improvements\",\n        description=\"Generate improvement suggestions\",\n        stream=True,\n    )\n    def suggest_improvements(self, analysis_result: str) -&gt; str:\n        \"\"\"Second step: Generate improvement suggestions.\"\"\"\n        current_code = self.context.state.get(\"current_code\", \"\")\n\n        return f\"\"\"\n        Based on the code analysis of:\n\n        ```python\n        {current_code}\n        ```\n\n        And the analysis results:\n        {analysis_result}\n\n        Please suggest improvements for:\n        1. Reducing complexity\n        2. Fixing style issues\n        3. Improving organization\n        4. Enhancing readability\n        \"\"\"\n</code></pre> <p>This step takes the results from our analysis and asks the AI to generate specific suggestions for improvement. We're using the <code>@think</code> decorator here because this step involves processing information and making recommendations rather than just gathering data.</p> <p>Finally, we need a way to use our assistant. Here's how we set up the command-line interface:</p> <pre><code>def main():\n    \"\"\"Run the code analysis assistant.\"\"\"\n    config = OllamaServerConfig(\n        host=\"127.0.0.1\",\n        port=11434,\n        gpu_layers=35,\n        cpu_threads=8,\n    )\n\n    with OllamaManager(config) as manager:\n        client = ClientAI(\"ollama\", host=f\"http://{config.host}:{config.port}\")\n\n        assistant = CodeReviewAssistant(\n            client=client,\n            default_model=\"llama3\",\n            tools=create_review_tools(),\n            tool_confidence=0.8,\n            max_tools_per_step=2,\n        )\n</code></pre> <p>This sets up our connection to the local AI model and creates our assistant. We configure it to use our analysis tools and set parameters for how confidently it should use those tools.</p> <p>When you use the assistant, you can enter Python code and get back detailed analysis and suggestions:</p> <pre><code>def example(x,y):\n    if x &gt; 0:\n        if y &gt; 0:\n            return x+y\n    return 0\n</code></pre> <p>The assistant will analyze this code and point out several things:</p> <ul> <li>The nested if statements increase complexity</li> <li>The function name is good (it uses snake_case)</li> <li>It's missing type hints and a docstring</li> <li>The logic could be simplified</li> </ul> <p>The beauty of this system is that it combines static analysis (our Python tools) with AI-powered insights to provide comprehensive code review feedback. The AI can explain issues in a way that's easy to understand and suggest specific improvements based on best practices.</p> <p>You can build on this foundation by adding more types of analysis, improving the style checks, or even adding support for automatically fixing some of the issues it finds. The key is that we've created a flexible framework that can grow with your needs.</p>"},{"location":"examples/agent/simple_qa/","title":"Building a Simple Q&amp;A Bot with ClientAI","text":"<p>Let's build a straightforward Q&amp;A bot using ClientAI's <code>create_agent</code> function. This approach gives us powerful features like context management and response streaming while keeping the code minimal and easy to understand.</p>"},{"location":"examples/agent/simple_qa/#setting-up","title":"Setting Up","text":"<p>Before we start coding, you'll need to install ClientAI with OpenAI support. Open your terminal and run:</p> <pre><code>pip install clientai[openai]\n</code></pre> <p>You'll also need an OpenAI API key. Create a <code>.env</code> file in your project directory and add your key:</p> <pre><code>OPENAI_API_KEY=your_openai_api_key_here\n</code></pre>"},{"location":"examples/agent/simple_qa/#creating-the-bot","title":"Creating the Bot","text":"<p>Let's create our bot in a file called <code>qa_bot.py</code>. We'll break down each part of the code and understand what it does.</p> <p>First, let's import what we need:</p> <pre><code>from clientai import ClientAI\nfrom clientai.agent import create_agent\nfrom typing import Iterator, Union\n</code></pre> <p>Now let's write the function that creates our bot:</p> <pre><code>def create_bot(api_key: str = None):\n    \"\"\"Create a simple Q&amp;A bot.\"\"\"\n    # Initialize the AI client\n    client = ClientAI('openai', api_key=api_key)\n\n    # Create an agent with a helpful personality\n    system_prompt = \"\"\"\n    You are a friendly and helpful assistant. Your role is to:\n    - Answer questions clearly and concisely\n    - Maintain a conversational tone\n    - Ask for clarification when needed\n    \"\"\"\n\n    return create_agent(\n        client=client,\n        role=\"assistant\",\n        system_prompt=system_prompt,\n        model=\"gpt-4\",  # Or use \"gpt-3.5-turbo\" for a more economical option\n        stream=True,    # Enable real-time response streaming\n        temperature=0.7 # Add some creativity to responses\n    )\n</code></pre> <p>The <code>create_bot</code> function does two important things. First, it sets up a connection to OpenAI through ClientAI. Then it creates an agent with a specific personality defined in the system prompt. The agent will use GPT-4 (though you can switch to GPT-3.5 to save costs), stream its responses in real-time, and use a moderate temperature setting to balance creativity and accuracy.</p> <p>Next, we need a way to display the bot's responses. Since we're using streaming, we need to handle both regular and streaming responses:</p> <pre><code>def display_response(response: Union[str, Iterator[str]]):\n    \"\"\"Display the bot's response, handling both streaming and non-streaming.\"\"\"\n    if isinstance(response, str):\n        print(response)\n    else:\n        for chunk in response:\n            print(chunk, end=\"\", flush=True)\n        print()\n</code></pre> <p>This function checks whether it received a complete string or a stream of text chunks. For streams, it prints each chunk as it arrives, creating that nice \"thinking in real-time\" effect.</p> <p>Finally, let's create the main interaction loop:</p> <pre><code>def main():\n    # Create our bot\n    bot = create_bot()\n\n    print(\"Simple Q&amp;A Bot (type 'quit' to exit, 'clear' to reset)\")\n    print(\"Watch the bot think in real-time!\\n\")\n\n    while True:\n        # Get user input\n        question = input(\"\\nYou: \").strip()\n\n        # Handle commands\n        if question.lower() == 'quit':\n            break\n        elif question.lower() == 'clear':\n            bot.reset_context()\n            print(\"Memory cleared!\")\n            continue\n\n        # Get and display response\n        print(\"\\nBot: \", end=\"\")\n        response = bot.run(question)\n        display_response(response)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>The main loop creates a simple command-line interface where users can ask questions, clear the conversation history, or quit the program. When a question is asked, it runs it through the agent and displays the response in real-time.</p>"},{"location":"examples/agent/simple_qa/#using-your-bot","title":"Using Your Bot","text":"<p>Running the bot is as simple as executing the Python file:</p> <pre><code>python qa_bot.py\n</code></pre> <p>When you run it, you'll see a welcome message and a prompt for your first question. The bot will maintain context between questions, so you can have natural back-and-forth conversations. If you want to start fresh, just type 'clear'.</p>"},{"location":"examples/agent/simple_qa/#making-it-your-own","title":"Making It Your Own","text":"<p>The bot is quite flexible and can be customized in several ways. Want a more creative bot? Increase the temperature to 0.9. Need more precise answers? Lower it to 0.2. You can even change the bot's personality by modifying the system prompt - make it funny, professional, or anything in between.</p> <p>If you're watching costs, switch to \"gpt-3.5-turbo\" instead of \"gpt-4\". And if you prefer instant complete responses rather than streaming, just set <code>stream=False</code> in the create_agent call.</p>"},{"location":"examples/agent/simple_qa/#taking-it-further","title":"Taking It Further","text":"<p>This simple bot can grow with your needs. You might want to add error handling for when the API has issues, or save conversations to files for later reference. You could even create a web interface or add support for different AI providers. The foundation we've built here makes all of these enhancements straightforward to add.</p>"},{"location":"examples/agent/task_planner/","title":"ClientAI Tutorial: Building a Local Task Planner","text":"<p>In this tutorial, we'll create a local task planning system using ClientAI and Ollama. Our planner will break down goals into actionable tasks, create realistic timelines, and manage resources - all running on your local machine.</p>"},{"location":"examples/agent/task_planner/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Setting Up the Project</li> <li>Building the Task Planner</li> <li>Creating the Tools</li> <li>Implementing the Interface</li> <li>Running the Planner</li> <li>Further Improvements</li> </ol>"},{"location":"examples/agent/task_planner/#1-introduction","title":"1. Introduction","text":"<p>ClientAI with Ollama allows us to run AI models locally, making it perfect for tools like task planners. Our implementation will demonstrate: - Local AI model management with OllamaManager - Tool-based task decomposition - Realistic timeline generation with error handling - Structured plan formatting</p> <p>The end result will be a practical planning tool that runs entirely on your machine, with no need for external API calls.</p>"},{"location":"examples/agent/task_planner/#2-setting-up-the-project","title":"2. Setting Up the Project","text":"<p>First, create a new directory for your project:</p> <pre><code>mkdir local_task_planner\ncd local_task_planner\n</code></pre> <p>Install ClientAI with Ollama support:</p> <pre><code>pip install \"clientai[ollama]\"\n</code></pre> <p>Make sure you have Ollama installed on your system. You can get it from Ollama's website.</p>"},{"location":"examples/agent/task_planner/#3-building-the-task-planner","title":"3. Building the Task Planner","text":"<p>Let's start by importing our required modules and setting up logging:</p> <pre><code>from datetime import datetime, timedelta\nfrom typing import Dict, List\nimport logging\n\nfrom clientai import ClientAI\nfrom clientai.agent import create_agent, tool\nfrom clientai.ollama import OllamaManager\n\nlogger = logging.getLogger(__name__)\n</code></pre> <p>Now, let's create our TaskPlanner class. This will be the core of our application:</p> <pre><code>class TaskPlanner:\n    \"\"\"A local task planning system using Ollama.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the task planner with Ollama.\"\"\"\n        # Create an instance of OllamaManager to handle the local AI server\n        self.manager = OllamaManager()\n        # Placeholders for the AI client and planner agent\n        self.client = None  # Will hold the ClientAI instance\n        self.planner = None  # Will hold the planning agent\n\n    def start(self):\n        \"\"\"Start the Ollama server and initialize the client.\"\"\"\n        # Start the local Ollama server\n        self.manager.start()\n        # Create a ClientAI instance connected to the local Ollama server\n        self.client = ClientAI(\"ollama\", host=\"http://localhost:11434\")\n\n        # Create a planning agent with specific capabilities\n        self.planner = create_agent(\n            client=self.client,\n            role=\"task planner\",  # Define the agent's role\n            system_prompt=\"\"\"You are a practical task planner. Break down goals into\n            specific, actionable tasks with realistic time estimates and resource needs.\n            Use the tools provided to validate timelines and format plans properly.\"\"\",\n            model=\"llama3\",  # Specify which local model to use\n            step=\"think\",    # Set the agent to use thinking steps\n            tools=[validate_timeline, format_plan],  # Provide planning tools\n            tool_confidence=0.8,  # Set minimum confidence for tool usage\n            stream=True,  # Enable real-time response streaming\n        )\n\n    def stop(self):\n        \"\"\"Stop the Ollama server.\"\"\"\n        # Safely shut down the Ollama server if it's running\n        if self.manager:\n            self.manager.stop()\n</code></pre> <p>The initialization is straightforward - we create an OllamaManager instance and prepare placeholders for our client and planner. The actual initialization happens in the start method, which:</p> <ul> <li>Starts the Ollama server</li> <li>Creates a ClientAI instance connected to the server</li> <li>Initializes the planning agent with the necessary tools</li> </ul>"},{"location":"examples/agent/task_planner/#4-creating-the-tools","title":"4. Creating the Tools","text":"<p>Our planner needs two main tools: one for validating timelines and another for formatting plans:</p> <pre><code>@tool(name=\"validate_timeline\")  # Register this as a tool for the agent to use\ndef validate_timeline(tasks: Dict[str, int]) -&gt; Dict[str, dict]:\n    \"\"\"\n    Validate time estimates and create a realistic timeline.\n\n    Args:\n        tasks: Dictionary of task names and estimated hours\n\n    Returns:\n        Dictionary with start dates and deadlines\n    \"\"\"\n    try:\n        # Initialize timeline calculation with current date as starting point\n        current_date = datetime.now()\n        timeline = {}\n        accumulated_hours = 0  # Track total hours for sequential scheduling\n\n        for task, hours in tasks.items():\n            try:\n                # Convert hours to integer safely, handling various input types\n                # str() handles potential non-string inputs\n                # float() handles decimal numbers\n                # int() converts to final integer form\n                hours_int = int(float(str(hours)))\n\n                # Skip tasks with zero or negative hours\n                if hours_int &lt;= 0:\n                    logger.warning(f\"Skipping task {task}: Invalid hours value {hours}\")\n                    continue\n\n                # Calculate working days needed (assuming 6 productive hours per day)\n                days_needed = hours_int / 6\n\n                # Calculate start date based on accumulated hours from previous tasks\n                start_date = current_date + timedelta(hours=accumulated_hours)\n                # Calculate end date based on working days needed\n                end_date = start_date + timedelta(days=days_needed)\n\n                # Store task timeline information\n                timeline[task] = {\n                    \"start\": start_date.strftime(\"%Y-%m-%d\"),  # Format dates as strings\n                    \"end\": end_date.strftime(\"%Y-%m-%d\"),\n                    \"hours\": hours_int,\n                }\n\n                # Add this task's hours to accumulated total for next task's scheduling\n                accumulated_hours += hours_int\n\n            except (ValueError, TypeError) as e:\n                # Handle invalid hour values (non-numeric strings, invalid types, etc.)\n                logger.warning(f\"Skipping task {task}: Invalid hours value {hours} - {e}\")\n                continue\n\n        return timeline\n    except Exception as e:\n        # Catch any unexpected errors and return empty timeline rather than failing\n        logger.error(f\"Error validating timeline: {str(e)}\")\n        return {}\n\n@tool(name=\"format_plan\")  # Register this as a named tool for the agent\ndef format_plan(\n    tasks: List[str],          # List of task names\n    timeline: Dict[str, dict], # Timeline data from validate_timeline tool\n    resources: List[str]       # List of required resources\n) -&gt; str:\n    \"\"\"\n    Format the plan in a clear, structured way.\n\n    Args:\n        tasks: List of tasks\n        timeline: Timeline from validate_timeline\n        resources: List of required resources\n\n    Returns:\n        Formatted plan as a string\n    \"\"\"\n    try:\n        # Create header for the plan\n        plan = \"== Project Plan ==\\n\\n\"\n\n        # Start tasks section with timeline details\n        plan += \"Tasks and Timeline:\\n\"\n        for i, task in enumerate(tasks, 1):  # Enumerate from 1 for natural numbering\n            if task in timeline:  # Only include tasks that have timeline data\n                t = timeline[task]\n                # Format each task with indented details\n                plan += f\"\\n{i}. {task}\\n\"                      # Task name with number\n                plan += f\"   Start: {t['start']}\\n\"            # Start date indented\n                plan += f\"   End: {t['end']}\\n\"                # End date indented\n                plan += f\"   Estimated Hours: {t['hours']}\\n\"  # Hours indented\n\n        # Add resources section\n        plan += \"\\nRequired Resources:\\n\"\n        for resource in resources:\n            plan += f\"- {resource}\\n\"  # Bullet points for resources\n\n        return plan\n    except Exception as e:\n        # Log any formatting errors and return error message\n        logger.error(f\"Error formatting plan: {str(e)}\")\n        return \"Error: Unable to format plan\"\n</code></pre>"},{"location":"examples/agent/task_planner/#5-implementing-the-interface","title":"5. Implementing the Interface","text":"<p>Now let's add the main interface method and command-line interface:</p> <pre><code>def get_plan(self, goal: str) -&gt; str:\n    \"\"\"\n    Generate a plan for the given goal.\n\n    Args:\n        goal: The goal to plan for\n\n    Returns:\n        A formatted plan string\n    \"\"\"\n    if not self.planner:\n        raise RuntimeError(\"Planner not initialized. Call start() first.\")\n\n    return self.planner.run(goal)\n\ndef main():\n    # Create an instance of our TaskPlanner\n    planner = TaskPlanner()\n\n    try:\n        # Display welcome messages and instructions\n        print(\"Task Planner (Local AI)\")\n        print(\n            \"Enter your goal, and I'll create a practical, timeline-based plan.\"\n        )\n        print(\"Type 'quit' to exit.\")\n\n        # Initialize the planner and start the Ollama server\n        planner.start()\n\n        # Main interaction loop\n        while True:\n            # Visual separator for better readability\n            print(\"\\n\" + \"=\" * 50 + \"\\n\")\n            # Get user input\n            goal = input(\"Enter your goal: \")\n\n            # Check for quit command\n            if goal.lower() == \"quit\":\n                break\n\n            try:\n                # Get the plan from the planner\n                plan = planner.get_plan(goal)\n                print(\"\\nYour Plan:\\n\")\n                # Stream the response chunk by chunk for real-time output\n                for chunk in plan:\n                    print(chunk, end=\"\", flush=True)  # flush=True ensures immediate display\n            except Exception as e:\n                # Handle any errors during plan generation\n                print(f\"Error: {str(e)}\")\n\n    finally:\n        # Ensure the Ollama server is properly shut down\n        # This runs even if there's an error or user quits\n        planner.stop()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>The interface includes:</p> <ul> <li>Proper error handling for uninitialized planner</li> <li>Graceful shutdown in the finally block</li> <li>Streaming output support with flush</li> <li>Clear user instructions</li> <li>Clean exit handling</li> </ul>"},{"location":"examples/agent/task_planner/#6-running-the-planner","title":"6. Running the Planner","text":"<p>To use the planner, simply run:</p> <pre><code>python task_planner.py\n</code></pre> <p>Example interaction:</p> <pre><code>Task Planner (Local AI)\nEnter your goal, and I'll create a practical, timeline-based plan.\nType 'quit' to exit.\n\n==================================================\n\nEnter your goal: Create a personal portfolio website\n\nYour Plan:\n\n== Project Plan ==\n\nTasks and Timeline:\n1. Requirements Analysis and Planning\n   Start: 2024-12-08\n   End: 2024-12-09\n   Estimated Hours: 6\n\n2. Design and Wireframing\n   Start: 2024-12-09\n   End: 2024-12-11\n   Estimated Hours: 12\n\n3. Content Creation\n   Start: 2024-12-11\n   End: 2024-12-12\n   Estimated Hours: 8\n\n4. Development\n   Start: 2024-12-12\n   End: 2024-12-15\n   Estimated Hours: 20\n\nRequired Resources:\n- Design software (e.g., Figma)\n- Text editor or IDE\n- Web hosting service\n- Version control system\n</code></pre>"},{"location":"examples/agent/task_planner/#understanding-the-implementation","title":"Understanding the Implementation","text":"<p>Our task planner has several key components working together:</p> <ol> <li> <p>Timeline Validator:</p> <ul> <li>Converts all time estimates to integers safely</li> <li>Assumes 6 productive hours per day for realistic scheduling</li> <li>Handles invalid inputs gracefully</li> <li>Provides detailed logging for debugging</li> </ul> </li> <li> <p>Plan Formatter:</p> <ul> <li>Creates consistent, readable output</li> <li>Includes error handling for malformed data</li> <li>Clearly separates tasks, timelines, and resources</li> </ul> </li> <li> <p>OllamaManager Integration:</p> <ul> <li>Handles server lifecycle automatically</li> <li>Provides clean startup and shutdown</li> <li>Manages the connection to the local AI model</li> </ul> </li> <li> <p>Error Handling:</p> <ul> <li>Comprehensive try/except blocks</li> <li>Detailed logging</li> <li>User-friendly error messages</li> <li>Graceful degradation on failures</li> </ul> </li> </ol>"},{"location":"examples/agent/task_planner/#7-further-improvements","title":"7. Further Improvements","text":"<p>Consider these enhancements to make the planner even more robust:</p> <ul> <li>Add dependency tracking between tasks</li> <li>Include cost calculations for resources</li> <li>Save plans to files or project management tools</li> <li>Track progress against the original plan</li> <li>Add validation for resource availability</li> <li>Implement parallel task scheduling</li> <li>Add support for recurring tasks</li> <li>Include priority levels for tasks</li> </ul> <p>Remember that performance will depend on your chosen Ollama model. Experiment with different models to find the right balance between speed and plan quality for your needs.</p>"},{"location":"examples/agent/writing_assistant/","title":"Building an AI Writing Assistant with ClientAI","text":"<p>Let's create a sophisticated writing assistant using ClientAI's agent framework. This assistant will analyze text, suggest improvements, and rewrite content while maintaining context throughout the process. We'll build it step by step and explain each component.</p>"},{"location":"examples/agent/writing_assistant/#setting-up","title":"Setting Up","text":"<p>First, you'll need to install ClientAI with Groq support. Open your terminal and run:</p> <pre><code>pip install clientai[groq]\n</code></pre> <p>You'll need a Groq API key. Create a <code>.env</code> file in your project directory and add your key:</p> <pre><code>GROQ_API_KEY=your_groq_api_key_here\n</code></pre>"},{"location":"examples/agent/writing_assistant/#creating-the-writing-assistant","title":"Creating the Writing Assistant","text":"<p>Let's create our assistant in <code>writing_assistant.py</code>. We'll break down each component to understand how it works.</p> <p>First, let's import our dependencies:</p> <pre><code>from clientai import ClientAI\nfrom clientai.agent import Agent, think, act, synthesize\nfrom clientai.agent.config import ToolConfig\n</code></pre> <p>Now let's create a helpful formatting tool that our assistant can use:</p> <pre><code>def format_text(text: str, style: str = \"paragraph\") -&gt; str:\n    \"\"\"Format text in different styles.\"\"\"\n    text = text.strip()\n\n    if style == \"bullet\":\n        lines = text.split('\\n')\n        return '\\n'.join(f\"\u2022 {line.strip()}\" for line in lines if line.strip())\n    elif style == \"numbered\":\n        lines = text.split('\\n')\n        return '\\n'.join(f\"{i}. {line.strip()}\" for i, line in enumerate(lines, 1) if line.strip())\n    else:\n        return ' '.join(line.strip() for line in text.split('\\n') if line.strip())\n</code></pre> <p>This tool helps format text in different styles - as paragraphs, bullet points, or numbered lists. The assistant can use this when analyzing or presenting improvements.</p> <p>Next, let's create our WritingAssistant class:</p> <pre><code>class WritingAssistant(Agent):\n    \"\"\"\n    An AI writing assistant that helps improve text by:\n    1. Analyzing the input text\n    2. Suggesting improvements \n    3. Applying those improvements\n    \"\"\"\n\n    @think(\n        # Using explicit name\n        name=\"analyze_text\",\n        # Using explicit description, docstring will be ignored\n        description=\"Identify issues in the provided text\", \n        model={\n            \"name\": \"llama-3.2-3b-preview\",\n            \"temperature\": 0.7\n        }\n    )\n    def analyze(self, text: str) -&gt; str:\n        \"\"\"This docstring is ignored since description is provided in decorator.\"\"\"\n        return f\"\"\"\n        Identify specific issues in this text that need improvement:\n\n        {text}\n\n        List only the actual problems in the text as bullet points. Be specific and brief.\n        \"\"\"\n\n    # No name specified - will use function name \"suggest\"\n    # No description specified - will use function docstring\n    @act\n    def suggest(self, analysis: str) -&gt; str:\n        \"\"\"Generate specific suggestions for improving the analyzed text.\"\"\"\n        return f\"\"\"\n        Based on these identified issues:\n        {analysis}\n\n        Looking at this text:\n        {self.context.current_input}\n\n        Provide 3 specific ways to further improve this text.\n        Each suggestion should directly address an issue from the analysis.\n        Format as a numbered list of brief, actionable changes.\n        \"\"\"\n\n    @synthesize(\n        description=\"Create improved version of the text\"  # Explicit description - docstring ignored\n        # No name - will use function name \"improve\"\n    )\n    def improve(self, suggestions: str) -&gt; str:\n        \"\"\"This docstring is ignored since description is provided in decorator.\"\"\"\n        return f\"\"\"\n        Using these improvement suggestions:\n        {suggestions}\n\n        Rewrite this original text:\n        {self.context.current_input}\n\n        IMPORTANT: Your response should be ONLY the improved text itself.\n        Do not include any preamble like \"Here's the improved version\" or any other commentary.\n        Write just the improved text as a single cohesive paragraph.\n        \"\"\"\n</code></pre> <p>The WritingAssistant works in three steps:</p> <ol> <li><code>analyze_text</code>: Identifies specific issues in the text</li> <li><code>suggest</code>: Generates actionable improvement suggestions</li> <li><code>improve</code>: Creates an improved version incorporating the suggestions</li> </ol> <p>Let's create a function to set up our assistant:</p> <pre><code>def create_assistant(api_key: str = None):\n    \"\"\"\n    Create and configure the writing assistant.\n\n    Sets up:\n    1. The AI client with appropriate provider (Groq)\n    2. The writing assistant with default model configuration\n    3. Available tools with proper scoping\n\n    Args:\n        api_key: Optional Groq API key (can also use environment variable)\n\n    Returns:\n        WritingAssistant: Configured assistant ready to process text\n    \"\"\"\n    # Try to get API key from parameter or environment\n    groq_api_key = api_key or os.getenv(\"GROQ_API_KEY\")\n\n    if not groq_api_key:\n        raise ValueError(\n            \"Groq API key must be provided either as a parameter or \"\n            \"through the GROQ_API_KEY environment variable\"\n        )\n\n    # Initialize the AI client with Groq\n    client = ClientAI('groq', api_key=groq_api_key)\n\n    # Create the writing assistant with:\n    # - Base model: llama-3.2-3b-preview for general text processing\n    # - Formatting tool: Available during analysis phase\n    return WritingAssistant(\n        client,\n        default_model=\"llama-3.2-3b-preview\",\n        tools=[\n            ToolConfig(\n                # The tool function to register\n                format_text,\n                # Name used by LLM to reference the tool\n                name=\"format\",\n                # Only available during analysis (\"think\") phase\n                scopes=[\"think\"],\n                # Clear description helps LLM understand when to use the tool\n                description=\"Format text as paragraphs, bullet points, or numbered lists.\"\n                            \"Requires 'text' parameter and optional 'style' parameter \"\n                            \"('paragraph', 'bullet', 'numbered').\"\n            )\n        ]\n    )\n</code></pre> <p>Now let's create a simple interface to use our assistant:</p> <pre><code>def main():\n    try:\n        # Create our assistant\n        assistant = create_assistant()\n\n        print(\"AI Writing Assistant (type 'quit' to exit)\")\n        print(\"Enter your text and watch it get improved!\\n\")\n\n        while True:\n            # Get user input\n            text = input(\"\\nEnter text to improve (or 'quit'): \").strip()\n\n            if text.lower() == 'quit':\n                break\n\n            # Improve the text\n            print(\"\\nAnalyzing and improving text...\")\n            improved = assistant.run(text)\n\n            # Show results\n            print(\"\\nOriginal text:\")\n            print(text)\n\n            print(\"\\nAnalysis:\")\n            print(assistant.context.last_results[\"analyze_text\"])\n\n            print(\"\\nSuggestions:\")\n            print(assistant.context.last_results[\"suggest\"])\n\n            print(\"\\nImproved text:\")\n            print(improved)\n\n    except ValueError as e:\n        print(f\"\\nError: {e}\")\n        print(\"Please make sure your Groq API key is properly set.\")\n    except Exception as e:\n        print(f\"\\nAn unexpected error occurred: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/agent/writing_assistant/#using-your-writing-assistant","title":"Using Your Writing Assistant","text":"<p>To use the assistant, simply run the Python file:</p> <pre><code>python writing_assistant.py\n</code></pre> <p>You'll see a prompt where you can enter text. The assistant will:</p> <ol> <li>Analyze the text for issues</li> <li>Suggest specific improvements</li> <li>Provide an improved version</li> <li>Show you the full analysis and reasoning</li> </ol>"},{"location":"examples/agent/writing_assistant/#taking-it-further","title":"Taking It Further","text":"<p>This foundation can be extended in many ways:</p> <ul> <li>Add support for different writing styles (formal, casual, technical)</li> <li>Include grammar checking tools</li> <li>Save before/after versions of improved texts</li> <li>Create a web interface</li> <li>Add support for longer documents</li> <li>Integrate with document editors</li> </ul> <p>The modular design makes it easy to add new capabilities while maintaining the clear three-step improvement process.</p>"},{"location":"examples/client/ai_dungeon_master/","title":"ClientAI Tutorial: Building an AI Dungeon Master","text":"<p>In this tutorial, we'll walk through the process of creating an AI-powered Dungeon Master using the ClientAI package. We'll explain each concept in detail and build our game step-by-step, providing context for every decision we make, both technical and gameplay-related.</p>"},{"location":"examples/client/ai_dungeon_master/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Setting Up the Project  2.5 Creating the Project Structure</li> <li>Creating the Game Structure</li> <li>Integrating Multiple AI Providers</li> <li>Developing the Enhanced AI Dungeon Master</li> <li>Main Script that Runs the Game</li> <li>Running the Game</li> <li>Conclusion and Further Improvements</li> </ol>"},{"location":"examples/client/ai_dungeon_master/#1-introduction","title":"1. Introduction","text":"<p>ClientAI is a Python package that provides a unified interface for interacting with multiple AI providers. In this tutorial, we'll use ClientAI to create an AI Dungeon Master that can generate story elements, NPC dialogues, and dynamic environments using different AI models.</p> <p>Our AI Dungeon Master will be a text-based role-playing game (RPG) where the game's content is dynamically generated by AI. This approach allows for infinite replayability and unique experiences for each player.</p> <p>We'll focus on explaining both technical decisions (such as class structures and AI interactions) and gameplay decisions (like character creation and game mechanics).</p> <p>The final result is available in this github repo.</p>"},{"location":"examples/client/ai_dungeon_master/#2-setting-up-the-project","title":"2. Setting Up the Project","text":"<p>First, let's set up our project and install the necessary dependencies.</p> <ol> <li>Create a new directory for your project:</li> </ol> <pre><code>mkdir ai_dungeon_master\ncd ai_dungeon_master\n</code></pre> <ol> <li> <p>Install ClientAI and its dependencies:</p> <p>If you want to use poetry, you may skip this part.</p> </li> </ol> <pre><code>pip install clientai[all]\n</code></pre> <p>This command installs ClientAI with support for all providers. If you only need specific providers, you can install them individually (e.g., <code>pip install clientai[openai]</code> for just OpenAI support).</p> <ol> <li> <p>Install additional dependencies:</p> <p>If you want to use poetry, you may also skip this part.</p> </li> </ol> <p>We'll need some additional packages for our project.</p> <pre><code>pip install requests\n</code></pre> <ul> <li> <p><code>requests</code>: For making HTTP requests to check if the local AI servers are running.</p> </li> <li> <p>Install Ollama:</p> </li> </ul> <p>Ollama is a local AI model server that we'll use to run the Llama 3 model. Follow these steps to install Ollama:</p> <ul> <li> <p>For macOS or Linux:   <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre></p> </li> <li> <p>For Windows:   Download the installer from the Ollama GitHub releases page and follow the installation instructions.</p> </li> <li> <p>Pull the Llama 3 model from Ollama:</p> </li> </ul> <p>After installing Ollama, you need to download the Llama 3 model. Run the following command:</p> <pre><code>ollama pull llama3\n</code></pre> <p>This command will download and set up the Llama 3 model for use with Ollama. The download might take some time depending on your internet connection.</p> <p>These imports will be used throughout our project:</p> <ul> <li><code>random</code>: For generating random numbers and making random choices.</li> <li><code>subprocess</code>: For starting and managing subprocesses like local AI servers.</li> <li><code>time</code>: For adding delays and managing timeouts.</li> <li><code>requests</code>: For making HTTP requests to check server availability.</li> <li><code>logging</code>: For logging information and errors.</li> <li><code>ClientAI</code>: The main class from the ClientAI package that we'll use to interact with AI providers.</li> </ul>"},{"location":"examples/client/ai_dungeon_master/#25-creating-the-project-structure","title":"2.5 Creating the Project Structure","text":"<p>Before we dive into the code, let's set up a proper project structure. This will help us organize our code and make it easier to maintain and expand in the future.</p> <ol> <li>Create the following directory structure:</li> </ol> <pre><code>clientai_dungeon_master/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 ai_dungeon_master/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 main.py\n    \u251c\u2500\u2500 game/\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 character.py\n    \u2502   \u251c\u2500\u2500 game_state.py\n    \u2502   \u2514\u2500\u2500 dungeon_master.py\n    \u251c\u2500\u2500 ai/\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 ai_providers.py\n    \u2502   \u2514\u2500\u2500 ollama_server.py\n    \u2514\u2500\u2500 utils/\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 text_utils.py\n</code></pre> <ol> <li> <p>Create a <code>pyproject.toml</code> file in the root directory with the following content:</p> <p>If you're using pip directly, you may skip this part</p> </li> </ol> <pre><code>[tool.poetry]\nname = \"clientai-dungeon-master\"\nversion = \"0.1.0\"\ndescription = \"An AI-powered dungeon master for text-based RPG adventures\"\nauthors = [\"Your Name &lt;your.email@example.com&gt;\"]\nreadme = \"README.md\"\npackages = [{include = \"clientai_dungeon_master\"}]\n\n[tool.poetry.dependencies]\npython = \"^3.11\"\nclientai = \"^0.1.2\"\nrequests = \"^2.32.3\"\npython-decouple = \"^3.8\"\n\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n</code></pre> <p>and run</p> <pre><code>poetry install\n</code></pre> <ol> <li>Create a <code>.gitignore</code> file in the root directory with the following content:</li> </ol> <pre><code># Python\n__pycache__/\n*.py[cod]\n*.pyo\n*.pyd\n.Python\nenv/\nvenv/\nENV/\n\n# Poetry\n.venv/\ndist/\n\n# Environment variables\n.env\n\n# IDEs\n.vscode/\n.idea/\n\n# Logs\n*.log\n\n# OS generated files\n.DS_Store\n.DS_Store?\n._*\n.Spotlight-V100\n.Trashes\nehthumbs.db\nThumbs.db\n</code></pre> <ol> <li>Create a <code>.env</code> file in the root directory to store your API keys:</li> </ol> <pre><code>OPENAI_API_KEY=your_openai_api_key_here\nREPLICATE_API_KEY=your_replicate_api_key_here\n</code></pre> <p>Remember to replace <code>your_openai_api_key_here</code> and <code>your_replicate_api_key_here</code> with your actual API keys.</p> <ol> <li>Move the relevant code into the appropriate files based on the new structure.</li> </ol> <p>This structure separates concerns, making the code more modular and easier to maintain. It also sets up the project for potential future expansion, such as adding more game features or integrating additional AI providers.</p>"},{"location":"examples/client/ai_dungeon_master/#3-creating-the-game-structure","title":"3. Creating the Game Structure","text":"<p>Before integrating AI, we'll create the basic structure of our game. This includes classes to represent the character, game state, and AI providers.</p>"},{"location":"examples/client/ai_dungeon_master/#character-class","title":"Character Class","text":"<p>The <code>Character</code> class represents the player's character in the game. It stores essential character information like name, race, class, background story, and stats.</p> ai_dungeon_master/game/character.py<pre><code>class Character:\n    def __init__(self, name: str, race: str, class_type: str, background: str, stats: dict):\n        self.name = name\n        self.race = race\n        self.class_type = class_type\n        self.background = background\n        self.stats = stats\n\n    def __str__(self):\n        return f\"Name: {self.name}, Race: {self.race}, Class: {self.class_type}, Background: {self.background}, Stats: {self.stats}\"\n</code></pre> <p>Here we define a character with attributes like a name, race, class, background and stats (like Strength, Intelligence, Wisdom). This is really simple, but will be enough to customize what happens in the story.</p> <p>We'll also define the <code>__str__</code> method to be able to print the character's details easily.</p>"},{"location":"examples/client/ai_dungeon_master/#gamestate-class","title":"GameState Class","text":"<p>The <code>GameState</code> class keeps track of the game's current state, including the character's status, location, inventory, health, experience, and quests.</p> ai_dungeon_master/game/game_state.py<pre><code>from typing import Optional\n\nfrom .character import Character\n\nclass GameState:\n    def __init__(self, character: Character):\n        self.character = character\n        self.location = \"entrance\"\n        self.inventory = []\n        self.health = 100\n        self.experience = 0\n        self.quests = []\n\n    def update(self, location: Optional[str] = None, item: Optional[str] = None, health_change: int = 0, exp_gain: int = 0, quest: Optional[str] = None):\n        if location:\n            self.location = location\n        if item:\n            self.inventory.append(item)\n        self.health = max(0, min(100, self.health + health_change))\n        self.experience += exp_gain\n        if quest:\n            self.quests.append(quest)\n\n    def __str__(self):\n        return f\"{str(self.character)}\\nLocation: {self.location}, Health: {self.health}, XP: {self.experience}, Inventory: {', '.join(self.inventory)}, Quests: {', '.join(self.quests)}\"\n</code></pre> <p>We keep track of the state to keep a more consistent experience, we can't expect this to be always generated by the llm. We need to pass the game state as a guide to generate the content.</p> <p>The update method allows easy updates to the game state, we'll keep health within 0 to 100, and add an inventory and quests to add more depth to the game.</p>"},{"location":"examples/client/ai_dungeon_master/#4-integrating-multiple-ai-providers","title":"4. Integrating Multiple AI Providers","text":"<p>We'll use ClientAI to create a class that manages interactions with different AI providers. This abstraction allows us to switch between providers seamlessly.</p>"},{"location":"examples/client/ai_dungeon_master/#aiproviders-class","title":"AIProviders Class","text":"ai_dungeon_master/ai/ai_providers.py<pre><code>from typing import List\n\nfrom clientai import ClientAI\n\nclass AIProviders:\n    def __init__(self):\n        self.openai = ClientAI('openai', api_key=openai_token)\n        self.replicate = ClientAI('replicate', api_key=replicate_token)\n        self.ollama = ClientAI('ollama', host=\"http://localhost:11434\")\n\n    def chat(\n        self,\n        messages: List[dict],\n        provider: str = 'openai',\n        openai_model=\"gpt-4o-mini\",\n        replicate_model=\"meta/meta-llama-3-8b-instruct\",\n        ollama_model=\"llama3\",\n    ):\n        if provider == 'openai':\n            return self.openai.chat(messages, model=openai_model, stream=True)\n        elif provider == 'replicate':\n            return self.replicate.chat(messages, model=replicate_model, stream=True)\n        elif provider == 'ollama':\n            return self.ollama.chat(messages, model=ollama_model, stream=True)\n        else:\n            raise ValueError(f\"Unknown provider: {provider}\")\n</code></pre> <p>We create instances of ClientAI for each provider with the necessary API keys or host information, then abstract the chat method to allow for easy switching between AI providers.</p> <p>We are going to use ClientAI to use multiple AI models from different providers, since we want to find what is the best model for each task balancing performance and costs.</p>"},{"location":"examples/client/ai_dungeon_master/#managing-api-keys-with-python-decouple-and-a-env-file","title":"Managing API Keys with python-decouple and a .env File","text":"<p>To securely handle your API keys without exposing them in your codebase, you can use the python-decouple package and store your keys in a .env file. This approach keeps sensitive information out of your code and version control.</p> <ol> <li>Install python-decouple:    You may skip this if you used poetry</li> </ol> <pre><code>pip install python-decouple\n</code></pre> <ol> <li>Create a .env File:    In your project's root directory, make sure the <code>.env</code> has your API keys:</li> </ol> <pre><code>OPENAI_API_KEY=your_openai_api_key_here\nREPLICATE_API_KEY=your_replicate_api_key_here\n</code></pre> <p>Replace <code>your_openai_api_key_here</code> and <code>your_replicate_api_key_here</code> with your actual API keys.</p> <ol> <li>Ensure .env is added to .gitignore:    To prevent the .env file from being tracked by version control, ensure it is in your .gitignore file:</li> </ol> <pre><code># .gitignore\n.env\n</code></pre> <p>This ensures your API keys remain private and aren't pushed to repositories like GitHub.</p> <ol> <li>Access the API Keys in Your Code:    Import <code>config</code> from decouple and retrieve the API keys:</li> </ol> ai_dungeon_master/ai/ai_providers.py<pre><code>from decouple import config\n\nopenai_token = config('OPENAI_API_KEY')\nreplicate_token = config('REPLICATE_API_KEY')\n</code></pre> <p>Now, you can use these variables when initializing your AI providers.</p> <ol> <li>Update the AIProviders Class:    ai_dungeon_master/ai/ai_providers.py<pre><code>from typing import List\n\nfrom clientai import ClientAI\nfrom decouple import config\n\nopenai_token = config('OPENAI_API_KEY')\nreplicate_token = config('REPLICATE_API_KEY')he ol\n\nclass AIProviders:\n    def __init__(self):\n        self.openai = ClientAI('openai', api_key=openai_token)\n        self.replicate = ClientAI('replicate', api_key=replicate_token)\n        self.ollama = ClientAI('ollama', host=\"http://localhost:11434\")\n\n ...\n</code></pre></li> </ol>"},{"location":"examples/client/ai_dungeon_master/#managing-ai-servers","title":"Managing AI Servers","text":"<p>We need to ensure that local AI servers (like Ollama) are running before the game starts, so let's define a function to start ollama.</p> ai_dungeon_master/ai/ollama_server.py<pre><code>import subprocess\nimport time\nimport requests\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef start_ollama_server(timeout: int = 30, check_interval: float = 1.0):\n    \"\"\"\n    Start the Ollama server and wait for it to be ready.\n    \"\"\"\n    logging.info(\"Starting Ollama server...\")\n\n    try:\n        process = subprocess.Popen(\n            ['ollama', 'serve'],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n    except subprocess.SubprocessError as e:\n        logging.error(f\"Failed to start Ollama process: {e}\")\n        raise\n\n    start_time = time.time()\n    while time.time() - start_time &lt; timeout:\n        try:\n            response = requests.get('http://localhost:11434', timeout=5)\n            if response.status_code == 200:\n                logging.info(\"Ollama server is ready.\")\n                return process\n        except requests.ConnectionError:\n            pass\n        except requests.RequestException as e:\n            logging.error(f\"Unexpected error when checking Ollama server: {e}\")\n            process.terminate()\n            raise\n\n        if process.poll() is not None:\n            stdout, stderr = process.communicate()\n            logging.error(f\"Ollama process terminated unexpectedly. stdout: {stdout}, stderr: {stderr}\")\n            raise subprocess.SubprocessError(\"Ollama process terminated unexpectedly\")\n\n        time.sleep(check_interval)\n\n    process.terminate()\n    raise TimeoutError(f\"Ollama server did not start within {timeout} seconds\")\n</code></pre> <p>By managing the server startup within the code, we reduce the setup burden on the player.</p>"},{"location":"examples/client/ai_dungeon_master/#5-developing-the-enhanced-ai-dungeon-master","title":"5. Developing the Enhanced AI Dungeon Master","text":"<p>Now we'll develop the main class that controls the game logic and interactions with AI models.</p>"},{"location":"examples/client/ai_dungeon_master/#enhancedaidungeonmaster-class","title":"EnhancedAIDungeonMaster Class","text":"ai_dungeon_master/game/dungeon_master.py<pre><code>from typing import Tuple, List\nimport random\nimport time\n\nfrom ai.ai_providers import AIProviders\nfrom utils.text_utils import print_separator\nfrom game.character import Character\nfrom game.game_state import GameState\n\nclass EnhancedAIDungeonMaster:\n    def __init__(self):\n        self.ai = AIProviders()\n        self.conversation_history = []\n        self.game_state = None\n\n    # Methods will be added here...\n</code></pre>"},{"location":"examples/client/ai_dungeon_master/#creating-the-character","title":"Creating the Character","text":"<p>We need a method to create the player's character. We'll use AI to do this automatically for us:</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def create_character(self):\n        print(\"Let's create your character!\")\n        name = input(\"What is your character's name? \")\n\n        # We start by defining a prompt\n        character_prompt = f\"\"\"\n        Create a character for a fantasy RPG with the following details:\n        Name: {name}\n\n        Please provide:\n        1. A suitable race (e.g., Human, Elf, Dwarf, etc.)\n        2. A class (e.g., Warrior, Mage, Rogue, etc.)\n        3. A brief background story (2-3 sentences)\n        4. Basic stats (Strength, Dexterity, Constitution, Intelligence, Wisdom, Charisma) on a scale of 1-20\n\n        Format the response as follows:\n        Race: [race]\n        Class: [class]\n        Background: [background story]\n        Stats:\n        - Strength: [value]\n        - Dexterity: [value]\n        - Constitution: [value]\n        - Intelligence: [value]\n        - Wisdom: [value]\n        - Charisma: [value]\n        \"\"\"\n\n        # And we add this prompt to our chat history\n        self.add_to_history(\"user\", character_prompt)\n        character_info = self.print_stream(self.ai.chat(self.conversation_history, provider='openai'))\n\n        # Parse the character info\n        lines = character_info.strip().split('\\n')\n        race = class_type = background = \"\"\n        stats = {}\n\n        for line in lines:\n            if line.startswith(\"Race:\"):\n                race = line.split(\": \", 1)[1].strip()\n            elif line.startswith(\"Class:\"):\n                class_type = line.split(\": \", 1)[1].strip()\n            elif line.startswith(\"Background:\"):\n                background = line.split(\": \", 1)[1].strip()\n            elif \":\" in line and not line.startswith(\"Stats:\"):\n                key, value = line.split(\":\", 1)\n                key = key.strip(\"- \")\n                try:\n                    stats[key] = int(value.strip())\n                except ValueError:\n                    stats[key] = random.randint(1, 20)\n\n        # Just in case, let's ensure it'the player has stats\n        # If any stat is missing, assign a random value\n        for stat in [\"Strength\", \"Dexterity\", \"Constitution\", \"Intelligence\", \"Wisdom\", \"Charisma\"]:\n            if stat not in stats:\n                stats[stat] = random.randint(1, 20)\n\n        # And let's also ensure other required attributes are assigned\n        # If race, class, or background is empty, assign default values\n        race = race or \"Human\"\n        class_type = class_type or \"Adventurer\"\n        background = background or \"A mysterious traveler with an unknown past.\"\n\n        return Character(name, race, class_type, background, stats)\n</code></pre> <p>We'll use GPT 4o mini to create initial stuff we need, like the race, class, background etc, and extract the information from the generated content to handle errors.</p> <p>Note that since we are leaving this information to the LLM, the name will influence the attributes. If you need a more consistently random generation, do it in the python code and just pass it to the prompt.</p>"},{"location":"examples/client/ai_dungeon_master/#maintaining-conversation-history","title":"Maintaining Conversation History","text":"<p>To provide context to the AI, we maintain a conversation history.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def add_to_history(self, role: str, content: str):\n        if not self.conversation_history or self.conversation_history[-1]['content'] != content:\n            self.conversation_history.append({\"role\": role, \"content\": content})\n            if len(self.conversation_history) &gt; 10:\n                self.conversation_history = self.conversation_history[-10:]\n</code></pre> <p>Here we will ensure we don't add the same message twice. Plus, we are limiting the conversation history to 10 messages to prevent exceeding token limits.</p>"},{"location":"examples/client/ai_dungeon_master/#generating-the-environment","title":"Generating the Environment","text":"<p>Next, let's create detailed environments to enhance the imersion.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def generate_environment(self):\n        if not hasattr(self, 'current_environment'):\n            prompt = f\"\"\"\n            The character {self.game_state.character.name} is a {self.game_state.character.race} {self.game_state.character.class_type} \n            currently in the {self.game_state.location}.\n\n            Describe the current environment in detail, focusing on:\n            1. The physical setting and atmosphere\n            2. Any notable NPCs present\n            3. Interesting objects or features\n\n            Do not create a new character or change any existing character details.\n            Do not include any actions or dialogue for {self.game_state.character.name}.\n\n            End your description with one of these tags if appropriate:\n            [INTERACT_OPPORTUNITY] - if there's a chance for the player to interact with someone or something\n            [QUEST_OPPORTUNITY] - if there's a potential quest or mission available\n            \"\"\"\n            self.add_to_history(\"user\", prompt)\n            self.current_environment = self.ai.chat(self.conversation_history, provider='openai')\n        return self.current_environment\n</code></pre> <p>Here we instruct the AI to provide specific details, and we use tags for opportunities. We'll parse these tags <code>INTERACT_OPPORTUNITY</code> and <code>QUEST_OPPORTUNITY</code> later to perform other actions.</p> <p>We'll also store the environment description to avoid regenerating it unnecessarily.</p>"},{"location":"examples/client/ai_dungeon_master/#handling-player-actions","title":"Handling Player Actions","text":"<p>Now let's process the player's actions and generate outcomes. We'll run this one locally with ollama.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def handle_player_action(self, action):\n        prompt = f\"\"\"\n        The player ({self.game_state.character.name}, a {self.game_state.character.race} {self.game_state.character.class_type}) \n        attempts to {action} in {self.game_state.location}. \n        Describe the immediate result of this action, focusing on the environment and NPCs' reactions.\n        Do not generate any further actions or dialogue for {self.game_state.character.name}.\n        If the player is trying to interact with an NPC, end your response with [NPC_INTERACTION: &lt;npc_name&gt;].\n        \"\"\"\n        self.add_to_history(\"user\", prompt)\n        return self.ai.chat(self.conversation_history, provider='ollama')\n</code></pre> <p>Here we pass what the player wants to do to the AI and generate the outcomes for the players actions. We are also using a tag here for interactions, so we can process those in a different way.</p>"},{"location":"examples/client/ai_dungeon_master/#generating-npc-dialogue","title":"Generating NPC Dialogue","text":"<p>Next, let's create a function to generate a dialogue with an npc. We'll use replicate with llama3 8b for this.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def generate_npc_dialogue(self, npc_name: str, player_input: str):\n        prompt = f\"\"\"\n        The player ({self.game_state.character.name}) said to {npc_name}: \"{player_input}\"\n        Generate a single, natural response from {npc_name}, addressing the player's input directly.\n        If the player is asking about items for sale, list 2-3 specific items with brief descriptions and prices.\n        Do not include any actions or responses from the player character.\n        Keep the response concise and relevant to the player's input.\n        Do not include any formatting tags, headers, or quotation marks in your response.\n        Respond as if you are {npc_name} speaking directly to the player.\n        \"\"\"\n        self.add_to_history(\"user\", prompt)\n        return self.ai.chat(self.conversation_history, provider='replicate')\n</code></pre> <p>Note that in the prompt we ensure the AI provides responses that are in character and appropriate, so we can pass this directly to the player.</p>"},{"location":"examples/client/ai_dungeon_master/#handling-conversations","title":"Handling Conversations","text":"<p>We manage conversations with NPCs in a separate method. We start with a conversation loop, to allow the player to have a back-and-forth dialogue with an NPC, and we reset the conversation history to focus the AI on the dialogue.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def handle_conversation(self, npc_name):\n        print(f\"\\nYou are now in conversation with {npc_name}.\")\n        self.conversation_history = [\n            {\"role\": \"system\", \"content\": f\"You are {npc_name}, speaking directly to the player. Respond naturally and in character.\"}\n        ]\n        while True:\n            player_input = input(f\"\\nWhat do you say to {npc_name}? (or type 'end conversation' to stop): \")\n            if player_input.lower() == \"end conversation\":\n                print(f\"\\nYou end your conversation with {npc_name}.\")\n                break\n\n            print(f\"\\n{npc_name}:\")\n            self.print_stream(self.generate_npc_dialogue(npc_name, player_input))\n</code></pre> <p>We also add the possibility for the player to end the conversation at any time.</p>"},{"location":"examples/client/ai_dungeon_master/#updating-the-game-state","title":"Updating the Game State","text":"<p>We update the game state based on the outcomes provided by the AI.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def update_game_state(self, outcome):\n        if \"found\" in outcome.lower():\n            item = outcome.split(\"found\")[1].split(\".\")[0].strip()\n            self.game_state.update(item=item)\n        if \"new area\" in outcome.lower():\n            new_location = outcome.split(\"new area\")[1].split(\".\")[0].strip()\n            self.game_state.update(location=new_location)\n        if \"damage\" in outcome.lower():\n            self.game_state.update(health_change=-10)\n        if \"healed\" in outcome.lower():\n            self.game_state.update(health_change=10)\n        if \"quest\" in outcome.lower():\n            quest = outcome.split(\"quest\")[1].split(\".\")[0].strip()\n            self.game_state.update(quest=quest)\n        self.game_state.update(exp_gain=5)\n</code></pre> <p>This is a simpler way to do it, but we will just look for keywords in the AI's response to determine what changes to make. This isn't the most consistent way to do it, but is easy to do and will easily allow the game to respond to the player's actions, making the experience feel more dynamic.</p>"},{"location":"examples/client/ai_dungeon_master/#processing-story-elements","title":"Processing Story Elements","text":"<p>Let's process the AI-generated story to extract content and any special flags.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def process_story(self, story_generator) -&gt; Tuple[str, List[str]]:\n        story = self.print_stream(story_generator, print_output=True)\n        story_lines = story.split('\\n')\n\n        flags = []\n        for line in reversed(story_lines):\n            if line.strip().startswith('[') and line.strip().endswith(']'):\n                flags.append(line.strip('[').strip(']'))\n                story_lines.remove(line)\n            else:\n                break\n\n        story_content = '\\n'.join(story_lines).strip()\n\n        if any(flag.startswith(\"NPC_INTERACTION:\") for flag in flags):\n            npc_name = next(flag.split(':')[1].strip() for flag in flags if flag.startswith(\"NPC_INTERACTION:\"))\n            return story_content, npc_name\n        else:\n            return story_content, flags\n</code></pre> <p>Here is where we'll actually separates the special tags we defined earlier from the story content and ensure the player sees a coherent story without tags.</p>"},{"location":"examples/client/ai_dungeon_master/#printing-streamed-content","title":"Printing Streamed Content","text":"<p>We also don't want to wait until the whole content is generated to print, so let's define a function to display the AI's response in real-time, simulating typing.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def print_stream(self, stream, print_output=True) -&gt; str:\n        full_text = \"\"\n        for chunk in stream:\n            if print_output:\n                print(chunk, end='', flush=True)\n            full_text += chunk\n            time.sleep(0.03)\n        if print_output:\n            print()\n        return full_text\n</code></pre>"},{"location":"examples/client/ai_dungeon_master/#main-game-loop","title":"Main Game Loop","text":"<p>Finally, we bring everything together in the play_game method.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def play_game(self):\n        print(\"Welcome to the Dungeon!\")\n        character = self.create_character()\n        self.game_state = GameState(character)\n\n        print(\"\\nYour adventure begins...\")\n        while True:\n            print_separator()\n            environment_description, env_flags = self.process_story(self.generate_environment())\n\n            if \"INTERACT_OPPORTUNITY\" in env_flags:\n                print(\"\\nThere seems to be an opportunity to interact.\")\n            if \"QUEST_OPPORTUNITY\" in env_flags:\n                print(\"\\nThere might be a quest available.\")\n\n            action = input(\"\\nWhat do you do? \")\n            if action.lower() == \"quit\":\n                break\n\n            print(\"\\nOutcome:\")\n            outcome, npc_interaction = self.process_story(self.handle_player_action(action))\n\n            self.update_game_state(outcome)\n\n            if npc_interaction:\n                self.handle_conversation(npc_interaction)\n\n            print_separator()\n            print(f\"Current state: {str(self.game_state)}\")\n\n            if self.game_state.health &lt;= 0:\n                print(\"Game Over! Your health reached 0.\")\n                break\n\n            if hasattr(self, 'current_environment'):\n                del self.current_environment\n</code></pre> <p>The game loop continuously processes player actions and updates the game state, new environments are generated to keep the game dynamic and the player is allowed to quit whenever they want.</p> <p>Plus, the game is over if health reaches zero.</p>"},{"location":"examples/client/ai_dungeon_master/#helper-methods","title":"Helper Methods","text":"<p>Let's also create some methods for improved user experience, we want to separate content to make it easier to see and also create a print_slowly to simulate streamed content in important messages.</p> ai_dungeon_master/utils/text_utils.py<pre><code>import time\n\ndef print_separator(self):\n    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n\ndef print_slowly(text, delay=0.03):\n    for char in text:\n        print(char, end='', flush=True)\n        time.sleep(delay)\n    print()\n</code></pre>"},{"location":"examples/client/ai_dungeon_master/#6-main-script-that-runs-the-game","title":"6. Main Script that Runs the Game","text":"<p>At our main script, we initialize and start the game.</p> ai_dungeon_master/main.py<pre><code>from game.dungeon_master import EnhancedAIDungeonMaster\nfrom utils.text_utils import print_slowly\nfrom ai.ollama_server import start_ollama_server\n\ndef main():\n    print_slowly(\"Welcome to the AI Dungeon Master!\")\n    print_slowly(\"Prepare for an adventure guided by multiple AI models.\")\n    print_slowly(\"Type 'quit' at any time to exit the game.\")\n    print()\n\n    # Start the Ollama server before the game begins\n    ollama_process = start_ollama_server()\n\n    game = EnhancedAIDungeonMaster()\n    game.play_game()\n\n    print_slowly(\"Thank you for playing AI Dungeon Master!\")\n\n    # Terminate the Ollama server when the game ends\n    if ollama_process:\n        ollama_process.terminate()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/client/ai_dungeon_master/#7-running-the-game","title":"7. Running the Game","text":"<ol> <li> <p>Ensure you're in the root directory of the project.</p> </li> <li> <p>Run the game using Poetry:</p> </li> </ol> <pre><code>poetry run python ai_dungeon_master/main.py\n</code></pre> <p>Or directly if you used pip:</p> <pre><code>python ai_dungeon_master/main.py\n</code></pre> <p>This command will execute the <code>main.py</code> file, which should contain the game initialization and main loop.</p>"},{"location":"examples/client/ai_dungeon_master/#8-conclusion-and-further-improvements","title":"8. Conclusion and Further Improvements","text":"<p>Congratulations! You've now created an AI Dungeon Master using the ClientAI package. This project demonstrates how to integrate multiple AI providers and manage game logic to create a dynamic and engaging text-based RPG.</p>"},{"location":"examples/client/ai_dungeon_master/#potential-improvements","title":"Potential Improvements:","text":"<ol> <li>Error Handling: Implement try-except blocks to handle exceptions and improve robustness.</li> <li>Saving and Loading: Add functionality to save and load game states.</li> <li>Combat System: Develop a combat system that uses character stats and AI to determine outcomes.</li> <li>Quest Management: Create a more complex quest system with objectives and rewards.</li> <li>Multiplayer: Explore options for multiplayer interactions.</li> <li>User Interface: Develop a GUI for a more user-friendly experience.</li> <li>AI Fine-Tuning: Customize AI models for more consistent and relevant responses.</li> </ol>"},{"location":"examples/client/ai_dungeon_master/#8-conclusion-and-further-improvements_1","title":"8. Conclusion and Further Improvements","text":"<p>Congratulations! You've now created an AI Dungeon Master using the ClientAI package. This project demonstrates how to integrate multiple AI providers and manage game logic to create a dynamic and engaging text-based RPG.</p>"},{"location":"examples/client/ai_dungeon_master/#potential-improvements_1","title":"Potential Improvements:","text":"<ol> <li>Error Handling: Implement try-except blocks to handle exceptions and improve robustness.</li> <li>Saving and Loading: Add functionality to save and load game states.</li> <li>Combat System: Develop a combat system that uses character stats and AI to determine outcomes.</li> <li>Quest Management: Create a more complex quest system with objectives and rewards.</li> <li>Multiplayer: Explore options for multiplayer interactions.</li> <li>User Interface: Develop a GUI for a more user-friendly experience.</li> <li>AI Fine-Tuning: Customize AI models for more consistent and relevant responses.</li> </ol> <p>By implementing these improvements and exploring the agent-based approach, you can further enhance the gameplay experience and create an even more immersive and engaging AI-driven RPG.</p>"},{"location":"examples/client/simple_qa/","title":"ClientAI Tutorial: Building a Simple Q&amp;A Bot","text":"<p>In this tutorial, we'll create an interactive question-answering bot using the ClientAI package. Our bot will maintain conversation context, provide real-time responses, and demonstrate the core features of working with AI providers through a unified interface.</p>"},{"location":"examples/client/simple_qa/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Setting Up the Project</li> <li>Building the Q&amp;A Bot</li> <li>Creating the User Interface</li> <li>Running the Bot</li> <li>Further Improvements</li> </ol>"},{"location":"examples/client/simple_qa/#1-introduction","title":"1. Introduction","text":"<p>ClientAI makes it easy to work with various AI providers through a single, consistent interface. In this tutorial, we'll use it to build a Q&amp;A bot that does more than just ask and answer questions. Our bot will maintain conversation history, stream responses in real-time for a more engaging experience, and handle different AI providers seamlessly.</p> <p>The end result will be a practical bot that you can use for testing different AI providers, experimenting with conversation flows, or as a foundation for more complex chatbot applications.</p>"},{"location":"examples/client/simple_qa/#2-setting-up-the-project","title":"2. Setting Up the Project","text":"<p>Let's start by setting up our development environment. First, create a new directory for your project:</p> <pre><code>mkdir simple_qa_bot\ncd simple_qa_bot\n</code></pre> <p>Now we need to install ClientAI. We'll install it with support for OpenAI because that's what we'll use:</p> <pre><code>pip install clientai[openai]\n</code></pre> <p>Before we start coding, let's set up our API keys. Create a <code>.env</code> file in your project directory:</p> <pre><code>OPENAI_API_KEY=your_openai_api_key_here\n</code></pre> <p>This keeps our sensitive information separate from our code and makes it easy to switch between different development environments.</p>"},{"location":"examples/client/simple_qa/#3-building-the-qa-bot","title":"3. Building the Q&amp;A Bot","text":"<p>At the heart of our project is the SimpleQABot class. This class will handle all interactions with the AI provider and maintain our conversation state. Let's start building it:</p> <pre><code>from typing import Optional\nfrom clientai import ClientAI\nimport time\n\nclass SimpleQABot:\n    \"\"\"A basic question-answering bot that demonstrates core ClientAI functionality.\"\"\"\n\n    def __init__(self, provider: str = 'openai', api_key: Optional[str] = None):\n        \"\"\"Initialize the Q&amp;A bot with specified provider.\"\"\"\n        self.client = ClientAI(provider, api_key=api_key)\n        self.context = []\n</code></pre> <p>Our initialization is straightforward but flexible. We can specify which AI provider to use and optionally provide an API key. If no key is provided, ClientAI will look for it in environment variables.</p> <p>The conversation context is stored in a simple list. This allows our bot to remember previous interactions and maintain coherent conversations. Each interaction will be stored as a message with a role (either 'user' or 'assistant') and its content.</p> <p>Now let's add the core question-answering functionality:</p> <pre><code>def ask(self, question: str, stream: bool = False) -&gt; str:\n    \"\"\"Ask a question and get a response.\"\"\"\n    # Add the question to conversation context\n    self.context.append({\"role\": \"user\", \"content\": question})\n\n    # Get response from AI\n    response = self.client.chat(\n        messages=self.context,\n        model=\"gpt-3.5-turbo\",  # Default model\n        stream=stream\n    )\n\n    # Handle streaming vs non-streaming response\n    if stream:\n        full_response = \"\"\n        for chunk in response:\n            print(chunk, end=\"\", flush=True)\n            full_response += chunk\n            time.sleep(0.02)  # Small delay for readability\n        print()\n        final_response = full_response\n    else:\n        final_response = response\n\n    # Add response to context for future questions\n    self.context.append({\"role\": \"assistant\", \"content\": final_response})\n\n    return final_response\n</code></pre> <p>The <code>ask</code> method is where the magic happens. When a question is asked: 1. We add it to our conversation context 2. Send the entire context to the AI provider 3. Either stream the response word by word or return it all at once 4. Store the response in our context for future reference</p> <p>We've added a small delay when streaming responses. This makes the output more readable and gives the impression of the bot \"thinking\" as it responds.</p> <p>We also need a way to start fresh conversations:</p> <pre><code>def clear_context(self):\n    \"\"\"Clear the conversation history.\"\"\"\n    self.context = []\n</code></pre> <p>This simple method resets our conversation context, allowing users to start new conversations without previous context influencing the responses.</p>"},{"location":"examples/client/simple_qa/#4-creating-the-user-interface","title":"4. Creating the User Interface","text":"<p>Now let's create a simple but effective command-line interface for our bot. Create a main.py file:</p> <pre><code>def main():\n    # Initialize bot\n    bot = SimpleQABot('openai')\n\n    print(\"Simple Q&amp;A Bot (type 'quit' to exit, 'clear' to clear context)\")\n    print(\"Streaming mode is ON - watch the bot think!\\n\")\n\n    while True:\n        # Get user input\n        question = input(\"\\nYou: \")\n\n        # Handle commands\n        if question.lower() == 'quit':\n            break\n        elif question.lower() == 'clear':\n            bot.clear_context()\n            print(\"Context cleared!\")\n            continue\n\n        # Get and display response\n        print(\"\\nBot: \", end=\"\")\n        response = bot.ask(question, stream=True)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Our interface is straightforward but includes some nice features:</p> <ul> <li>Clear prompts for user input</li> <li>Command handling for quitting and clearing context</li> <li>Real-time response streaming</li> <li>Visual separation between user and bot messages</li> </ul>"},{"location":"examples/client/simple_qa/#5-running-the-bot","title":"5. Running the Bot","text":"<p>Using the bot is as simple as running the main script:</p> <pre><code>python main.py\n</code></pre> <p>Here's what you'll see:</p> <pre><code>Simple Q&amp;A Bot (type 'quit' to exit, 'clear' to clear context)\nStreaming mode is ON - watch the bot think!\n\nYou: What is Python?\nBot: Python is a high-level, interpreted programming language known for its \nsimple and readable syntax...\n\nYou: Why is it popular?\nBot: Python's popularity comes from several key factors...\n\nYou: clear\nContext cleared!\n\nYou: quit\n</code></pre>"},{"location":"examples/client/simple_qa/#6-further-improvements","title":"6. Further Improvements","text":"<p>While our Q&amp;A bot is already functional, there are many ways to enhance it. You could add error handling to gracefully manage API failures and rate limits. The context management could be expanded to include conversation saving and loading.</p> <p>The bot's interface could be improved with a web frontend, or you could add support for different response formats. You might also want to experiment with different AI providers and models to find the best fit for your needs.</p> <p>For a more robust implementation, consider using ClientAI's agent framework. This would give you access to:</p> <ul> <li>Automatic tool selection for complex tasks</li> <li>Structured workflow management</li> <li>Enhanced context handling</li> <li>Better error recovery</li> </ul> <p>The modular design of our bot makes it easy to add these improvements incrementally. You can start with the features that matter most to your use case and build from there.</p>"},{"location":"examples/client/translator/","title":"Building a Multi-Provider Translator with ClientAI","text":"<p>Let's build something interesting - a translator that can compare responses from different AI providers like OpenAI, Groq, and Replicate. We'll see how ClientAI makes it easy to work with multiple providers and compare their translations side by side.</p>"},{"location":"examples/client/translator/#getting-started","title":"Getting Started","text":"<p>First, we'll need ClientAI installed with support for all providers. Open your terminal and run:</p> <pre><code>pip install clientai[all]\n</code></pre> <p>You'll also need API keys for each provider. Set these up as environment variables:</p> <pre><code>export OPENAI_API_KEY=your_openai_api_key\nexport GROQ_API_KEY=your_groq_api_key\nexport REPLICATE_API_KEY=your_replicate_api_key\n</code></pre>"},{"location":"examples/client/translator/#building-the-translator","title":"Building the Translator","text":"<p>Let's start by importing what we need and setting up our environment:</p> <pre><code>from typing import Dict, List, Optional, Tuple\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\nimport os\nfrom clientai import ClientAI\n\n# Get our API keys from environment variables\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your_openai_api_key_here')\nGROQ_API_KEY = os.getenv('GROQ_API_KEY', 'your_groq_api_key_here')\nREPLICATE_API_KEY = os.getenv('REPLICATE_API_KEY', 'your_replicate_api_key_here')\n</code></pre> <p>Now let's create a simple class to hold our translation results. We'll use Python's dataclass feature to keep it clean and simple:</p> <pre><code>@dataclass\nclass Translation:\n    \"\"\"Represents a translation with metadata.\"\"\"\n    text: str\n    provider: str\n    confidence: float\n    model: str\n    time_taken: float\n</code></pre> <p>This Translation class will store not just the translated text, but also useful information about who did the translation, how confident they were, and how long it took. This will help us compare different providers.</p> <p>Now for the main translator class. Let's build it piece by piece:</p> <pre><code>class MultiProviderTranslator:\n    def __init__(self):\n        \"\"\"Initialize connections to different AI providers.\"\"\"\n        self.providers = {\n            'openai': ClientAI('openai', api_key=OPENAI_API_KEY),\n            'groq': ClientAI('groq', api_key=GROQ_API_KEY),\n            'replicate': ClientAI('replicate', api_key=REPLICATE_API_KEY),\n        }\n\n        self.default_models = {\n            'openai': 'gpt-4',\n            'groq': 'mixtral-8x7b-32768',\n            'replicate': 'meta/llama-2-70b-chat',\n        }\n\n        self.supported_languages = self._get_supported_languages()\n</code></pre> <p>In our initialization, we're setting up connections to each provider using ClientAI. We also define which models we want to use by default - GPT-4 for OpenAI, Mixtral for Groq, and Llama 2 for Replicate. Each has its own strengths, and we'll be able to compare them directly.</p> <p>Let's add the language support:</p> <pre><code>def _get_supported_languages(self) -&gt; Dict[str, str]:\n    \"\"\"Get dictionary of supported languages and their codes.\"\"\"\n    return {\n        'English': 'en',\n        'Spanish': 'es',\n        'French': 'fr',\n        'German': 'de',\n        'Italian': 'it',\n        'Portuguese': 'pt',\n        'Russian': 'ru',\n        'Japanese': 'ja',\n        'Chinese': 'zh',\n        'Korean': 'ko'\n    }\n</code></pre> <p>Next, we need to create clear instructions for our AI models. Here's how we'll format our translation requests:</p> <pre><code>def _create_translation_prompt(\n    self, \n    text: str, \n    source_lang: str, \n    target_lang: str\n) -&gt; str:\n    \"\"\"Create a clear prompt for translation.\"\"\"\n    return f\"\"\"\n    Translate the following text from {source_lang} to {target_lang}.\n    Provide only the direct translation without explanations or notes.\n\n    Text: {text}\n    \"\"\"\n</code></pre> <p>Now for the interesting part - getting translations from each provider. We want to handle errors gracefully and track how long each translation takes:</p> <pre><code>def _get_provider_translation(\n    self,\n    provider: str,\n    text: str,\n    source_lang: str,\n    target_lang: str,\n    model: Optional[str] = None\n) -&gt; Translation:\n    \"\"\"Get translation from a specific provider.\"\"\"\n    start_time = time.time()\n\n    model = model or self.default_models[provider]\n    prompt = self._create_translation_prompt(text, source_lang, target_lang)\n\n    try:\n        response = self.providers[provider].chat(\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            model=model,\n            temperature=0.3  # Lower temperature for more consistent translations\n        )\n\n        time_taken = time.time() - start_time\n        confidence = 0.95 if len(response.strip()) &gt; 0 else 0.0\n\n        return Translation(\n            text=response.strip(),\n            provider=provider,\n            confidence=confidence,\n            model=model,\n            time_taken=time_taken\n        )\n    except Exception as e:\n        print(f\"Error with {provider}: {str(e)}\")\n        return Translation(\n            text=f\"Error: {str(e)}\",\n            provider=provider,\n            confidence=0.0,\n            model=model,\n            time_taken=time.time() - start_time\n        )\n</code></pre> <p>This method does the heavy lifting for each provider. We time how long the translation takes, use the provider's API through ClientAI, and package everything up in our Translation class. If anything goes wrong, we handle it gracefully and return an error message instead of crashing.</p> <p>To make our translator faster, we'll run all providers in parallel:</p> <pre><code>def translate(\n    self,\n    text: str,\n    source_lang: str,\n    target_lang: str,\n    providers: Optional[List[str]] = None,\n    models: Optional[Dict[str, str]] = None\n) -&gt; List[Translation]:\n    \"\"\"Translate text using multiple providers in parallel.\"\"\"\n    if source_lang not in self.supported_languages.values():\n        raise ValueError(f\"Source language '{source_lang}' not supported\")\n    if target_lang not in self.supported_languages.values():\n        raise ValueError(f\"Target language '{target_lang}' not supported\")\n\n    providers = providers or list(self.providers.keys())\n    models = models or {}\n\n    translations = []\n    with ThreadPoolExecutor() as executor:\n        future_to_provider = {\n            executor.submit(\n                self._get_provider_translation,\n                provider,\n                text,\n                source_lang,\n                target_lang,\n                models.get(provider)\n            ): provider\n            for provider in providers\n        }\n\n        for future in as_completed(future_to_provider):\n            try:\n                translation = future.result()\n                translations.append(translation)\n            except Exception as e:\n                provider = future_to_provider[future]\n                print(f\"Error getting translation from {provider}: {str(e)}\")\n\n    translations.sort(key=lambda x: x.confidence, reverse=True)\n    return translations\n</code></pre> <p>This translate method is where everything comes together. It validates the languages, sets up parallel processing for all providers, and collects the results as they come in. We sort the translations by confidence score so the best ones appear first.</p> <p>Finally, let's add some nice user interface touches:</p> <pre><code>def print_slowly(text: str, delay: float = 0.03):\n    \"\"\"Print text with a slight delay between characters.\"\"\"\n    for char in text:\n        print(char, end='', flush=True)\n        time.sleep(delay)\n    print()\n\ndef display_translations(translations: List[Translation]):\n    \"\"\"Display translations in a formatted way.\"\"\"\n    print(\"\\nTranslations:\")\n    print(\"-\" * 50)\n    for t in translations:\n        print(f\"\\nProvider: {t.provider} ({t.model})\")\n        print(f\"Translation: {t.text}\")\n        print(f\"Confidence: {t.confidence:.2f}\")\n        print(f\"Time taken: {t.time_taken:.2f}s\")\n        print(\"-\" * 50)\n</code></pre> <p>These functions make our output more readable and add a nice typewriter effect for progress messages.</p>"},{"location":"examples/client/translator/#using-the-translator","title":"Using the Translator","text":"<p>Save all this code in a file called <code>translator.py</code>. Here's how to use it:</p> <pre><code>def main():\n    print_slowly(\"Initializing Multi-Provider Translator...\")\n    translator = MultiProviderTranslator()\n\n    print(\"\\nAvailable languages:\")\n    for name, code in translator.supported_languages.items():\n        print(f\"  {name}: {code}\")\n\n    while True:\n        try:\n            print(\"\\n\" + \"=\" * 50)\n            text = input(\"\\nEnter text to translate (or 'quit' to exit): \")\n            if text.lower() == 'quit':\n                break\n\n            source_lang = input(\"Enter source language code: \").lower()\n            target_lang = input(\"Enter target language code: \").lower()\n\n            print_slowly(\"\\nGetting translations from all providers...\")\n\n            translations = translator.translate(\n                text=text,\n                source_lang=source_lang,\n                target_lang=target_lang\n            )\n\n            display_translations(translations)\n\n        except ValueError as e:\n            print(f\"\\nError: {str(e)}\")\n        except Exception as e:\n            print(f\"\\nAn error occurred: {str(e)}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run it with:</p> <pre><code>python translator.py\n</code></pre>"},{"location":"examples/client/translator/#making-it-better","title":"Making It Better","text":"<p>This translator can be enhanced in many ways. You might want to add language detection, so users don't have to specify the source language. You could add translation memory to cache common translations, or create a web interface instead of using the command line.</p> <p>You could also make it smarter about choosing providers - maybe use cheaper ones for simple translations and reserve the more powerful models for complex text. Or add specialized vocabulary handling for technical translations.</p>"},{"location":"learn/overview/","title":"Learning Guide Overview","text":"<p>Welcome to the ClientAI Learning Guide! This comprehensive resource is designed to teach you the fundamentals of Large Language Models (LLMs), AI agents, Retrieval-Augmented Generation (RAG), and Machine Learning Engineering using practical examples with ClientAI.</p>"},{"location":"learn/overview/#under-construction","title":"\ud83d\udea7 Under Construction","text":"<p>This learning guide is currently under active development. While we're working hard to create comprehensive content, some sections may be incomplete or pending. We appreciate your patience and encourage you to check back regularly for updates.</p>"},{"location":"learn/overview/#what-youll-learn","title":"What You'll Learn","text":"<p>This guide will take you through:</p> <ul> <li>LLM Fundamentals: Understanding how large language models work, their capabilities, and how to effectively use them through ClientAI</li> <li>Agent Development: Learning to build intelligent AI agents that can reason, use tools, and solve complex tasks</li> <li>Retrieval Systems: Implementing RAG systems to give your applications access to custom knowledge</li> <li>Production Engineering: Deploying and managing LLM applications in production environments</li> </ul> <p>Tip</p> <p>Looking for Quick Start Instructions? This Learning Guide focuses on building a comprehensive understanding of theoretical concepts and practices, you might want to check our Usage Guide to get up and running with ClientAI.</p>"},{"location":"learn/overview/#prerequisites","title":"Prerequisites","text":"<p>To get the most out of this guide, you should have:</p> <ul> <li>Basic Python programming knowledge</li> <li>ClientAI installed in your environment</li> <li>Basic understanding of machine learning concepts (helpful but not required)</li> </ul>"},{"location":"learn/overview/#complete-curriculum","title":"Complete Curriculum","text":""},{"location":"learn/overview/#fundamentals","title":"Fundamentals","text":""},{"location":"learn/overview/#llm-basics-coming-soon","title":"LLM Basics <code>\ud83d\udea7 Coming Soon</code>","text":"<ul> <li>Learn the fundamentals of Large Language Models, including their architecture, capabilities, and key concepts through hands-on practice with ClientAI.</li> </ul>"},{"location":"learn/overview/#prompt-engineering-coming-soon","title":"Prompt Engineering <code>\ud83d\udea7 Coming Soon</code>","text":"<ul> <li>Master effective prompt design techniques, from basic principles to advanced templates, with practical ClientAI implementation examples.</li> </ul>"},{"location":"learn/overview/#working-with-agents","title":"Working with Agents","text":""},{"location":"learn/overview/#agent-fundamentals-coming-soon","title":"Agent Fundamentals <code>\ud83d\udea7 Coming Soon</code>","text":"<ul> <li>Understand AI agents and their architectures while building your first reasoning agents using ClientAI's framework.</li> </ul>"},{"location":"learn/overview/#tools-and-reasoning-coming-soon","title":"Tools and Reasoning <code>\ud83d\udea7 Coming Soon</code>","text":"<ul> <li>Create sophisticated tool-using agents with automated selection and custom tools for complex problem-solving tasks.</li> </ul>"},{"location":"learn/overview/#retrieval-and-knowledge","title":"Retrieval and Knowledge","text":""},{"location":"learn/overview/#introduction-to-rag-coming-soon","title":"Introduction to RAG <code>\ud83d\udea7 Coming Soon</code>","text":"<ul> <li>Explore the basics of Retrieval-Augmented Generation and build your first RAG system with vector databases and embeddings.</li> </ul>"},{"location":"learn/overview/#advanced-rag-coming-soon","title":"Advanced RAG <code>\ud83d\udea7 Coming Soon</code>","text":"<ul> <li>Master advanced retrieval strategies and optimization techniques for building sophisticated document processing systems.</li> </ul>"},{"location":"learn/overview/#mlops-and-engineering","title":"MLOps and Engineering","text":""},{"location":"learn/overview/#deployment-patterns-coming-soon","title":"Deployment Patterns <code>\ud83d\udea7 Coming Soon</code>","text":"<ul> <li>Learn essential deployment architectures and strategies for scaling LLM applications in production environments.</li> </ul>"},{"location":"learn/overview/#best-practices-coming-soon","title":"Best Practices <code>\ud83d\udea7 Coming Soon</code>","text":"<ul> <li>Master production-grade practices for testing, security, and optimization of LLM applications.</li> </ul>"},{"location":"learn/overview/#applied-projects","title":"Applied Projects","text":""},{"location":"learn/overview/#building-a-production-chatbot-coming-soon","title":"Building a Production Chatbot <code>\ud83d\udea7 Coming Soon</code>","text":"<ul> <li>Build a complete, production-ready chatbot from design to deployment with advanced context management.</li> </ul>"},{"location":"learn/overview/#document-assistant-coming-soon","title":"Document Assistant <code>\ud83d\udea7 Coming Soon</code>","text":"<ul> <li>Create a comprehensive document Q&amp;A system using RAG, with optimized retrieval and tool integration.</li> </ul>"},{"location":"learn/overview/#how-to-use-this-guide","title":"How to Use This Guide","text":"<p>The guide is structured progressively, building from fundamental concepts to advanced implementations. To better learn:</p> <ol> <li>Read the theoretical explanations and see how they translate to code</li> <li>Run the practical examples, experiment with them, and modify parameters</li> <li>Consult the Usage Guide or API documentation to understand features in depth</li> <li>Review the Examples for additional learning material</li> <li>Create your own projects! Building and breaking things is the best way to learn</li> </ol>"},{"location":"learn/overview/#contributing","title":"Contributing","text":"<p>This guide is open to community contributions! If you'd like to help improve or expand the learning materials:</p> <ul> <li>Check our Contributing Guidelines</li> <li>Submit issues or pull requests for improvements</li> <li>Share your feedback and suggestions with the community</li> </ul> <p>You'll soon be able to tart your learning journey by heading to the LLM Basics section!</p>"},{"location":"usage/ollama_manager/","title":"Ollama Manager Guide","text":""},{"location":"usage/ollama_manager/#introduction","title":"Introduction","text":"<p>Ollama Manager provides a streamlined way to prototype and develop applications using Ollama's AI models. Instead of manually managing the Ollama server process, installing it as a service, or running it in a separate terminal, Ollama Manager handles the entire lifecycle programmatically.</p> <p>Key Benefits for Prototyping:</p> <ul> <li>Start/stop Ollama server automatically within your Python code</li> <li>Configure resources dynamically based on your needs</li> <li>Handle multiple server instances for testing</li> <li>Automatic cleanup of resources</li> <li>Platform-independent operation</li> </ul>"},{"location":"usage/ollama_manager/#quick-start","title":"Quick Start","text":"<pre><code>from clientai import ClientAI\nfrom clientai.ollama import OllamaManager\n\n# Basic usage - server starts automatically and stops when done\nwith OllamaManager() as manager:\n    # Create a client that connects to the managed server\n    client = ClientAI('ollama', host=\"http://localhost:11434\")\n\n    # Use the client normally\n    response = client.generate_text(\n        \"Explain quantum computing\",\n        model=\"llama2\"\n    )\n    print(response)\n# Server automatically stops when exiting the context\n</code></pre>"},{"location":"usage/ollama_manager/#installation","title":"Installation","text":"<pre><code># Install with Ollama support\npip install \"clientai[ollama]\"\n\n# Install with all providers\npip install \"clientai[all]\"\n</code></pre>"},{"location":"usage/ollama_manager/#core-concepts","title":"Core Concepts","text":""},{"location":"usage/ollama_manager/#server-lifecycle-management","title":"Server Lifecycle Management","text":"<ol> <li> <p>Context Manager (Recommended) <pre><code>with OllamaManager() as manager:\n    # Server starts automatically\n    client = ClientAI('ollama')\n    # Use client...\n# Server stops automatically\n</code></pre></p> </li> <li> <p>Manual Management <pre><code>manager = OllamaManager()\ntry:\n    manager.start()\n    client = ClientAI('ollama')\n    # Use client...\nfinally:\n    manager.stop()\n</code></pre></p> </li> </ol>"},{"location":"usage/ollama_manager/#configuration-management","title":"Configuration Management","text":"<pre><code>from clientai.ollama import OllamaServerConfig\n\n# Create custom configuration\nconfig = OllamaServerConfig(\n    host=\"127.0.0.1\",\n    port=11434,\n    gpu_layers=35,\n    memory_limit=\"8GiB\"\n)\n\n# Use configuration with manager\nwith OllamaManager(config) as manager:\n    client = ClientAI('ollama')\n    # Use client...\n</code></pre>"},{"location":"usage/ollama_manager/#resource-configuration-guide","title":"Resource Configuration Guide","text":""},{"location":"usage/ollama_manager/#gpu-configuration","title":"GPU Configuration","text":"<ol> <li> <p>Basic GPU Setup <pre><code>config = OllamaServerConfig(\n    gpu_layers=35,  # Number of layers to run on GPU\n    gpu_memory_fraction=0.8  # 80% of GPU memory\n)\n</code></pre></p> </li> <li> <p>Multi-GPU Setup <pre><code>config = OllamaServerConfig(\n    gpu_devices=[0, 1],  # Use first two GPUs\n    gpu_memory_fraction=0.7\n)\n</code></pre></p> </li> </ol>"},{"location":"usage/ollama_manager/#memory-management","title":"Memory Management","text":"<p>The <code>memory_limit</code> parameter requires specific formatting following Go's memory limit syntax:</p> <pre><code># Correct memory limit formats\nconfig = OllamaServerConfig(\n    memory_limit=\"8GiB\"    # 8 gibibytes\n    # OR\n    memory_limit=\"8192MiB\" # Same as 8GiB\n)\n\n# Invalid formats that will cause errors\nconfig = OllamaServerConfig(\n    memory_limit=\"8GiB\"     # Wrong unit\n    memory_limit=\"8 GiB\"   # No spaces allowed\n    memory_limit=\"8g\"      # Must specify full unit\n)\n</code></pre> <p>Valid Memory Units: - <code>B</code> - Bytes - <code>KiB</code> - Kibibytes (1024 bytes) - <code>MiB</code> - Mebibytes (1024\u00b2 bytes) - <code>GiB</code> - Gibibytes (1024\u00b3 bytes) - <code>TiB</code> - Tebibytes (1024\u2074 bytes)</p> <p>Common Configurations:</p> <ol> <li> <p>High-Performance Setup <pre><code>config = OllamaServerConfig(\n    memory_limit=\"24GiB\",\n    cpu_threads=16,\n    gpu_layers=40\n)\n</code></pre></p> </li> <li> <p>Balanced Setup <pre><code>config = OllamaServerConfig(\n    memory_limit=\"16GiB\",\n    cpu_threads=8,\n    gpu_layers=32\n)\n</code></pre></p> </li> <li> <p>Resource-Constrained Setup <pre><code>config = OllamaServerConfig(\n    memory_limit=\"8GiB\",\n    cpu_threads=4,\n    gpu_layers=24\n)\n</code></pre></p> </li> <li> <p>Dynamic Memory Allocation <pre><code>import psutil\n\n# Calculate available memory and use 70%\navailable_gib = (psutil.virtual_memory().available / (1024**3))\nmemory_limit = f\"{int(available_gib * 0.7)}GiB\"\n\nconfig = OllamaServerConfig(\n    memory_limit=memory_limit,\n    cpu_threads=8\n)\n</code></pre></p> </li> </ol>"},{"location":"usage/ollama_manager/#model-specific-memory-guidelines","title":"Model-Specific Memory Guidelines","text":"<p>Different models require different amounts of memory:</p> <ol> <li> <p>Large Language Models (&gt;30B parameters) <pre><code>config = OllamaServerConfig(\n    memory_limit=\"24GiB\",\n    gpu_memory_fraction=0.9\n)\n</code></pre></p> </li> <li> <p>Medium Models (7B-30B parameters) <pre><code>config = OllamaServerConfig(\n    memory_limit=\"16GiB\",\n    gpu_memory_fraction=0.8\n)\n</code></pre></p> </li> <li> <p>Small Models (&lt;7B parameters) <pre><code>config = OllamaServerConfig(\n    memory_limit=\"8GiB\",\n    gpu_memory_fraction=0.7\n)\n</code></pre></p> </li> </ol>"},{"location":"usage/ollama_manager/#advanced-configuration-reference","title":"Advanced Configuration Reference","text":"<pre><code>config = OllamaServerConfig(\n    # Server settings\n    host=\"127.0.0.1\",      # Server bind address\n    port=11434,            # Server port number\n    timeout=30,            # Maximum startup wait time in seconds\n    check_interval=1.0,    # Health check interval in seconds\n\n    # GPU settings\n    gpu_layers=35,          # More layers = more GPU utilization\n    gpu_memory_fraction=0.8, # 0.0 to 1.0 (80% GPU memory)\n    gpu_devices=[0],        # Specific GPU devices to use\n\n    # CPU settings\n    cpu_threads=8,          # Number of CPU threads\n    memory_limit=\"16GiB\",   # Maximum RAM usage (must use GiB/MiB units)\n\n    # Compute settings\n    compute_unit=\"auto\",    # \"auto\", \"cpu\", or \"gpu\"\n\n    # Additional settings\n    env_vars={\"CUSTOM_VAR\": \"value\"},  # Additional environment variables\n    extra_args=[\"--verbose\"]           # Additional command line arguments\n)\n</code></pre> <p>Each setting explained:</p> <p>Server Settings:</p> <ul> <li> <p><code>host</code>: IP address to bind the server to</p> <ul> <li>Default: \"127.0.0.1\" (localhost)</li> <li>Use \"0.0.0.0\" to allow external connections</li> </ul> </li> <li> <p><code>port</code>: Port number for the server</p> <ul> <li>Default: 11434</li> <li>Change if default port is in use</li> </ul> </li> <li> <p><code>timeout</code>: Maximum time to wait for server startup</p> <ul> <li>Unit: seconds</li> <li>Increase for slower systems</li> </ul> </li> <li> <p><code>check_interval</code>: Time between server health checks</p> <ul> <li>Unit: seconds</li> <li>Adjust based on system responsiveness</li> </ul> </li> </ul> <p>GPU Settings:</p> <ul> <li> <p><code>gpu_layers</code>: Number of model layers to offload to GPU</p> <ul> <li>Higher = more GPU utilization</li> <li>Lower = more CPU utilization</li> <li>Model-dependent (typically 24-40)</li> </ul> </li> <li> <p><code>gpu_memory_fraction</code>: Portion of GPU memory to use</p> <ul> <li>Range: 0.0 to 1.0</li> <li>Higher values may improve performance</li> <li>Lower values leave room for other applications</li> </ul> </li> <li> <p><code>gpu_devices</code>: Specific GPU devices to use</p> <ul> <li>Single GPU: <code>gpu_devices=0</code></li> <li>Multiple GPUs: <code>gpu_devices=[0, 1]</code></li> <li>None: <code>gpu_devices=None</code></li> </ul> </li> </ul> <p>CPU Settings:</p> <ul> <li> <p><code>cpu_threads</code>: Number of CPU threads to use</p> <ul> <li>Default: System dependent</li> <li>Recommended: Leave some threads for system</li> <li>Example: <code>os.cpu_count() - 2</code></li> </ul> </li> <li> <p><code>memory_limit</code>: Maximum RAM allocation</p> <ul> <li>Must use <code>GiB</code> or <code>MiB</code> units</li> <li>Examples: \"8GiB\", \"16384MiB\"</li> <li>Should not exceed available system RAM</li> </ul> </li> </ul> <p>Compute Settings:</p> <ul> <li><code>compute_unit</code>: Preferred compute device<ul> <li>\"auto\": Let Ollama decide (recommended)</li> <li>\"cpu\": Force CPU-only operation</li> <li>\"gpu\": Force GPU operation if available</li> </ul> </li> </ul> <p>Additional Settings:</p> <ul> <li> <p><code>env_vars</code>: Additional environment variables</p> <ul> <li>Used for platform-specific settings</li> <li>Example: <code>{\"CUDA_VISIBLE_DEVICES\": \"0,1\"}</code></li> </ul> </li> <li> <p><code>extra_args</code>: Additional CLI arguments</p> <ul> <li>Passed directly to Ollama server</li> <li>Example: <code>[\"--verbose\", \"--debug\"]</code></li> </ul> </li> </ul>"},{"location":"usage/ollama_manager/#common-use-cases","title":"Common Use Cases","text":""},{"location":"usage/ollama_manager/#1-development-and-prototyping","title":"1. Development and Prototyping","text":"<pre><code># Quick setup for development\nwith OllamaManager() as manager:\n    client = ClientAI('ollama')\n\n    # Test different prompts\n    prompts = [\n        \"Write a poem about AI\",\n        \"Explain quantum physics\",\n        \"Create a Python function\"\n    ]\n\n    for prompt in prompts:\n        response = client.generate_text(prompt, model=\"llama2\")\n        print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n</code></pre>"},{"location":"usage/ollama_manager/#2-multiple-model-testing","title":"2. Multiple Model Testing","text":"<pre><code># Test different models with different configurations\nmodels = [\"llama2\", \"codellama\", \"mistral\"]\n\nfor model in models:\n    # Adjust configuration based on model\n    if model == \"llama2\":\n        config = OllamaServerConfig(gpu_layers=35)\n    else:\n        config = OllamaServerConfig(gpu_layers=28)\n\n    with OllamaManager(config) as manager:\n        client = ClientAI('ollama')\n        response = client.generate_text(\n            \"Explain how you work\",\n            model=model\n        )\n        print(f\"{model}: {response}\\n\")\n</code></pre>"},{"location":"usage/ollama_manager/#3-ab-testing-configurations","title":"3. A/B Testing Configurations","text":"<pre><code>def test_configuration(config, prompt, model):\n    start_time = time.time()\n\n    with OllamaManager(config) as manager:\n        client = ClientAI('ollama')\n        response = client.generate_text(prompt, model=model)\n\n    duration = time.time() - start_time\n    return response, duration\n\n# Test different configurations\nconfigs = {\n    \"high_gpu\": OllamaServerConfig(gpu_layers=40, gpu_memory_fraction=0.9),\n    \"balanced\": OllamaServerConfig(gpu_layers=32, gpu_memory_fraction=0.7),\n    \"low_resource\": OllamaServerConfig(gpu_layers=24, gpu_memory_fraction=0.5)\n}\n\nfor name, config in configs.items():\n    response, duration = test_configuration(\n        config,\n        \"Write a long story about space\",\n        \"llama2\"\n    )\n    print(f\"Configuration {name}: {duration:.2f} seconds\")\n</code></pre>"},{"location":"usage/ollama_manager/#4-production-setup","title":"4. Production Setup","text":"<pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef create_production_manager():\n    config = OllamaServerConfig(\n        # Stable production settings\n        gpu_layers=32,\n        gpu_memory_fraction=0.7,\n        memory_limit=\"16GiB\",\n        timeout=60,  # Longer timeout for stability\n        check_interval=2.0\n    )\n\n    try:\n        manager = OllamaManager(config)\n        manager.start()\n        return manager\n    except Exception as e:\n        logging.error(f\"Failed to start Ollama: {e}\")\n        raise\n\n# Use in production\ntry:\n    manager = create_production_manager()\n    client = ClientAI('ollama')\n    # Use client...\nfinally:\n    manager.stop()\n</code></pre>"},{"location":"usage/ollama_manager/#error-handling","title":"Error Handling","text":"<p>Ollama Manager provides several specific exception types to help you handle different error scenarios effectively:</p>"},{"location":"usage/ollama_manager/#executablenotfounderror","title":"ExecutableNotFoundError","text":"<p>Occurs when the Ollama executable cannot be found on the system.</p> <p>Common causes: - Ollama not installed - Ollama not in system PATH - Incorrect installation</p> <p>How to handle: <pre><code>try:\n    manager = OllamaManager()\n    manager.start()\nexcept ExecutableNotFoundError:\n    # Guide user through installation\n    if platform.system() == \"Darwin\":\n        print(\"Install Ollama using: brew install ollama\")\n    elif platform.system() == \"Linux\":\n        print(\"Install Ollama using: curl -fsSL https://ollama.com/install.sh | sh\")\n    else:\n        print(\"Download Ollama from: https://ollama.com/download\")\n</code></pre></p>"},{"location":"usage/ollama_manager/#serverstartuperror","title":"ServerStartupError","text":"<p>Occurs when the server fails to start properly.</p> <p>Common causes: - Port already in use - Insufficient permissions - Corrupt installation - Resource conflicts</p> <p>How to handle: <pre><code>try:\n    manager = OllamaManager()\n    manager.start()\nexcept ServerStartupError as e:\n    if \"address already in use\" in str(e).lower():\n        # Try alternative port\n        config = OllamaServerConfig(port=11435)\n        manager = OllamaManager(config)\n        manager.start()\n    elif \"permission denied\" in str(e).lower():\n        print(\"Please run with appropriate permissions\")\n    else:\n        print(f\"Server startup failed: {e}\")\n</code></pre></p>"},{"location":"usage/ollama_manager/#servertimeouterror","title":"ServerTimeoutError","text":"<p>Occurs when the server doesn't respond within the configured timeout period.</p> <p>Common causes: - System under heavy load - Insufficient resources - Network issues - Too short timeout period</p> <p>How to handle: <pre><code>config = OllamaServerConfig(\n    timeout=60,  # Increase timeout\n    check_interval=2.0  # Reduce check frequency\n)\ntry:\n    manager = OllamaManager(config)\n    manager.start()\nexcept ServerTimeoutError:\n    # Either retry with longer timeout or fail gracefully\n    config.timeout = 120\n    try:\n        manager = OllamaManager(config)\n        manager.start()\n    except ServerTimeoutError:\n        print(\"Server unresponsive after extended timeout\")\n</code></pre></p>"},{"location":"usage/ollama_manager/#resourceerror","title":"ResourceError","text":"<p>Occurs when there are insufficient system resources to run Ollama.</p> <p>Common causes: - Insufficient memory - GPU memory allocation failures - Too many CPU threads requested - Disk space constraints</p> <p>How to handle: <pre><code>try:\n    manager = OllamaManager()\n    manager.start()\nexcept ResourceError as e:\n    if \"memory\" in str(e).lower():\n        # Try with reduced memory settings\n        config = OllamaServerConfig(\n            memory_limit=\"4GiB\",\n            gpu_memory_fraction=0.5\n        )\n    elif \"gpu\" in str(e).lower():\n        # Fallback to CPU\n        config = OllamaServerConfig(compute_unit=\"cpu\")\n\n    try:\n        manager = OllamaManager(config)\n        manager.start()\n    except ResourceError:\n        print(\"Unable to allocate required resources\")\n</code></pre></p>"},{"location":"usage/ollama_manager/#configurationerror","title":"ConfigurationError","text":"<p>Occurs when provided configuration values are invalid.</p> <p>Common causes: - Invalid memory format - Invalid GPU configuration - Incompatible settings - Out-of-range values</p> <p>How to handle: <pre><code>try:\n    config = OllamaServerConfig(\n        gpu_memory_fraction=1.5  # Invalid value\n    )\nexcept ConfigurationError as e:\n    # Use safe default values\n    config = OllamaServerConfig(\n        gpu_memory_fraction=0.7,\n        gpu_layers=32\n    )\n</code></pre></p>"},{"location":"usage/ollama_manager/#unsupportedplatformerror","title":"UnsupportedPlatformError","text":"<p>Occurs when running on an unsupported platform or configuration.</p> <p>Common causes: - Unsupported operating system - Missing system features - Incompatible hardware</p> <p>How to handle: <pre><code>try:\n    manager = OllamaManager()\n    manager.start()\nexcept UnsupportedPlatformError as e:\n    # Fall back to alternative configuration or inform user\n    print(f\"Platform not supported: {e}\")\n    print(\"Supported platforms: Windows, macOS, Linux\")\n</code></pre></p>"},{"location":"usage/ollama_manager/#memory-related-error-handling","title":"Memory-Related Error Handling","text":"<pre><code>def start_with_memory_fallback():\n    try:\n        # Try optimal memory configuration\n        config = OllamaServerConfig(memory_limit=\"16GiB\")\n        return OllamaManager(config)\n    except ServerStartupError as e:\n        if \"GOMEMLIMIT\" in str(e):\n            # Fall back to lower memory configuration\n            config = OllamaServerConfig(memory_limit=\"8GiB\")\n            return OllamaManager(config)\n        raise  # Re-raise if error is not memory-related\n</code></pre>"},{"location":"usage/ollama_manager/#best-practices-for-error-handling","title":"Best Practices for Error Handling","text":"<ol> <li> <p>Graceful Degradation <pre><code>def start_with_fallback():\n    # Try optimal configuration first\n    config = OllamaServerConfig(\n        gpu_layers=35,\n        gpu_memory_fraction=0.8\n    )\n\n    try:\n        return OllamaManager(config)\n    except ResourceError:\n        # Fall back to minimal configuration\n        config = OllamaServerConfig(\n            gpu_layers=24,\n            gpu_memory_fraction=0.5\n        )\n        return OllamaManager(config)\n</code></pre></p> </li> <li> <p>Logging and Monitoring <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    manager = OllamaManager()\n    manager.start()\nexcept ServerStartupError as e:\n    logger.error(f\"Server startup failed: {e}\")\n    logger.debug(f\"Full error details: {e.original_exception}\")\n</code></pre></p> </li> <li> <p>Recovery Strategies <pre><code>def start_with_retry(max_attempts=3, delay=5):\n    for attempt in range(max_attempts):\n        try:\n            manager = OllamaManager()\n            manager.start()\n            return manager\n        except (ServerStartupError, ServerTimeoutError) as e:\n            if attempt == max_attempts - 1:\n                raise\n            logger.warning(f\"Attempt {attempt + 1} failed, retrying...\")\n            time.sleep(delay)\n</code></pre></p> </li> </ol> <p>Remember to always clean up resources properly, even when handling errors: <pre><code>manager = None\ntry:\n    manager = OllamaManager()\n    manager.start()\n    # Use manager...\nexcept Exception as e:\n    logger.error(f\"Error occurred: {e}\")\nfinally:\n    if manager is not None:\n        manager.stop()\n</code></pre></p>"},{"location":"usage/ollama_manager/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>import psutil\nfrom contextlib import contextmanager\nimport time\n\n@contextmanager\ndef monitor_performance():\n    start_time = time.time()\n    start_cpu = psutil.cpu_percent()\n    start_mem = psutil.virtual_memory().percent\n\n    yield\n\n    duration = time.time() - start_time\n    cpu_diff = psutil.cpu_percent() - start_cpu\n    mem_diff = psutil.virtual_memory().percent - start_mem\n\n    print(f\"Duration: {duration:.2f}s\")\n    print(f\"CPU impact: {cpu_diff:+.1f}%\")\n    print(f\"Memory impact: {mem_diff:+.1f}%\")\n\n# Use with Ollama Manager\nwith monitor_performance():\n    with OllamaManager() as manager:\n        client = ClientAI('ollama')\n        response = client.generate_text(\n            \"Write a story\",\n            model=\"llama2\"\n        )\n</code></pre>"},{"location":"usage/ollama_manager/#best-practices","title":"Best Practices","text":"<ol> <li>Always use context managers when possible</li> <li>Start with conservative resource settings and adjust up</li> <li>Monitor system resources during development</li> <li>Implement proper error handling</li> <li>Use appropriate configurations for development vs. production</li> <li>Clean up resources properly in all code paths</li> <li>Log important events for troubleshooting</li> <li>Test different configurations to find optimal settings</li> <li>Consider platform-specific settings for cross-platform applications</li> <li>Implement graceful degradation when resources are constrained</li> </ol>"},{"location":"usage/ollama_manager/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Server won't start: Check if Ollama is installed and port is available</li> <li>Performance issues: Monitor and adjust resource configuration</li> <li>Memory errors: Reduce memory_limit and gpu_memory_fraction</li> <li>GPU errors: Try reducing gpu_layers or switch to CPU</li> <li>Timeout errors: Increase timeout value in configuration</li> <li>Platform-specific issues: Check platform support and requirements</li> </ul> <p>Remember that Ollama Manager is a powerful tool for prototyping and development, but always monitor system resources and adjust configurations based on your specific needs and hardware capabilities.</p>"},{"location":"usage/overview/","title":"Usage Overview","text":"<p>This Usage section provides comprehensive guides on how to effectively use ClientAI's two main components: the Client for direct AI provider interactions and the Agent for building autonomous AI workflows. Each topic focuses on specific aspects, ensuring you have all the information needed to leverage the full potential of ClientAI in your projects.</p>"},{"location":"usage/overview/#client-features","title":"Client Features","text":"<p>Learn how to initialize and use ClientAI with different AI providers. These guides cover the fundamentals of direct AI interaction:</p> <ul> <li>Initialization Guide</li> <li>Text Generation Guide</li> <li>Chat Functionality Guide</li> <li>Multiple Providers Guide</li> <li>Ollama Manager Guide</li> </ul>"},{"location":"usage/overview/#agent-features","title":"Agent Features","text":"<p>Discover how to create and customize AI agents for autonomous workflows:</p> <ul> <li>Creating Agents Guide</li> <li>Workflow Steps Guide</li> <li>Tools and Tool Selection</li> <li>Context Management</li> </ul>"},{"location":"usage/overview/#getting-started","title":"Getting Started","text":""},{"location":"usage/overview/#quick-start-with-client","title":"Quick Start with Client","text":"<p>Here's a simple example using the basic client for direct AI interaction:</p> <pre><code>from clientai import ClientAI\n\n# Initialize the client\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Generate text\nresponse = client.generate_text(\n    \"Explain the concept of machine learning in simple terms.\",\n    model=\"gpt-3.5-turbo\"\n)\n\nprint(response)\n</code></pre>"},{"location":"usage/overview/#quick-start-with-agent","title":"Quick Start with Agent","text":"<p>Here's how to create a simple agent with tools:</p> <pre><code>from clientai import ClientAI, create_agent\n\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Create a calculator tool\n@tool(name=\"Calculator\", description=\"Performs basic math operations\")\ndef calculate(x: int, y: int) -&gt; int:\n    return x + y\n\n# Create an agent with the calculator tool\nagent = create_agent(\n    client=client,\n    role=\"math_helper\",\n    system_prompt=\"You are a helpful math assistant.\",\n    model=\"gpt-4\",\n    tools=[calculate]\n)\n\n# Run the agent\nresult = agent.run(\"What is 5 plus 3?\")\nprint(result)\n</code></pre>"},{"location":"usage/overview/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/overview/#streaming-with-client","title":"Streaming with Client","text":"<p>The client supports streaming responses:</p> <pre><code>for chunk in client.generate_text(\n    \"Tell me a story about space exploration\",\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"usage/overview/#multi-step-agent-workflows","title":"Multi-Step Agent Workflows","text":"<p>Create agents with multiple processing steps:</p> <pre><code>class AnalysisAgent(Agent):\n    @think(\"analyze\")\n    def analyze_data(self, input_data: str) -&gt; str:\n        return f\"Analyze this data: {input_data}\"\n\n    @act(\"process\")\n    def process_results(self, analysis: str) -&gt; str:\n        return f\"Based on the analysis: {analysis}\"\n\nagent = AnalysisAgent(\n    client=client,\n    default_model=\"gpt-4\",\n    tool_confidence=0.8\n)\n</code></pre>"},{"location":"usage/overview/#best-practices","title":"Best Practices","text":""},{"location":"usage/overview/#client-best-practices","title":"Client Best Practices","text":"<ol> <li>API Key Management: Store API keys securely as environment variables</li> <li>Error Handling: Implement proper error handling for API failures</li> <li>Model Selection: Choose models based on task requirements and budget</li> <li>Context Management: Manage conversation context efficiently</li> </ol>"},{"location":"usage/overview/#contribution","title":"Contribution","text":"<p>If you have suggestions or contributions to these guides, please refer to our Contributing Guidelines. We appreciate your input in improving our documentation and making ClientAI more accessible to all users.</p>"},{"location":"usage/agent/context/","title":"Context Management","text":"<p>Context management in ClientAI provides a structured way to maintain state and share information between workflow steps. The context system enables:</p> <ul> <li>State persistence across workflow steps</li> <li>Result tracking and access</li> <li>Data sharing between components</li> <li>Configuration management</li> <li>Error state tracking</li> </ul>"},{"location":"usage/agent/context/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites</li> <li>Understanding Context</li> <li>Working with Context</li> <li>Advanced Usage</li> <li>Best Practices</li> </ol>"},{"location":"usage/agent/context/#prerequisites","title":"Prerequisites","text":"<p>Before working with context, ensure you have:</p> <ol> <li> <p>Basic understanding of:</p> <ul> <li>ClientAI agent concepts</li> <li>Workflow steps</li> <li>Tool usage patterns</li> </ul> </li> <li> <p>Proper imports:     <pre><code>from clientai.agent import Agent\nfrom clientai.agent.context import AgentContext\n</code></pre></p> </li> </ol>"},{"location":"usage/agent/context/#understanding-context","title":"Understanding Context","text":"<p>The context system in ClientAI serves as a central state manager for agents, providing:</p>"},{"location":"usage/agent/context/#core-features","title":"Core Features","text":"<ol> <li> <p>State Management</p> <ul> <li>Persistent storage during workflow execution</li> <li>Scoped access to stored data</li> <li>Automatic cleanup of temporary data</li> </ul> </li> <li> <p>Result Tracking</p> <ul> <li>Storage of step execution results</li> <li>Access to historical results</li> <li>Tool execution outcome storage</li> </ul> </li> <li> <p>Configuration Access</p> <ul> <li>Agent configuration storage</li> <li>Step-specific settings</li> <li>Tool configuration management</li> </ul> </li> </ol>"},{"location":"usage/agent/context/#working-with-context","title":"Working with Context","text":""},{"location":"usage/agent/context/#basic-context-operations","title":"Basic Context Operations","text":""},{"location":"usage/agent/context/#1-accessing-step-results","title":"1. Accessing Step Results","text":"<pre><code>class MyAgent(Agent):\n    @think\n    def analyze(self, input_data: str) -&gt; str:\n        \"\"\"First step: analyze data.\"\"\"\n        return f\"Analyzing: {input_data}\"\n\n    @act\n    def process(self, prev_result: str) -&gt; str:\n        \"\"\"Second step: access results.\"\"\"\n        # Get specific step result\n        analysis = self.context.get_step_result(\"analyze\")\n\n        # Get all previous results\n        all_results = self.context.get_all_results()\n\n        return f\"Processing analysis: {analysis}\"\n</code></pre>"},{"location":"usage/agent/context/#2-managing-state","title":"2. Managing State","text":"<pre><code>class StateManagingAgent(Agent):\n    @think\n    def first_step(self, input_data: str) -&gt; str:\n        # Store data in context\n        self.context.set_state(\"important_data\", input_data)\n        return \"Processing step 1\"\n\n    @act\n    def second_step(self, prev_result: str) -&gt; str:\n        # Retrieve data from context\n        data = self.context.get_state(\"important_data\")\n\n        # Check if data exists\n        if self.context.has_state(\"important_data\"):\n            return f\"Found data: {data}\"\n\n        return \"No data found\"\n</code></pre>"},{"location":"usage/agent/context/#3-configuration-access","title":"3. Configuration Access","text":"<pre><code>class ConfigAwareAgent(Agent):\n    @think\n    def configured_step(self, input_data: str) -&gt; str:\n        # Access agent configuration\n        model = self.context.config.default_model\n\n        # Access step configuration\n        step_config = self.context.get_step_config(\"configured_step\")\n\n        return f\"Using model: {model}\"\n</code></pre>"},{"location":"usage/agent/context/#context-lifecycle","title":"Context Lifecycle","text":"<p>The context system maintains state throughout an agent's workflow execution:</p> <ol> <li> <p>Initialization <pre><code>class LifecycleAgent(Agent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Context is automatically initialized\n\n    @think\n    def first_step(self, input_data: str) -&gt; str:\n        # Context is ready for use\n        self.context.set_state(\"init_time\", time.time())\n        return \"Step 1\"\n</code></pre></p> </li> <li> <p>Step Execution <pre><code>@act\ndef execution_step(self, prev_result: str) -&gt; str:\n    # Results automatically stored\n    current_step = self.context.current_step\n    step_start = self.context.get_state(\"step_start_time\")\n\n    return \"Processing\"\n</code></pre></p> </li> <li> <p>Cleanup <pre><code>def cleanup(self):\n    \"\"\"Optional cleanup method.\"\"\"\n    # Clear temporary data\n    self.context.clear_state(\"temp_data\")\n\n    # Persist important data\n    final_state = self.context.get_all_state()\n    self.save_state(final_state)\n</code></pre></p> </li> </ol>"},{"location":"usage/agent/context/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/agent/context/#1-custom-context-extensions","title":"1. Custom Context Extensions","text":"<pre><code>from clientai.agent.context import AgentContext\n\nclass CustomContext(AgentContext):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._custom_storage = {}\n\n    def store_custom(self, key: str, value: Any) -&gt; None:\n        \"\"\"Store custom data with validation.\"\"\"\n        if not isinstance(key, str):\n            raise ValueError(\"Key must be string\")\n        self._custom_storage[key] = value\n\n    def get_custom(self, key: str) -&gt; Any:\n        \"\"\"Retrieve custom data.\"\"\"\n        return self._custom_storage.get(key)\n\nclass CustomAgent(Agent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.context = CustomContext()\n</code></pre>"},{"location":"usage/agent/context/#2-context-event-handling","title":"2. Context Event Handling","text":"<pre><code>class EventAwareAgent(Agent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.context.on_state_change(self._handle_state_change)\n        self.context.on_result_added(self._handle_new_result)\n\n    def _handle_state_change(self, key: str, value: Any) -&gt; None:\n        \"\"\"Handle state changes.\"\"\"\n        logger.info(f\"State changed: {key}\")\n\n    def _handle_new_result(self, step_name: str, result: Any) -&gt; None:\n        \"\"\"Handle new results.\"\"\"\n        logger.info(f\"New result from {step_name}\")\n</code></pre>"},{"location":"usage/agent/context/#3-context-serialization","title":"3. Context Serialization","text":"<pre><code>class PersistentAgent(Agent):\n    def save_context(self) -&gt; None:\n        \"\"\"Save context to storage.\"\"\"\n        state = self.context.serialize()\n        with open(\"agent_state.json\", \"w\") as f:\n            json.dump(state, f)\n\n    def load_context(self) -&gt; None:\n        \"\"\"Load context from storage.\"\"\"\n        with open(\"agent_state.json\", \"r\") as f:\n            state = json.load(f)\n        self.context.deserialize(state)\n</code></pre>"},{"location":"usage/agent/context/#best-practices","title":"Best Practices","text":""},{"location":"usage/agent/context/#1-state-management","title":"1. State Management","text":"<pre><code># Good Practice\nclass WellManagedAgent(Agent):\n    @think\n    def first_step(self, input_data: str) -&gt; str:\n        # Use clear, descriptive keys\n        self.context.set_state(\"analysis_start_time\", time.time())\n        self.context.set_state(\"raw_input\", input_data)\n\n        # Group related data\n        self.context.set_state(\"analysis\", {\n            \"input\": input_data,\n            \"timestamp\": time.time(),\n            \"status\": \"started\"\n        })\n\n        return \"Processing\"\n\n    @act\n    def cleanup_step(self, prev_result: str) -&gt; str:\n        # Clear temporary data\n        self.context.clear_state(\"analysis_start_time\")\n\n        # Keep important data\n        analysis = self.context.get_state(\"analysis\")\n        return f\"Completed analysis: {analysis}\"\n</code></pre>"},{"location":"usage/agent/context/#2-result-access-patterns","title":"2. Result Access Patterns","text":"<pre><code>class ResultPatternAgent(Agent):\n    @think\n    def access_results(self, input_data: str) -&gt; str:\n        # Prefer specific result access\n        last_result = self.context.get_step_result(\"specific_step\")\n\n        # Use get_all_results sparingly\n        all_results = self.context.get_all_results()\n\n        # Check result existence\n        if self.context.has_step_result(\"specific_step\"):\n            return \"Process result\"\n\n        return \"Handle missing result\"\n</code></pre>"},{"location":"usage/agent/context/#3-error-handling","title":"3. Error Handling","text":"<pre><code>class ErrorAwareAgent(Agent):\n    @think\n    def safe_step(self, input_data: str) -&gt; str:\n        try:\n            # Access state safely\n            data = self.context.get_state(\"key\", default=\"fallback\")\n\n            # Validate state before use\n            if not self._validate_state(data):\n                raise ValueError(\"Invalid state\")\n\n            return f\"Processing: {data}\"\n\n        except (KeyError, ValueError) as e:\n            logger.error(f\"Context error: {e}\")\n            self.context.set_state(\"error_state\", str(e))\n            raise\n</code></pre>"},{"location":"usage/agent/context/#4-performance-considerations","title":"4. Performance Considerations","text":"<ol> <li> <p>Clear unnecessary state:    <pre><code># Remove temporary data\nself.context.clear_state(\"temp_calculation\")\n\n# Clear multiple states\nself.context.clear_states([\"temp1\", \"temp2\"])\n</code></pre></p> </li> <li> <p>Use efficient access patterns:    <pre><code># Good: Direct access\nresult = self.context.get_step_result(\"specific_step\")\n\n# Avoid: Frequent full result access\nall_results = self.context.get_all_results()  # Use sparingly\n</code></pre></p> </li> <li> <p>Batch state updates:    <pre><code># Good: Batch update\nself.context.set_states({\n    \"key1\": \"value1\",\n    \"key2\": \"value2\",\n    \"key3\": \"value3\"\n})\n\n# Avoid: Multiple individual updates\nself.context.set_state(\"key1\", \"value1\")\nself.context.set_state(\"key2\", \"value2\")\nself.context.set_state(\"key3\", \"value3\")\n</code></pre></p> </li> </ol> <p>Now that you've mastered context management, check out our Examples section to see these concepts in action:</p> <ul> <li>How to build a simple Q&amp;A bot that introduces core agent features</li> <li>Techniques for creating a task planner using <code>create_agent</code> and basic tools</li> <li>Methods for implementing a writing assistant with multi-step workflows</li> <li>Patterns for developing a sophisticated code analyzer with custom workflows</li> </ul> <p>Each example demonstrates different aspects of ClientAI, from basic agent creation to complex systems combining steps, tools, and context management. Start with the Simple Q&amp;A Bot to see ClientAI's fundamentals in practice, or jump straight to the Code Analyzer for a more advanced implementation.</p>"},{"location":"usage/agent/creating_agents/","title":"Creating Agents","text":"<p>Agents in ClientAI are AI-powered entities that can execute tasks ranging from simple operations to complex multi-step workflows. An agent combines:</p> <ul> <li>A Large Language Model (LLM) for reasoning</li> <li>Optional tools for specific capabilities</li> <li>A workflow system for organizing tasks</li> <li>State management for tracking progress</li> </ul>"},{"location":"usage/agent/creating_agents/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites</li> <li>Understanding Agent Types</li> <li>Simple Single-Step Agents</li> <li>Multi-Step Agents</li> <li>Agent Configuration</li> <li>Error Handling</li> <li>Best Practices</li> </ol>"},{"location":"usage/agent/creating_agents/#prerequisites","title":"Prerequisites","text":"<p>Before creating agents, ensure you have:</p> <ol> <li> <p>Installed ClientAI with your chosen provider:    <pre><code># For OpenAI\npip install clientai[openai]\n</code></pre></p> </li> <li> <p>Initialize a ClientAI instance:    <pre><code>from clientai import ClientAI\n\n# OpenAI\nclient = ClientAI('openai', api_key=\"your-api-key\")\n\n# See provider-specific documentation for other options\n</code></pre></p> </li> <li> <p>Basic understanding of:</p> <ul> <li>Language model capabilities and limitations</li> <li>Python type hints and decorators</li> </ul> </li> </ol>"},{"location":"usage/agent/creating_agents/#understanding-agent-types","title":"Understanding Agent Types","text":"<p>Choose the right agent type based on your needs:</p>"},{"location":"usage/agent/creating_agents/#single-step-agents","title":"Single-Step Agents","text":"<p>Best for:</p> <ul> <li>Simple, focused tasks (translation, summarization)</li> <li>Quick responses needed</li> <li>Minimal context requirements</li> <li>Independent operations</li> </ul>"},{"location":"usage/agent/creating_agents/#multi-step-agents","title":"Multi-Step Agents","text":"<p>Best for:</p> <ul> <li>Complex reasoning chains</li> <li>Tool-heavy workflows</li> <li>Context-dependent operations</li> <li>Tasks requiring multiple capabilities</li> </ul>"},{"location":"usage/agent/creating_agents/#simple-single-step-agents","title":"Simple Single-Step Agents","text":"<p>The <code>create_agent</code> factory function provides the fastest way to create task-specific agents:</p>"},{"location":"usage/agent/creating_agents/#basic-translation-agent","title":"Basic Translation Agent","text":"<pre><code>translator = create_agent(\n    client=client,\n    role=\"translator\",\n    system_prompt=\"You are a French translator. Translate input to French.\",\n    model=\"gpt-4\"\n)\nresult = translator.run(\"Hello world!\")\n</code></pre>"},{"location":"usage/agent/creating_agents/#analysis-agent-with-streaming","title":"Analysis Agent with Streaming","text":"<pre><code>analyzer = create_agent(\n    client=client,\n    role=\"analyzer\",\n    system_prompt=\"Analyze data and provide detailed insights.\",\n    model=\"gpt-4\",\n    step=\"think\",    # Uses analysis-optimized parameters\n    stream=True      # Enable streaming responses\n)\n\n# Stream the analysis\nfor chunk in analyzer.run(\"Sales data: [100, 150, 120]\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"usage/agent/creating_agents/#complete-configuration-example","title":"Complete Configuration Example","text":"<pre><code># Analysis agent with \"think\" step type\nanalyzer = create_agent(\n    client=client,\n    role=\"analyzer\",\n    system_prompt=\"Analyze data and provide insights.\",\n    model=\"gpt-4\",\n    step=\"think\",    # affects default parameters like temperature\n                     # Can be: \n                     # - \"think\" (analysis, temp=0.7),\n                     # - \"act\" (decisions, temp=0.2),\n                     # - \"observe\" (data gathering, temp=0.1),\n                     # -  \"synthesize\" (summarizing, temp=0.4)\n\n    # ---------- optional parameters ----------\n    temperature=0.7,            # Controls randomness in responses (0.0-1.0)\n    top_p=0.9,                  # Controls diversity of responses(0.0-1.0)\n    stream=False,               # Whether to stream responses chunk by chunk\n    tools=[],                   # List of tools the agent can use (functions, ToolConfig)\n    tool_selection_config=None, # Complete tool selection configuration\n    tool_confidence=0.8,        # Threshold for tool selection confidence (0.0-1.0)\n    tool_model=\"gpt-4\",         # Specific model to use for tool selection decisions\n    max_tools_per_step=3,       # Maximum number of tools to use in a single step\n    **model_kwargs              # Any additional model-specific parameters\n)\n</code></pre>"},{"location":"usage/agent/creating_agents/#multi-step-agents_1","title":"Multi-Step Agents","text":"<p>For more complex tasks, create custom agents with multiple steps:</p> <pre><code>class AnalysisAgent(Agent):\n    @think\n    def analyze(self, input_data: str) -&gt; str:\n        \"\"\"First step: analyze the input.\"\"\"\n        return f\"Please analyze this data: {input_data}\"\n\n    @act\n    def recommend(self, analysis: str) -&gt; str:\n        \"\"\"Second step: make recommendations.\"\"\"\n        return f\"Based on the analysis, recommend actions for: {analysis}\"\n</code></pre> <p>Initialize with configuration: <pre><code>agent = AnalysisAgent(\n    client=client,\n    default_model=ModelConfig(\n        name=\"gpt-4\",\n        temperature=0.7,\n        stream=True\n    )\n)\n</code></pre></p>"},{"location":"usage/agent/creating_agents/#agent-configuration","title":"Agent Configuration","text":"<p>Agents can be configured in several ways, with options ranging from simple to complex depending on your needs. The configuration primarily focuses on the model settings, which control how the agent interacts with the language model.</p>"},{"location":"usage/agent/creating_agents/#model-configuration-options","title":"Model Configuration Options","text":"<p>The default model configuration determines how your agent interacts with the AI provider. You have three ways to specify these settings, each offering different levels of control:</p> <ol> <li> <p>Simple String Name     The most basic approach - just specify the model name when you only need default settings:</p> <pre><code>agent = MyAgent(client, default_model=\"gpt-4\")\n</code></pre> <p>This is ideal for quick prototypes or when the default parameters work well for your use case.</p> </li> <li> <p>Dictionary Configuration     When you need more control, use a dictionary to specify multiple parameters:</p> <p><pre><code>agent = MyAgent(client, default_model={\n    \"name\": \"gpt-4\",\n    \"temperature\": 0.7, # Control response creativity\n    \"top_p\": 0.9,       # Control response diversity\n    \"stream\": True      # Enable streaming responses\n})\n</code></pre> This approach is good for runtime configuration or when loading settings from configuration files.</p> </li> <li> <p>ModelConfig Object     The most flexible approach, providing type safety and additional functionality:</p> <pre><code>agent = MyAgent(client, default_model=ModelConfig(\n    name=\"gpt-4\",\n    temperature=0.7,\n    top_p=0.9,\n    stream=False,\n    extra_param=\"value\"  # Provider-specific parameters\n))\n</code></pre> <p>Use this approach for production code.</p> </li> </ol>"},{"location":"usage/agent/creating_agents/#best-practices","title":"Best Practices","text":"<p>When creating agents, consider these key recommendations:</p>"},{"location":"usage/agent/creating_agents/#agent-type-selection","title":"Agent Type Selection","text":"<p>Choose the right agent based on your task:</p> <ul> <li> <p>Single-step agents for:</p> <ul> <li>Simple, independent operations (translation, summarization)</li> <li>Quick responses with minimal setup</li> <li>Basic transformations without complex logic</li> </ul> </li> <li> <p>Multi-step agents for:</p> <ul> <li>Complex workflows needing multiple steps</li> <li>Tasks requiring tool combinations</li> <li>Operations needing intermediate results</li> </ul> </li> </ul>"},{"location":"usage/agent/creating_agents/#configuration-guidelines","title":"Configuration Guidelines","text":"<ol> <li> <p>Model Selection</p> <ul> <li>Use powerful models (e.g., GPT-4) for complex reasoning</li> <li>Use faster models (e.g., qwen-2\u20137b) for simple transformations</li> <li>Start with default configurations and adjust as needed</li> </ul> </li> <li> <p>Basic Settings    <pre><code>agent = MyAgent(\n    client=client,\n    default_model=\"gpt-4\",\n    tools=[calculator, formatter], # Add tools only if needed\n    tool_confidence=0.8            # Adjust based on task criticality\n)\n</code></pre></p> </li> </ol>"},{"location":"usage/agent/creating_agents/#error-handling","title":"Error Handling","text":"<ul> <li>Always wrap agent execution in try/except blocks</li> <li>Configure retries for critical operations</li> <li>Enable logging during development:   <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></li> </ul> <p>Now that you've learned how to create and configure agents, head to the Workflow Steps section to discover:</p> <ul> <li>How to build complex reasoning chains using specialized steps</li> <li>The four types of steps and when to use each one</li> <li>Techniques for controlling data flow between steps</li> <li>Advanced step configuration for robust workflows</li> </ul>"},{"location":"usage/agent/tools/","title":"Tools and Tool Selection","text":"<p>Tools in ClientAI are functions that extend an agent's capabilities beyond language model interactions. The tool system provides:</p> <ul> <li>Function registration with automatic signature analysis</li> <li>Automated tool selection based on context</li> <li>Scoped tool availability for different workflow steps</li> <li>Comprehensive error handling and validation</li> </ul>"},{"location":"usage/agent/tools/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites</li> <li>Understanding Tools</li> <li>Creating Tools</li> <li>Tool Registration</li> <li>Tool Selection Configuration</li> <li>Advanced Usage</li> <li>Best Practices</li> </ol>"},{"location":"usage/agent/tools/#prerequisites","title":"Prerequisites","text":"<p>Before working with tools, ensure you have:</p> <ol> <li> <p>A basic understanding of:</p> <ul> <li>Python type hints and decorators</li> <li>Function documentation practices</li> <li>Basic ClientAI agent concepts</li> </ul> </li> <li> <p>Proper imports:     <pre><code>from clientai.agent import Agent, tool\nfrom clientai.agent.tools import ToolSelectionConfig\n</code></pre></p> </li> </ol>"},{"location":"usage/agent/tools/#understanding-tools","title":"Understanding Tools","text":"<p>Tools in ClientAI are functions with:</p> <ul> <li>Clear type hints for parameters and return values</li> <li>Descriptive docstrings explaining functionality</li> <li>Optional configuration for usage scopes and selection criteria</li> </ul>"},{"location":"usage/agent/tools/#tool-characteristics","title":"Tool Characteristics","text":"<p>Good tools should be:</p> <ul> <li>Focused on a single task</li> <li>Well-documented with clear inputs/outputs</li> <li>Error-handled appropriately</li> <li>Stateless when possible</li> </ul>"},{"location":"usage/agent/tools/#creating-tools","title":"Creating Tools","text":"<p>There are two main ways to create tools:</p>"},{"location":"usage/agent/tools/#1-using-the-tool-decorator","title":"1. Using the @tool Decorator","text":"<pre><code>@tool(name=\"Calculator\", description=\"Performs basic arithmetic\")\ndef add_numbers(x: int, y: int) -&gt; int:\n    \"\"\"Add two numbers together.\n\n    Args:\n        x: First number\n        y: Second number\n\n    Returns:\n        Sum of the two numbers\n    \"\"\"\n    return x + y\n</code></pre>"},{"location":"usage/agent/tools/#2-direct-tool-creation","title":"2. Direct Tool Creation","text":"<pre><code>from clientai.agent.tools import Tool\n\ndef multiply(x: int, y: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return x * y\n\nmultiply_tool = Tool.create(\n    func=multiply,\n    name=\"Multiplier\",\n    description=\"Multiplies two numbers together\"\n)\n</code></pre>"},{"location":"usage/agent/tools/#tool-registration","title":"Tool Registration","text":"<p>Tools can be registered with agents in several ways:</p>"},{"location":"usage/agent/tools/#1-during-agent-creation","title":"1. During Agent Creation","text":"<pre><code>agent = MyAgent(\n    client=client,\n    default_model=\"gpt-4\",\n    tools=[\n        calculator_tool,\n        formatter_tool,\n        ToolConfig(\n            tool=multiply,\n            scopes=[\"think\", \"act\"],\n            name=\"Multiplier\"\n        )\n    ]\n)\n</code></pre>"},{"location":"usage/agent/tools/#2-using-register_tool-method","title":"2. Using register_tool Method","text":"<pre><code># Register with specific scopes\nagent.register_tool(\n    tool=process_text,\n    name=\"TextProcessor\",\n    description=\"Processes text input\",\n    scopes=[\"think\", \"synthesize\"]\n)\n\n# Register for all scopes (default)\nagent.register_tool(\n    tool=calculate,\n    name=\"Calculator\",\n    description=\"Performs calculations\"\n)\n</code></pre>"},{"location":"usage/agent/tools/#3-using-register_tool-as-a-decorator","title":"3. Using register_tool as a Decorator","text":"<pre><code># Register with specific scopes\n@agent.register_tool(\n    name=\"Calculator\",\n    description=\"Performs calculations\",\n    scopes=[\"think\", \"act\"]\n)\ndef calculate(x: int, y: int) -&gt; int:\n    return x + y\n\n# Register for all scopes\n@agent.register_tool(\n    name=\"TextFormatter\",\n    description=\"Formats text\"\n)\ndef format_text(text: str) -&gt; str:\n    return text.upper()\n</code></pre>"},{"location":"usage/agent/tools/#tool-selection-configuration","title":"Tool Selection Configuration","text":"<p>Tool selection is a key feature that allows agents to automatically choose and use appropriate tools based on the task at hand. The selection process is powered by LLMs and can be customized to meet specific needs.</p>"},{"location":"usage/agent/tools/#basic-configuration","title":"Basic Configuration","text":"<p>Tool selection behavior can be customized using ToolSelectionConfig:</p> <pre><code>config = ToolSelectionConfig(\n    confidence_threshold=0.8,    # Minimum confidence for tool selection\n    max_tools_per_step=3,        # Maximum tools per step\n    prompt_template=\"Custom selection prompt: {task}\\nTools: {tool_descriptions}\"\n)\n\nagent = MyAgent(\n    client=client,\n    default_model=\"gpt-4\",\n    tool_selection_config=config,\n    tool_model=\"gpt-4\"          # Model for tool selection decisions\n)\n</code></pre>"},{"location":"usage/agent/tools/#understanding-tool-selection","title":"Understanding Tool Selection","text":"<p>When a step with <code>use_tools=True</code> is executed, the tool selection process:</p> <ol> <li>Builds a task description from the step's output</li> <li>Gathers available tools based on the step's scope</li> <li>Sends a structured prompt to the LLM</li> <li>Processes the LLM's decision</li> <li>Executes selected tools</li> <li>Incorporates results back into the workflow</li> </ol>"},{"location":"usage/agent/tools/#selection-prompt-structure","title":"Selection Prompt Structure","text":"<p>The default selection prompt looks like this:</p> <pre><code>You are a helpful AI that uses tools to solve problems.\n\nTask: [Step's output text]\n\nCurrent Context:\n- context_key1: value1\n- context_key2: value2\n\nAvailable Tools:\n- Calculator\n  Signature: add(x: int, y: int) -&gt; int\n  Description: Adds two numbers together\n- TextProcessor\n  Signature: process(text: str, uppercase: bool = False) -&gt; str\n  Description: Processes text with optional case conversion\n\nRespond ONLY with a JSON object in this format:\n{\n    \"tool_calls\": [\n        {\n            \"tool_name\": \"&lt;name of tool&gt;\",\n            \"arguments\": {\n                \"param_name\": \"param_value\"\n            },\n            \"confidence\": &lt;0.0-1.0&gt;,\n            \"reasoning\": \"&lt;why you chose this tool&gt;\"\n        }\n    ]\n}\n</code></pre>"},{"location":"usage/agent/tools/#tool-selection-results","title":"Tool Selection Results","text":"<p>The LLM responds with structured decisions:</p> <pre><code>{\n    \"tool_calls\": [\n        {\n            \"tool_name\": \"Calculator\",\n            \"arguments\": {\n                \"x\": 5,\n                \"y\": 3\n            },\n            \"confidence\": 0.95,\n            \"reasoning\": \"The task requires adding two numbers together\"\n        }\n    ]\n}\n</code></pre> <p>These decisions are then:</p> <ol> <li>Validated against tool signatures</li> <li>Filtered by confidence threshold</li> <li>Limited to max_tools_per_step</li> <li>Executed in order</li> <li>Results added to the prompt:</li> </ol> <pre><code>[Original prompt...]\n\nTool Execution Results:\n\nCalculator:\nResult: 8\nConfidence: 0.95\nReasoning: The task requires adding two numbers together\n</code></pre>"},{"location":"usage/agent/tools/#customizing-selection","title":"Customizing Selection","text":"<p>The selection process can be customized in several ways:</p>"},{"location":"usage/agent/tools/#1-custom-prompt-template","title":"1. Custom Prompt Template","text":"<pre><code>custom_template = \"\"\"\nGiven the current task and tools, decide which tools would help.\n\nTask: {task}\nContext: {context}\nTools Available:\n{tool_descriptions}\n\nReturn decision in JSON format:\n{\n    \"tool_calls\": [\n        {\n            \"tool_name\": \"name\",\n            \"arguments\": {\"param\": \"value\"},\n            \"confidence\": 0.0-1.0,\n            \"reasoning\": \"explanation\"\n        }\n    ]\n}\n\"\"\"\n\nconfig = ToolSelectionConfig(\n    prompt_template=custom_template\n)\n</code></pre>"},{"location":"usage/agent/tools/#2-selection-model-configuration","title":"2. Selection Model Configuration","text":"<pre><code>agent = MyAgent(\n    client=client,\n    default_model=\"gpt-4\",\n    tool_model=ModelConfig(\n        name=\"gpt-3.5-turbo\",\n        temperature=0.2,  # Lower temperature for more consistent selection\n        top_p=0.1        # More focused sampling for decisions\n    )\n)\n</code></pre>"},{"location":"usage/agent/tools/#3-confidence-thresholds","title":"3. Confidence Thresholds","text":"<pre><code># Global configuration\nconfig = ToolSelectionConfig(confidence_threshold=0.8)\n\n# Step-specific configuration\n@think(tool_confidence=0.9)  # Higher threshold for this step\ndef analyze(self, input_data: str) -&gt; str:\n    return f\"Analyze this: {input_data}\"\n</code></pre>"},{"location":"usage/agent/tools/#monitoring-tool-selection","title":"Monitoring Tool Selection","text":"<p>The tool selection process can be monitored through logging:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Will show:\n# - Full selection prompts\n# - LLM responses\n# - Tool execution attempts\n# - Validation results\n# - Error messages\n</code></pre> <p>Tool decisions are also stored in the agent's context:</p> <pre><code># After execution\ndecisions = agent.context.state[\"last_tool_decisions\"]\nfor decision in decisions:\n    print(f\"Tool: {decision['tool_name']}\")\n    print(f\"Confidence: {decision['confidence']}\")\n    print(f\"Result: {decision['result']}\")\n</code></pre>"},{"location":"usage/agent/tools/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/agent/tools/#tool-scopes","title":"Tool Scopes","text":"<p>Tools can be restricted to specific workflow steps:</p> <pre><code>agent.register_tool(\n    tool=analyze_data,\n    name=\"DataAnalyzer\",\n    scopes=[\"think\", \"observe\"]  # Only available in these steps\n)\n</code></pre> <p>Valid scopes are:</p> <ul> <li>\"think\": For analysis and reasoning steps</li> <li>\"act\": For action and decision steps</li> <li>\"observe\": For data collection steps</li> <li>\"synthesize\": For summarization steps</li> <li>\"all\": Available in all steps (default)</li> </ul>"},{"location":"usage/agent/tools/#custom-tool-models","title":"Custom Tool Models","text":"<p>Use different models for tool selection:</p> <pre><code>agent = MyAgent(\n    client=client,\n    default_model=\"gpt-4\",\n    tool_model=ModelConfig(\n        name=\"gpt-3.5-turbo\",\n        temperature=0.2\n    )\n)\n</code></pre>"},{"location":"usage/agent/tools/#direct-tool-usage","title":"Direct Tool Usage","text":"<p>Tools can be used directly when needed:</p> <pre><code>result = agent.use_tool(\n    \"Calculator\",\n    x=5,\n    y=3\n)\n</code></pre>"},{"location":"usage/agent/tools/#best-practices","title":"Best Practices","text":""},{"location":"usage/agent/tools/#tool-design","title":"Tool Design","text":"<ol> <li> <p>Keep tools focused and simple    <pre><code># Good\n@tool\ndef add(x: int, y: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return x + y\n\n# Avoid\n@tool\ndef math_operations(x: int, y: int, operation: str) -&gt; int:\n    \"\"\"Perform various math operations.\"\"\"\n    if operation == \"add\":\n        return x + y\n    elif operation == \"multiply\":\n        return x * y\n    # etc...\n</code></pre></p> </li> <li> <p>Use clear type hints and docstrings    <pre><code>@tool\ndef process_text(\n    text: str,\n    uppercase: bool = False,\n    max_length: Optional[int] = None\n) -&gt; str:\n    \"\"\"Process input text with formatting options.\n\n    Args:\n        text: The input text to process\n        uppercase: Whether to convert to uppercase\n        max_length: Optional maximum length\n\n    Returns:\n        Processed text string\n\n    Raises:\n        ValueError: If text is empty\n    \"\"\"\n    if not text:\n        raise ValueError(\"Text cannot be empty\")\n\n    result = text.upper() if uppercase else text\n    return result[:max_length] if max_length else result\n</code></pre></p> </li> <li> <p>Handle errors gracefully    <pre><code>@tool\ndef divide(x: float, y: float) -&gt; float:\n    \"\"\"Divide two numbers.\n\n    Args:\n        x: Numerator\n        y: Denominator\n\n    Returns:\n        Result of division\n\n    Raises:\n        ValueError: If attempting to divide by zero\n    \"\"\"\n    if y == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return x / y\n</code></pre></p> </li> </ol>"},{"location":"usage/agent/tools/#tool-configuration","title":"Tool Configuration","text":"<ol> <li>Set appropriate confidence thresholds based on task criticality</li> <li>Group related tools with consistent scopes</li> <li>Use specific tool models for complex selection decisions</li> <li>Monitor and log tool usage for optimization</li> </ol>"},{"location":"usage/agent/tools/#error-handling","title":"Error Handling","text":"<ul> <li>Always validate tool inputs</li> <li>Provide clear error messages</li> <li>Use appropriate exception types</li> <li>Log errors for debugging <pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\n@tool\ndef process_data(data: List[float]) -&gt; float:\n    \"\"\"Process numerical data.\n\n    Args:\n        data: List of numbers to process\n\n    Returns:\n        Processed result\n\n    Raises:\n        ValueError: If data is empty or contains invalid values\n    \"\"\"\n    try:\n        if not data:\n            raise ValueError(\"Data cannot be empty\")\n        return sum(data) / len(data)\n    except Exception as e:\n        logger.error(f\"Error processing data: {e}\")\n        raise\n</code></pre></li> </ul> <p>Now that you understand tools, explore the Context section to discover:</p> <ul> <li>How to maintain state across your agent's workflow</li> <li>Techniques for sharing data between steps</li> <li>Methods for tracking and accessing results</li> <li>Patterns for efficient context management</li> </ul>"},{"location":"usage/agent/workflow_steps/","title":"ClientAI Step Module Documentation","text":"<p>The Step Module is a core component of ClientAI's agent system, providing a structured way to build complex workflows through a series of specialized processing steps.</p>"},{"location":"usage/agent/workflow_steps/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Understanding Step Types</li> <li>Creating Steps</li> <li>Step Execution and Flow</li> <li>Managing Results</li> <li>Advanced Configuration</li> <li>Error Handling</li> </ol>"},{"location":"usage/agent/workflow_steps/#understanding-step-types","title":"Understanding Step Types","text":"<p>ClientAI provides four specialized step types, each optimized for different aspects of information processing and decision making. Each type comes with pre-configured parameters that optimize the language model's behavior for that specific purpose.</p>"},{"location":"usage/agent/workflow_steps/#think-steps","title":"Think Steps","text":"<p>Think steps are designed for analysis and reasoning tasks. They use higher temperature settings to encourage creative and exploratory thinking, making them ideal for problem-solving and complex analysis.</p> <pre><code>@think # Analysis &amp; Reasoning\n       # Temperature: 0.7 - Encourages creative thinking\n       # Top_p: 0.9 - Allows for diverse idea generation\n       # Best for: Complex analysis, planning, brainstorming\ndef analyze_data(self, input_data: str) -&gt; str:\n    \"\"\"Performs detailed analysis of input data.\"\"\"\n    return f\"Please analyze this data in detail: {input_data}\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#act-steps","title":"Act Steps","text":"<p>Act steps are optimized for decisive action and concrete decision-making. They use lower temperature settings to produce more focused and consistent outputs.</p> <pre><code>@act # Decision Making &amp; Action\n     # Temperature: 0.2 - Promotes consistent decisions\n     # Top_p: 0.8 - Maintains reasonable variation\n     # Best for: Making choices, executing actions\ndef make_decision(self, input_data: str) -&gt; str:\n    \"\"\"Makes a concrete decision based on analysis.\"\"\"\n    return f\"Based on the analysis, decide the best course of action: {input_data}\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#observe-steps","title":"Observe Steps","text":"<p>Observe steps are designed for accurate data collection and observation. They use very low temperature settings to maximize precision and accuracy.</p> <pre><code>@observe # Data Collection &amp; Observation\n         # Temperature: 0.1 - Maximizes accuracy\n         # Top_p: 0.5 - Ensures high precision\n         # Best for: Data gathering, validation, fact-checking\ndef collect_data(self, input_data: str) -&gt; str:\n    \"\"\"Gathers and validates specific information.\"\"\"\n    return f\"Please extract and validate the following information: {input_data}\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#synthesize-steps","title":"Synthesize Steps","text":"<p>Synthesize steps are optimized for combining information and creating summaries. They use moderate temperature settings to balance creativity with coherence.</p> <pre><code>@synthesize # Summarization &amp; Integration\n            # Temperature: 0.4 - Balances creativity and focus\n            # Top_p: 0.7 - Enables coherent synthesis\n            # Best for: Summarizing, combining information\ndef combine_insights(self, input_data: str) -&gt; str:\n    \"\"\"Synthesizes multiple pieces of information.\"\"\"\n    return f\"Please synthesize the following information into a coherent summary: {input_data}\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#creating-steps","title":"Creating Steps","text":"<p>Steps can be created with varying levels of configuration, from simple decorators to fully customized implementations. Here's a progression from basic to advanced step creation:</p>"},{"location":"usage/agent/workflow_steps/#basic-step-definition","title":"Basic Step Definition","text":"<p>The simplest way to create a step is using a decorator with default settings. This approach uses the function name as the step name and the docstring as the description.</p> <pre><code>class SimpleAgent(Agent):\n    @think\n    def analyze(self, input_data: str) -&gt; str:\n        \"\"\"Analyzes the input data.\"\"\"\n        return f\"Please analyze this data: {input_data}\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#configured-step-definition","title":"Configured Step Definition","text":"<p>For more control, you can provide specific configurations to customize the step's behavior. This allows you to override default settings and specify exactly how the step should operate.</p> <pre><code>class ConfiguredAgent(Agent):\n    @think(\n        name=\"detailed_analysis\",                 # Custom step name\n        description=\"Performs detailed analysis\", # Step description\n        send_to_llm=True,                         # Send output to LLM\n        model=ModelConfig(                        # Step-specific model config\n            name=\"gpt-4\",\n            temperature=0.7\n        ),\n        stream=True,        # Stream responses\n        json_output=False,  # Plain text output\n        use_tools=True      # Enable tool usage\n    )\n    def analyze(self, input_data: str) -&gt; str:\n        \"\"\"Customized analysis step with specific configuration.\"\"\"\n        return f\"Please perform a detailed analysis of: {input_data}\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#error-aware-step-definition","title":"Error-Aware Step Definition","text":"<p>For critical operations, you can create steps with robust error handling and retry logic. This is particularly important for steps that interact with external services or perform critical operations.</p> <pre><code>class RobustAgent(Agent):\n    @think(step_config=StepConfig(\n        enabled=True,           # Step can be enabled/disabled\n        required=True,          # Failure stops workflow\n        retry_count=2,          # Retry twice on failure\n        timeout=10.0,           # 10 second timeout\n        pass_result=True,       # Pass result to next step\n        use_internal_retry=True # Use internal retry mechanism\n    ))\n    def critical_step(self, input_data: str) -&gt; str:\n        \"\"\"Critical step with error handling and retry logic.\"\"\"\n        return f\"Process this critical data with care: {input_data}\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#step-execution-and-flow","title":"Step Execution and Flow","text":"<p>Understanding how steps execute and how data flows between them is crucial for building effective workflows. Here's a detailed look at the execution process:</p>"},{"location":"usage/agent/workflow_steps/#basic-flow","title":"Basic Flow","text":"<p>Steps execute in sequence, with each step's output potentially becoming input for the next step. The execution engine handles the flow of data and interaction with the language model.</p> <pre><code>class WorkflowAgent(Agent):\n    @think\n    def step1(self, input_data: str) -&gt; str:\n        # 1. Receives initial input or previous step result\n        # 2. Returns string that becomes LLM prompt\n        return f\"Please analyze: {input_data}\"\n        # 3. LLM processes prompt\n        # 4. Response stored in context.last_results[\"step1\"]\n        # 5. If pass_result=True, response becomes next step's input\n\n    @act(send_to_llm=False)\n    def step2(self, prev_result: str) -&gt; str:\n        # Manual processing - no LLM interaction\n        # Return value becomes step result directly\n        return prev_result.upper()\n\n    @synthesize\n    def step3(self, prev_result: str) -&gt; str:\n        # Gets step2's result as prev_result\n        # Can access other results through context\n        step1_result = self.context.get_step_result(\"step1\")\n        return f\"Synthesize {prev_result} with {step1_result}\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#controlling-data-flow","title":"Controlling Data Flow","text":"<p>You can control how results flow between steps using the <code>pass_result</code> configuration. This gives you fine-grained control over what data each step receives.</p> <pre><code>class FlowControlAgent(Agent):\n    @think(step_config=StepConfig(pass_result=False))\n    def analyze(self, input_data: str) -&gt; str:\n        # Result stored but not passed as next input\n        return f\"Analysis: {input_data}\"\n\n    @act\n    def process(self, input_data: str) -&gt; str:\n        # Still gets original input\n        # Must explicitly access analysis result if needed\n        analysis = self.context.get_step_result(\"analyze\")\n        return f\"Processing {input_data} using {analysis}\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#managing-results","title":"Managing Results","text":"<p>ClientAI provides multiple ways to access and manage step results, giving you flexibility in how you handle data between steps.</p>"},{"location":"usage/agent/workflow_steps/#parameter-based-access","title":"Parameter-Based Access","text":"<p>The most straightforward way to access previous results is through step parameters. The number of parameters determines which results a step receives:</p> <pre><code>class ResultAgent(Agent):\n    @think\n    def first(self, input_data: str) -&gt; str:\n        # Gets initial input\n        return \"First step result\"\n\n    @act\n    def second(self, prev_result: str) -&gt; str:\n        # Gets first step's result\n        return f\"Using previous result: {prev_result}\"\n\n    @synthesize\n    def third(self, latest: str, earlier: str) -&gt; str:\n        # latest = second step result\n        # earlier = first step result\n        return f\"Combining latest ({latest}) with earlier ({earlier})\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#context-based-access","title":"Context-Based Access","text":"<p>For more flexible access to results, you can use the context system. This allows you to access any result at any time:</p> <pre><code>class ContextAgent(Agent):\n    @think\n    def flexible_step(self) -&gt; str:\n        # Access results through context\n        current = self.context.current_input\n        first_result = self.context.get_step_result(\"first\")\n        return f\"\"\"\n        Working with current input ({current}) and first result ({first_result})\n        \"\"\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#advanced-configuration","title":"Advanced Configuration","text":"<p>Advanced configuration options allow you to fine-tune step behavior for specific needs.</p>"},{"location":"usage/agent/workflow_steps/#model-configuration-per-step","title":"Model Configuration Per Step","text":"<p>Different steps may require different model configurations for optimal performance:</p> <pre><code>class AdvancedAgent(Agent):\n    @think(model=ModelConfig(\n        name=\"gpt-4\",        # Powerful model for complex analysis\n        temperature=0.7,     # Creative thinking\n        stream=True          # Stream responses\n    ))\n    def complex_step(self, input_data: str) -&gt; str:\n        \"\"\"Complex analysis requiring a powerful model.\"\"\"\n        return f\"Perform complex analysis of: {input_data}\"\n\n    @act(\n        model=\"gpt-3.5\",     # Simpler model for basic actions\n        tool_model=\"llama-2\" # Different model for tool selection\n    )\n    def simple_step(self, input_data: str) -&gt; str:\n        \"\"\"Simple action using a more efficient model.\"\"\"\n        return f\"Perform simple action on: {input_data}\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#tool-usage-configuration","title":"Tool Usage Configuration","text":"<p>Configure how steps interact with tools:</p> <pre><code>class ToolingAgent(Agent):\n    @think(\n        use_tools=True,\n        tool_confidence=0.8,    # High confidence requirement\n        tool_model=\"gpt-4\",     # Specific model for tool selection\n        max_tools_per_step=2    # Limit tool usage\n    )\n    def tool_step(self, input_data: str) -&gt; str:\n        \"\"\"Step that carefully selects and uses tools.\"\"\"\n        return f\"Analyze this data using appropriate tools: {input_data}\"\n\n    @act(tool_selection_config=ToolSelectionConfig(\n        confidence_threshold=0.8,\n        max_tools_per_step=3,\n        prompt_template=\"Custom tool selection: {task}\"\n    ))\n    def custom_tool_step(self, input_data: str) -&gt; str:\n        \"\"\"Step with custom tool selection behavior.\"\"\"\n        return f\"Process this data with custom tool selection: {input_data}\"\n</code></pre>"},{"location":"usage/agent/workflow_steps/#error-handling","title":"Error Handling","text":"<p>Robust error handling is crucial for reliable agent workflows. Here's how to implement comprehensive error handling:</p>"},{"location":"usage/agent/workflow_steps/#basic-error-handling","title":"Basic Error Handling","text":"<p>Configure steps with appropriate error handling settings based on their importance:</p> <pre><code>class ErrorAwareAgent(Agent):\n    @think(step_config=StepConfig(\n        required=True,     # Must succeed\n        retry_count=2,     # Retry twice\n        timeout=5.0        # 5 second timeout\n    ))\n    def must_succeed(self, input_data: str) -&gt; str:\n        \"\"\"Critical step that must complete successfully.\"\"\"\n        return f\"Critical processing of {input_data}\"\n\n    @act(step_config=StepConfig(\n        required=False,   # Optional step\n        retry_count=1     # One retry\n    ))\n    def can_fail(self, input_data: str) -&gt; str:\n        \"\"\"Optional step that can fail without stopping workflow.\"\"\"\n        return f\"Optional processing of {input_data}\"\n</code></pre> <p>Robust error handling is essential for building reliable agent workflows. Your agent needs to handle various types of failures, from invalid inputs to failed LLM calls, while maintaining a clear record of what went wrong. Here's how to implement comprehensive error checking and recovery mechanisms.</p>"},{"location":"usage/agent/workflow_steps/#basic-result-checking","title":"Basic Result Checking","text":"<p>Monitoring step execution results is your first line of defense. This involves validating both the presence and quality of results from each step, with different handling strategies for critical versus optional steps. Here's a pattern that covers the essential checks:</p> <pre><code>class ErrorCheckingAgent(Agent):\n    def check_results(self) -&gt; None:\n        \"\"\"Check results after workflow execution.\"\"\"\n        try:\n            # Check critical steps\n            critical_result = self.context.get_step_result(\"must_succeed\")\n            if not critical_result:\n                raise WorkflowError(\"Critical step failed to complete\")\n\n            # Validate result format/content\n            if not self._validate_result(critical_result):\n                raise ValidationError(f\"Invalid result format: {critical_result}\")\n\n            # Check optional steps\n            optional_result = self.context.get_step_result(\"can_fail\")\n            if optional_result is None:\n                logger.warning(\"Optional step failed but workflow continued\")\n\n        except (WorkflowError, ValidationError) as e:\n            logger.error(f\"Workflow validation failed: {e}\")\n            self._initiate_recovery()\n        except Exception as e:\n            logger.exception(\"Unexpected error during result checking\")\n            raise\n\n    def _validate_result(self, result: str) -&gt; bool:\n        \"\"\"Validate basic result requirements.\"\"\"\n        if not result:\n            return False\n        try:\n            # Check for required components\n            required_keywords = [\"analysis\", \"recommendation\"]\n            return all(keyword in result.lower() for keyword in required_keywords)\n        except Exception:\n            return False\n</code></pre> <p>Key points to remember:</p> <ul> <li>Catch and handle specific exceptions appropriately - be explicit about what can fail and how to handle each case</li> <li>Log errors with sufficient context for debugging - include relevant state information and clear error messages</li> <li>Implement validation for critical results - don't assume outputs will always be valid</li> <li>Have clear recovery strategies for different error types - know what to do when things go wrong</li> <li>Use logging levels appropriately (error, warning, debug) - this helps with monitoring and debugging</li> </ul>"},{"location":"usage/agent/workflow_steps/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Choose Step Types Wisely</p> <ul> <li>Use <code>@think</code> for complex analysis and reasoning</li> <li>Use <code>@act</code> for decisive actions and choices</li> <li>Use <code>@observe</code> for accurate data collection</li> <li>Use <code>@synthesize</code> for combining information</li> </ul> </li> <li> <p>Manage Results Carefully</p> <ul> <li>Use parameter-based access when possible for clarity</li> <li>Use context access for complex workflows</li> <li>Be explicit about result flow with pass_result</li> </ul> </li> <li> <p>Handle Errors Appropriately</p> <ul> <li>Configure retry and timeout for unreliable operations</li> <li>Mark critical steps as required</li> <li>Implement proper error checking</li> </ul> </li> <li> <p>Optimize Performance</p> <ul> <li>Use appropriate models for each step's complexity</li> <li>Configure tool selection carefully</li> <li>Stream responses when appropriate</li> </ul> </li> </ol> <p>Remember that steps are building blocks - design them to be modular, focused, and reusable when possible. Clear documentation and appropriate error handling will make your workflows more maintainable and reliable.</p> <p>Now that you've mastered workflow steps, continue to the Tools section to learn:</p> <ul> <li>How to extend your agent's capabilities with custom tools</li> <li>Techniques for automatic tool selection and execution</li> <li>Best practices for tool design and implementation</li> <li>Advanced tool configuration and error handling</li> </ul>"},{"location":"usage/client/chat_functionality/","title":"Chat Functionality in ClientAI","text":"<p>This guide covers how to leverage ClientAI's chat functionality. You'll learn about creating chat conversations, managing context, and handling chat-specific features across supported providers.</p>"},{"location":"usage/client/chat_functionality/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Chat Interaction</li> <li>Managing Conversation Context</li> <li>Advanced Chat Features</li> <li>Provider-Specific Chat Capabilities</li> <li>Best Practices</li> </ol>"},{"location":"usage/client/chat_functionality/#basic-chat-interaction","title":"Basic Chat Interaction","text":"<p>To use the chat functionality in ClientAI, use the <code>chat</code> method:</p> <pre><code>from clientai import ClientAI\n\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hello, who are you?\"}\n]\n\nresponse = client.chat(messages, model=\"gpt-3.5-turbo\")\nprint(response)\n\n# Continue the conversation\nmessages.append({\"role\": \"assistant\", \"content\": response})\nmessages.append({\"role\": \"user\", \"content\": \"What can you help me with?\"})\n\nresponse = client.chat(messages, model=\"gpt-3.5-turbo\")\nprint(response)\n</code></pre> <p>This example demonstrates a simple back-and-forth conversation.</p>"},{"location":"usage/client/chat_functionality/#managing-conversation-context","title":"Managing Conversation Context","text":"<p>Effective context management is crucial for coherent conversations:</p> <pre><code>conversation = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in Python programming.\"},\n    {\"role\": \"user\", \"content\": \"How do I use list comprehensions in Python?\"}\n]\n\nresponse = client.chat(conversation, model=\"gpt-3.5-turbo\")\nprint(response)\n\nconversation.append({\"role\": \"assistant\", \"content\": response})\nconversation.append({\"role\": \"user\", \"content\": \"Can you give an example?\"})\n\nresponse = client.chat(conversation, model=\"gpt-3.5-turbo\")\nprint(response)\n</code></pre> <p>This example shows how to maintain context across multiple exchanges, including a system message to set the assistant's role.</p>"},{"location":"usage/client/chat_functionality/#advanced-chat-features","title":"Advanced Chat Features","text":""},{"location":"usage/client/chat_functionality/#streaming-chat-responses","title":"Streaming Chat Responses","text":"<p>For real-time conversation, you can stream chat responses:</p> <pre><code>conversation = [\n    {\"role\": \"user\", \"content\": \"Tell me a long story about space exploration\"}\n]\n\nfor chunk in client.chat(conversation, model=\"gpt-3.5-turbo\", stream=True):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"usage/client/chat_functionality/#temperature-and-top-p-sampling","title":"Temperature and Top-p Sampling","text":"<p>Adjust the creativity and randomness of responses:</p> <pre><code>response = client.chat(\n    conversation,\n    model=\"gpt-3.5-turbo\",\n    temperature=0.7,\n    top_p=0.9\n)\n</code></pre>"},{"location":"usage/client/chat_functionality/#provider-specific-chat-capabilities","title":"Provider-Specific Chat Capabilities","text":"<p>Different providers may offer unique chat features:</p>"},{"location":"usage/client/chat_functionality/#openai","title":"OpenAI","text":"<pre><code>openai_client = ClientAI('openai', api_key=\"your-openai-api-key\")\n\nresponse = openai_client.chat(\n    [{\"role\": \"user\", \"content\": \"Translate 'Hello, world!' to Japanese\"}],\n    model=\"gpt-4\"\n)\n</code></pre>"},{"location":"usage/client/chat_functionality/#replicate","title":"Replicate","text":"<pre><code>replicate_client = ClientAI('replicate', api_key=\"your-replicate-api-key\")\n\nresponse = replicate_client.chat(\n    [{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n    model=\"meta/llama-2-70b-chat:latest\"\n)\n</code></pre>"},{"location":"usage/client/chat_functionality/#ollama","title":"Ollama","text":"<pre><code>ollama_client = ClientAI('ollama', host=\"http://localhost:11434\")\n\nresponse = ollama_client.chat(\n    [{\"role\": \"user\", \"content\": \"What are the three laws of robotics?\"}],\n    model=\"llama2\"\n)\n</code></pre>"},{"location":"usage/client/chat_functionality/#best-practices","title":"Best Practices","text":"<ol> <li>Context Management: Keep track of the conversation history, but be mindful of token limits.</li> </ol> <pre><code>max_context_length = 10\nif len(conversation) &gt; max_context_length:\n    conversation = conversation[-max_context_length:]\n</code></pre> <ol> <li>Error Handling: Implement robust error handling for chat interactions:</li> </ol> <pre><code>try:\n    response = client.chat(conversation, model=\"gpt-3.5-turbo\")\nexcept Exception as e:\n    print(f\"An error occurred during chat: {e}\")\n    response = \"I'm sorry, I encountered an error. Could you please try again?\"\n</code></pre> <ol> <li>User Input Validation: Validate and sanitize user inputs to prevent potential issues:</li> </ol> <pre><code>def sanitize_input(user_input):\n    # Implement appropriate sanitization logic\n    return user_input.strip()\n\nuser_message = sanitize_input(input(\"Your message: \"))\nconversation.append({\"role\": \"user\", \"content\": user_message})\n</code></pre> <ol> <li>Graceful Fallbacks: Implement fallback mechanisms for when the AI doesn't understand or can't provide a suitable response:</li> </ol> <pre><code>if not response or response.lower() == \"i don't know\":\n    response = \"I'm not sure about that. Could you please rephrase or ask something else?\"\n</code></pre> <ol> <li>Model Selection: Choose appropriate models based on the complexity of your chat application:</li> </ol> <pre><code>model = \"gpt-4\" if complex_conversation else \"gpt-3.5-turbo\"\nresponse = client.chat(conversation, model=model)\n</code></pre> <ol> <li>Conversation Resetting: Provide options to reset or start new conversations:</li> </ol> <pre><code>def reset_conversation():\n    return [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n\n# Usage\nconversation = reset_conversation()\n</code></pre> <p>By following these guidelines and exploring the various features available, you can create sophisticated chat applications using ClientAI across different AI providers.</p>"},{"location":"usage/client/error_handling/","title":"Error Handling in ClientAI","text":"<p>ClientAI provides a robust error handling system that unifies exceptions across different AI providers. This guide covers how to handle potential errors when using ClientAI.</p>"},{"location":"usage/client/error_handling/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Exception Hierarchy</li> <li>Handling Errors</li> <li>Provider-Specific Error Mapping</li> <li>Best Practices</li> </ol>"},{"location":"usage/client/error_handling/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>ClientAI uses a custom exception hierarchy to provide consistent error handling across different AI providers:</p> <pre><code>from clientai.exceptions import (\n    ClientAIError,\n    AuthenticationError,\n    RateLimitError,\n    InvalidRequestError,\n    ModelError,\n    TimeoutError,\n    APIError\n)\n</code></pre> <ul> <li><code>ClientAIError</code>: Base exception class for all ClientAI errors.</li> <li><code>AuthenticationError</code>: Raised when there's an authentication problem with the AI provider.</li> <li><code>RateLimitError</code>: Raised when the AI provider's rate limit is exceeded.</li> <li><code>InvalidRequestError</code>: Raised when the request to the AI provider is invalid.</li> <li><code>ModelError</code>: Raised when there's an issue with the specified model.</li> <li><code>TimeoutError</code>: Raised when a request to the AI provider times out.</li> <li><code>APIError</code>: Raised when there's an API-related error from the AI provider.</li> </ul>"},{"location":"usage/client/error_handling/#handling-errors","title":"Handling Errors","text":"<p>Here's how to handle potential errors when using ClientAI:</p> <pre><code>from clientai import ClientAI\nfrom clientai.exceptions import (\n    ClientAIError,\n    AuthenticationError,\n    RateLimitError,\n    InvalidRequestError,\n    ModelError,\n    TimeoutError,\n    APIError\n)\n\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\ntry:\n    response = client.generate_text(\"Tell me a joke\", model=\"gpt-3.5-turbo\")\n    print(f\"Generated text: {response}\")\nexcept AuthenticationError as e:\n    print(f\"Authentication error: {e}\")\nexcept RateLimitError as e:\n    print(f\"Rate limit exceeded: {e}\")\nexcept InvalidRequestError as e:\n    print(f\"Invalid request: {e}\")\nexcept ModelError as e:\n    print(f\"Model error: {e}\")\nexcept TimeoutError as e:\n    print(f\"Request timed out: {e}\")\nexcept APIError as e:\n    print(f\"API error: {e}\")\nexcept ClientAIError as e:\n    print(f\"An unexpected ClientAI error occurred: {e}\")\n</code></pre>"},{"location":"usage/client/error_handling/#provider-specific-error-mapping","title":"Provider-Specific Error Mapping","text":"<p>ClientAI maps provider-specific errors to its custom exception hierarchy. For example:</p>"},{"location":"usage/client/error_handling/#openai","title":"OpenAI","text":"<pre><code>def _map_exception_to_clientai_error(self, e: Exception) -&gt; None:\n    error_message = str(e)\n    status_code = getattr(e, 'status_code', None)\n\n    if isinstance(e, OpenAIAuthenticationError) or \"incorrect api key\" in error_message.lower():\n        raise AuthenticationError(error_message, status_code, original_error=e)\n    elif status_code == 429 or \"rate limit\" in error_message.lower():\n        raise RateLimitError(error_message, status_code, original_error=e)\n    elif status_code == 404 or \"not found\" in error_message.lower():\n        raise ModelError(error_message, status_code, original_error=e)\n    elif status_code == 400 or \"invalid\" in error_message.lower():\n        raise InvalidRequestError(error_message, status_code, original_error=e)\n    elif status_code == 408 or \"timeout\" in error_message.lower():\n        raise TimeoutError(error_message, status_code, original_error=e)\n    elif status_code and status_code &gt;= 500:\n        raise APIError(error_message, status_code, original_error=e)\n\n    raise ClientAIError(error_message, status_code, original_error=e)\n</code></pre>"},{"location":"usage/client/error_handling/#replicate","title":"Replicate","text":"<pre><code>def _map_exception_to_clientai_error(self, e: Exception, status_code: int = None) -&gt; ClientAIError:\n    error_message = str(e)\n    status_code = status_code or getattr(e, 'status_code', None)\n\n    if \"authentication\" in error_message.lower() or \"unauthorized\" in error_message.lower():\n        return AuthenticationError(error_message, status_code, original_error=e)\n    elif \"rate limit\" in error_message.lower():\n        return RateLimitError(error_message, status_code, original_error=e)\n    elif \"not found\" in error_message.lower():\n        return ModelError(error_message, status_code, original_error=e)\n    elif \"invalid\" in error_message.lower():\n        return InvalidRequestError(error_message, status_code, original_error=e)\n    elif \"timeout\" in error_message.lower() or status_code == 408:\n        return TimeoutError(error_message, status_code, original_error=e)\n    elif status_code == 400:\n        return InvalidRequestError(error_message, status_code, original_error=e)\n    else:\n        return APIError(error_message, status_code, original_error=e)\n</code></pre>"},{"location":"usage/client/error_handling/#groq","title":"Groq","text":"<pre><code>def _map_exception_to_clientai_error(self, e: Exception) -&gt; ClientAIError:\n    error_message = str(e)\n\n    if isinstance(e, (GroqAuthenticationError | PermissionDeniedError)):\n        return AuthenticationError(\n            error_message,\n            status_code=getattr(e, \"status_code\", 401),\n            original_error=e,\n        )\n    elif isinstance(e, GroqRateLimitError):\n        return RateLimitError(error_message, status_code=429, original_error=e)\n    elif isinstance(e, NotFoundError):\n        return ModelError(error_message, status_code=404, original_error=e)\n    elif isinstance(e, (BadRequestError, UnprocessableEntityError, ConflictError)):\n        return InvalidRequestError(\n            error_message,\n            status_code=getattr(e, \"status_code\", 400),\n            original_error=e,\n        )\n    elif isinstance(e, APITimeoutError):\n        return TimeoutError(error_message, status_code=408, original_error=e)\n    elif isinstance(e, InternalServerError):\n        return APIError(\n            error_message,\n            status_code=getattr(e, \"status_code\", 500),\n            original_error=e,\n        )\n    elif isinstance(e, APIStatusError):\n        status = getattr(e, \"status_code\", 500)\n        if status &gt;= 500:\n            return APIError(error_message, status_code=status, original_error=e)\n        return InvalidRequestError(error_message, status_code=status, original_error=e)\n\n    return ClientAIError(error_message, status_code=500, original_error=e)\n</code></pre>"},{"location":"usage/client/error_handling/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Specific Exception Handling: Catch specific exceptions when you need to handle them differently.</p> </li> <li> <p>Logging: Log errors for debugging and monitoring purposes.</p> </li> </ol> <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    response = client.generate_text(\"Tell me a joke\", model=\"gpt-3.5-turbo\")\nexcept ClientAIError as e:\n    logger.error(f\"An error occurred: {e}\", exc_info=True)\n</code></pre> <ol> <li>Retry Logic: Implement retry logic for transient errors like rate limiting.</li> </ol> <pre><code>import time\nfrom clientai.exceptions import RateLimitError\n\ndef retry_generate(prompt, model, max_retries=3, delay=1):\n    for attempt in range(max_retries):\n        try:\n            return client.generate_text(prompt, model=model)\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n            wait_time = e.retry_after if hasattr(e, 'retry_after') else delay * (2 ** attempt)\n            logger.warning(f\"Rate limit reached. Waiting for {wait_time} seconds...\")\n            time.sleep(wait_time)\n</code></pre> <ol> <li>Graceful Degradation: Implement fallback options when errors occur.</li> </ol> <pre><code>def generate_with_fallback(prompt, primary_client, fallback_client):\n    try:\n        return primary_client.generate_text(prompt, model=\"gpt-3.5-turbo\")\n    except ClientAIError as e:\n        logger.warning(f\"Primary client failed: {e}. Falling back to secondary client.\")\n        return fallback_client.generate_text(prompt, model=\"llama-2-70b-chat\")\n</code></pre> <p>By following these practices and utilizing ClientAI's unified error handling system, you can create more robust and maintainable applications that gracefully handle errors across different AI providers.</p>"},{"location":"usage/client/initialization/","title":"Initializing ClientAI","text":"<p>This guide covers the process of initializing ClientAI with different AI providers. You'll learn how to set up ClientAI for use with OpenAI, Replicate, and Ollama.</p>"},{"location":"usage/client/initialization/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites</li> <li>OpenAI Initialization</li> <li>Replicate Initialization</li> <li>Ollama Initialization</li> <li>Multiple Provider Initialization</li> <li>Best Practices</li> </ol>"},{"location":"usage/client/initialization/#prerequisites","title":"Prerequisites","text":"<p>Before initializing ClientAI, ensure you have:</p> <ol> <li>Installed ClientAI: <code>pip install clientai[all]</code></li> <li>Obtained necessary API keys for the providers you plan to use</li> <li>Basic understanding of Python and asynchronous programming</li> </ol>"},{"location":"usage/client/initialization/#openai-initialization","title":"OpenAI Initialization","text":"<p>To initialize ClientAI with OpenAI:</p> <pre><code>from clientai import ClientAI\n\nopenai_client = ClientAI('openai', api_key=\"your-openai-api-key\")\n</code></pre> <p>Replace <code>\"your-openai-api-key\"</code> with your actual OpenAI API key.</p>"},{"location":"usage/client/initialization/#replicate-initialization","title":"Replicate Initialization","text":"<p>To initialize ClientAI with Replicate:</p> <pre><code>from clientai import ClientAI\n\nreplicate_client = ClientAI('replicate', api_key=\"your-replicate-api-key\")\n</code></pre> <p>Replace <code>\"your-replicate-api-key\"</code> with your actual Replicate API key.</p>"},{"location":"usage/client/initialization/#ollama-initialization","title":"Ollama Initialization","text":"<p>To initialize ClientAI with Ollama:</p> <pre><code>from clientai import ClientAI\n\nollama_client = ClientAI('ollama', host=\"http://localhost:11434\")\n</code></pre> <p>Ensure that you have Ollama running locally on the specified host.</p>"},{"location":"usage/client/initialization/#groq-initialization","title":"Groq Initialization","text":"<p>To initialize ClientAI with Groq:</p> <pre><code>from clientai import ClientAI\n\nreplicate_client = ClientAI('groq', api_key=\"your-groq-api-key\")\n</code></pre>"},{"location":"usage/client/initialization/#multiple-provider-initialization","title":"Multiple Provider Initialization","text":"<p>You can initialize multiple providers in the same script:</p> <pre><code>from clientai import ClientAI\n\nopenai_client = ClientAI('openai', api_key=\"your-openai-api-key\")\nreplicate_client = ClientAI('replicate', api_key=\"your-replicate-api-key\")\ngroq_client = ClientAI('groq', api_key=\"your-groq-api-key\")\nollama_client = ClientAI('ollama', host=\"http://localhost:11434\")\n</code></pre>"},{"location":"usage/client/initialization/#best-practices","title":"Best Practices","text":"<ol> <li>Environment Variables: Store API keys in environment variables instead of hardcoding them in your script:</li> </ol> <pre><code>import os\nfrom clientai import ClientAI\n\nopenai_client = ClientAI('openai', api_key=os.getenv('OPENAI_API_KEY'))\n</code></pre> <ol> <li>Error Handling: Wrap initialization in a try-except block to handle potential errors:</li> </ol> <pre><code>try:\n    client = ClientAI('openai', api_key=\"your-openai-api-key\")\nexcept ValueError as e:\n    print(f\"Error initializing ClientAI: {e}\")\n</code></pre> <ol> <li>Configuration Files: For projects with multiple providers, consider using a configuration file:</li> </ol> <pre><code>import json\nfrom clientai import ClientAI\n\nwith open('config.json') as f:\n    config = json.load(f)\n\nopenai_client = ClientAI('openai', **config['openai'])\nreplicate_client = ClientAI('replicate', **config['replicate'])\n</code></pre> <ol> <li>Lazy Initialization: If you're not sure which provider you'll use, initialize clients only when needed:</li> </ol> <pre><code>def get_client(provider):\n    if provider == 'openai':\n        return ClientAI('openai', api_key=\"your-openai-api-key\")\n    elif provider == 'replicate':\n        return ClientAI('replicate', api_key=\"your-replicate-api-key\")\n    # ... other providers ...\n\n# Use the client when needed\nclient = get_client('openai')\n</code></pre> <p>By following these initialization guidelines, you'll be well-prepared to start using ClientAI with various AI providers in your projects.</p>"},{"location":"usage/client/multiple_providers/","title":"Working with Multiple Providers in ClientAI","text":"<p>This guide explores techniques for effectively using multiple AI providers within a single project using ClientAI. You'll learn how to switch between providers and leverage their unique strengths.</p>"},{"location":"usage/client/multiple_providers/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Setting Up Multiple Providers</li> <li>Switching Between Providers</li> <li>Leveraging Provider Strengths</li> <li>Load Balancing and Fallback Strategies</li> <li>Best Practices</li> </ol>"},{"location":"usage/client/multiple_providers/#setting-up-multiple-providers","title":"Setting Up Multiple Providers","text":"<p>First, initialize ClientAI with multiple providers:</p> <pre><code>from clientai import ClientAI\n\nopenai_client = ClientAI('openai', api_key=\"your-openai-api-key\")\nreplicate_client = ClientAI('replicate', api_key=\"your-replicate-api-key\")\nollama_client = ClientAI('ollama', host=\"http://localhost:11434\")\n</code></pre>"},{"location":"usage/client/multiple_providers/#switching-between-providers","title":"Switching Between Providers","text":"<p>Create a function to switch between providers based on your requirements:</p> <pre><code>def get_provider(task):\n    if task == \"translation\":\n        return openai_client\n    elif task == \"code_generation\":\n        return replicate_client\n    elif task == \"local_inference\":\n        return ollama_client\n    else:\n        return openai_client  # Default to OpenAI\n\n# Usage\ntask = \"translation\"\nprovider = get_provider(task)\nresponse = provider.generate_text(\"Translate 'Hello' to French\", model=\"gpt-3.5-turbo\")\n</code></pre> <p>This approach allows you to dynamically select the most appropriate provider for each task.</p>"},{"location":"usage/client/multiple_providers/#leveraging-provider-strengths","title":"Leveraging Provider Strengths","text":"<p>Different providers excel in different areas. Here's how you can leverage their strengths:</p> <pre><code>def translate_text(text, target_language):\n    return openai_client.generate_text(\n        f\"Translate '{text}' to {target_language}\",\n        model=\"gpt-3.5-turbo\"\n    )\n\ndef generate_code(prompt):\n    return replicate_client.generate_text(\n        prompt,\n        model=\"meta/llama-2-70b-chat:latest\"\n    )\n\ndef local_inference(prompt):\n    return ollama_client.generate_text(\n        prompt,\n        model=\"llama2\"\n    )\n\n# Usage\nfrench_text = translate_text(\"Hello, world!\", \"French\")\npython_code = generate_code(\"Write a Python function to calculate the Fibonacci sequence\")\nquick_response = local_inference(\"What's the capital of France?\")\n</code></pre>"},{"location":"usage/client/multiple_providers/#load-balancing-and-fallback-strategies","title":"Load Balancing and Fallback Strategies","text":"<p>Implement load balancing and fallback strategies to ensure reliability:</p> <pre><code>import random\n\nproviders = [openai_client, replicate_client, ollama_client]\n\ndef load_balanced_generate(prompt, max_retries=3):\n    for _ in range(max_retries):\n        try:\n            provider = random.choice(providers)\n            return provider.generate_text(prompt, model=provider.default_model)\n        except Exception as e:\n            print(f\"Error with provider {provider.__class__.__name__}: {e}\")\n    raise Exception(\"All providers failed after max retries\")\n\n# Usage\ntry:\n    response = load_balanced_generate(\"Explain the concept of machine learning\")\n    print(response)\nexcept Exception as e:\n    print(f\"Failed to generate text: {e}\")\n</code></pre> <p>This function randomly selects a provider and falls back to others if there's an error.</p>"},{"location":"usage/client/multiple_providers/#best-practices","title":"Best Practices","text":"<ol> <li>Provider Selection Logic: Develop clear criteria for selecting providers based on task requirements, cost, and performance.</li> </ol> <pre><code>def select_provider(task, complexity, budget):\n    if complexity == \"high\" and budget == \"high\":\n        return openai_client  # Assuming OpenAI has more advanced models\n    elif task == \"code\" and budget == \"medium\":\n        return replicate_client\n    else:\n        return ollama_client  # Assuming Ollama is the most cost-effective\n</code></pre> <ol> <li>Consistent Interface: Create wrapper functions to provide a consistent interface across providers:</li> </ol> <pre><code>def unified_generate(prompt, provider=None):\n    if provider is None:\n        provider = get_default_provider()\n    return provider.generate_text(prompt, model=provider.default_model)\n\n# Usage\nresponse = unified_generate(\"Explain quantum computing\")\n</code></pre> <ol> <li>Error Handling and Logging: Implement comprehensive error handling and logging when working with multiple providers:</li> </ol> <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef safe_generate(prompt, provider):\n    try:\n        return provider.generate_text(prompt, model=provider.default_model)\n    except Exception as e:\n        logger.error(f\"Error with {provider.__class__.__name__}: {e}\")\n        return None\n</code></pre> <ol> <li>Performance Monitoring: Track the performance of different providers to optimize selection:</li> </ol> <pre><code>import time\n\ndef timed_generate(prompt, provider):\n    start_time = time.time()\n    result = provider.generate_text(prompt, model=provider.default_model)\n    elapsed_time = time.time() - start_time\n    logger.info(f\"{provider.__class__.__name__} took {elapsed_time:.2f} seconds\")\n    return result\n</code></pre> <ol> <li>Configuration Management: Use configuration files or environment variables to manage provider settings:</li> </ol> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nopenai_client = ClientAI('openai', api_key=os.getenv('OPENAI_API_KEY'))\nreplicate_client = ClientAI('replicate', api_key=os.getenv('REPLICATE_API_KEY'))\nollama_client = ClientAI('ollama', host=os.getenv('OLLAMA_HOST'))\n</code></pre> <ol> <li>Caching: Implement caching to reduce redundant API calls and improve response times:</li> </ol> <pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=100)\ndef cached_generate(prompt, provider_name):\n    provider = get_provider(provider_name)\n    return provider.generate_text(prompt, model=provider.default_model)\n\n# Usage\nresponse = cached_generate(\"What is the speed of light?\", \"openai\")\n</code></pre> <p>By following these practices and leveraging the strengths of multiple providers, you can create more robust, efficient, and versatile applications with ClientAI.</p>"},{"location":"usage/client/text_generation/","title":"Text Generation with ClientAI","text":"<p>This guide explores how to use ClientAI for text generation tasks across different AI providers. You'll learn about the various options and parameters available for generating text.</p>"},{"location":"usage/client/text_generation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Text Generation</li> <li>Advanced Parameters</li> <li>Streaming Responses</li> <li>Provider-Specific Features</li> <li>Best Practices</li> </ol>"},{"location":"usage/client/text_generation/#basic-text-generation","title":"Basic Text Generation","text":"<p>To generate text using ClientAI, use the <code>generate_text</code> method:</p> <pre><code>from clientai import ClientAI\n\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\nresponse = client.generate_text(\n    \"Write a short story about a robot learning to paint.\",\n    model=\"gpt-3.5-turbo\"\n)\n\nprint(response)\n</code></pre> <p>This will generate a short story based on the given prompt.</p>"},{"location":"usage/client/text_generation/#advanced-parameters","title":"Advanced Parameters","text":"<p>ClientAI supports various parameters to fine-tune text generation:</p> <pre><code>response = client.generate_text(\n    \"Explain the theory of relativity\",\n    model=\"gpt-4\",\n    max_tokens=150,\n    temperature=0.7,\n    top_p=0.9,\n    presence_penalty=0.1,\n    frequency_penalty=0.1\n)\n</code></pre> <ul> <li><code>max_tokens</code>: Maximum number of tokens to generate</li> <li><code>temperature</code>: Controls randomness (0.0 to 1.0)</li> <li><code>top_p</code>: Nucleus sampling parameter</li> <li><code>presence_penalty</code>: Penalizes new tokens based on their presence in the text so far</li> <li><code>frequency_penalty</code>: Penalizes new tokens based on their frequency in the text so far</li> </ul> <p>Note: Available parameters may vary depending on the provider.</p>"},{"location":"usage/client/text_generation/#streaming-responses","title":"Streaming Responses","text":"<p>For long-form content, you can use streaming to get partial responses as they're generated:</p> <pre><code>for chunk in client.generate_text(\n    \"Write a comprehensive essay on climate change\",\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre> <p>This allows for real-time display of generated text, which can be useful for user interfaces or long-running generations.</p>"},{"location":"usage/client/text_generation/#provider-specific-features","title":"Provider-Specific Features","text":"<p>Different providers may offer unique features. Here are some examples:</p>"},{"location":"usage/client/text_generation/#openai","title":"OpenAI","text":"<pre><code>response = openai_client.generate_text(\n    \"Translate the following to French: 'Hello, how are you?'\",\n    model=\"gpt-3.5-turbo\"\n)\n</code></pre>"},{"location":"usage/client/text_generation/#replicate","title":"Replicate","text":"<pre><code>response = replicate_client.generate_text(\n    \"Generate a haiku about mountains\",\n    model=\"meta/llama-2-70b-chat:latest\"\n)\n</code></pre>"},{"location":"usage/client/text_generation/#ollama","title":"Ollama","text":"<pre><code>response = ollama_client.generate_text(\n    \"Explain the concept of neural networks\",\n    model=\"llama2\"\n)\n</code></pre>"},{"location":"usage/client/text_generation/#best-practices","title":"Best Practices","text":"<ol> <li>Prompt Engineering: Craft clear and specific prompts for better results.</li> </ol> <pre><code>good_prompt = \"Write a detailed description of a futuristic city, focusing on transportation and architecture.\"\n</code></pre> <ol> <li> <p>Model Selection: Choose appropriate models based on your task complexity and requirements.</p> </li> <li> <p>Error Handling: Always handle potential errors in text generation:</p> </li> </ol> <pre><code>try:\n    response = client.generate_text(\"Your prompt here\", model=\"gpt-3.5-turbo\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n</code></pre> <ol> <li> <p>Rate Limiting: Be mindful of rate limits imposed by providers. Implement appropriate delays or queuing mechanisms for high-volume applications.</p> </li> <li> <p>Content Filtering: Implement content filtering or moderation for user-facing applications to ensure appropriate outputs.</p> </li> <li> <p>Consistency: For applications requiring consistent outputs, consider using lower temperature values or implementing your own post-processing.</p> </li> </ol> <p>By following these guidelines and exploring the various parameters and features available, you can effectively leverage ClientAI for a wide range of text generation tasks across different AI providers.</p>"}]}