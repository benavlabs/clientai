{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ClientAI","text":"<p> A unified client for seamless interaction with multiple AI providers. </p> <p> </p> <p> ClientAI is a Python package that provides a unified interface for interacting with multiple AI providers, including OpenAI, Replicate, and Ollama. It offers seamless integration and consistent methods for text generation and chat functionality across different AI platforms. </p>"},{"location":"#features","title":"Features","text":"<ul> <li>Unified Interface: Consistent methods for text generation and chat across multiple AI providers.</li> <li>Multiple Providers: Support for OpenAI, Replicate, and Ollama, with easy extensibility for future providers.</li> <li>Streaming Support: Efficient streaming of responses for real-time applications.</li> <li>Flexible Configuration: Easy setup with provider-specific configurations.</li> <li>Customizable: Extensible design for adding new providers or customizing existing ones.</li> <li>Type Hinting: Comprehensive type annotations for better development experience.</li> <li>Provider Isolation: Optional installation of provider-specific dependencies to keep your environment lean.</li> </ul>"},{"location":"#minimal-example","title":"Minimal Example","text":"<p>Here's a quick example to get you started with ClientAI:</p> <pre><code>from clientai import ClientAI\n\n# Initialize with OpenAI\nclient = ClientAI('openai', api_key=\"your-openai-key\")\n\n# Generate text\nresponse = client.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n)\n\nprint(response)\n\n# Chat functionality\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"Paris.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\n\nresponse = client.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n)\n\nprint(response)\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<p>Before installing ClientAI, ensure you have the following prerequisites:</p> <ul> <li>Python: Version 3.9 or newer.</li> <li>Dependencies: The core ClientAI package has minimal dependencies. Provider-specific packages (e.g., <code>openai</code>, <code>replicate</code>, <code>ollama</code>) are optional and can be installed separately.</li> </ul>"},{"location":"#installing","title":"Installing","text":"<p>To install ClientAI with all providers, run:</p> <pre><code>pip install clientai[all]\n</code></pre> <p>Or, if you prefer to install only specific providers:</p> <pre><code>pip install clientai[openai]  # For OpenAI support\npip install clientai[replicate]  # For Replicate support\npip install clientai[ollama]  # For Ollama support\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>ClientAI offers a consistent way to interact with different AI providers:</p> <ol> <li>Initialize the client with your chosen provider and credentials.</li> <li>Use the <code>generate_text</code> method for text generation tasks.</li> <li>Use the <code>chat</code> method for conversational interactions.</li> </ol> <p>Both methods support streaming responses and returning full response objects.</p> <p>For more detailed usage examples and advanced features, please refer to the Usage section of this documentation.</p>"},{"location":"#license","title":"License","text":"<p><code>MIT</code></p>"},{"location":"installing/","title":"Installing","text":""},{"location":"installing/#requirements","title":"Requirements","text":"<p>Before installing ClientAI, ensure you have the following prerequisites:</p> <ul> <li>Python: Version 3.9 or newer.</li> <li>Core Dependencies: ClientAI has minimal core dependencies, which will be automatically installed.</li> <li>Provider-Specific Libraries: Depending on which AI providers you plan to use, you may need to install additional libraries:<ul> <li>For OpenAI: <code>openai</code> library</li> <li>For Replicate: <code>replicate</code> library</li> <li>For Ollama: <code>ollama</code> library</li> </ul> </li> </ul>"},{"location":"installing/#installing_1","title":"Installing","text":"<p>ClientAI offers flexible installation options to suit your needs:</p>"},{"location":"installing/#basic-installation","title":"Basic Installation","text":"<p>To install the core ClientAI package without any provider-specific dependencies:</p> <pre><code>pip install clientai\n</code></pre> <p>Or, if using poetry:</p> <pre><code>poetry add clientai\n</code></pre>"},{"location":"installing/#installation-with-specific-providers","title":"Installation with Specific Providers","text":"<p>To install ClientAI with support for specific providers:</p> <pre><code>pip install clientai[openai]  # For OpenAI support\npip install clientai[replicate]  # For Replicate support\npip install clientai[ollama]  # For Ollama support\n</code></pre> <p>Or with poetry:</p> <pre><code>poetry add clientai[openai]\npoetry add clientai[replicate]\npoetry add clientai[ollama]\n</code></pre>"},{"location":"installing/#full-installation","title":"Full Installation","text":"<p>To install ClientAI with support for all providers:</p> <pre><code>pip install clientai[all]\n</code></pre> <p>Or with poetry:</p> <pre><code>poetry add clientai[all]\n</code></pre>"},{"location":"quick-start/","title":"Quickstart","text":"<p>This guide will help you get started with ClientAI quickly. We'll cover the basic setup and usage for each supported AI provider.</p>"},{"location":"quick-start/#minimal-example","title":"Minimal Example","text":"<p>Here's a minimal example to get you started with ClientAI:</p> quickstart.py<pre><code>from clientai import ClientAI\n\n# Initialize the client (example with OpenAI)\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Generate text\nresponse = client.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n)\nprint(response)\n\n# Use chat functionality\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n    {\"role\": \"user\", \"content\": \"What's its population?\"}\n]\nresponse = client.chat(messages, model=\"gpt-3.5-turbo\")\nprint(response)\n</code></pre>"},{"location":"quick-start/#setup-for-different-providers","title":"Setup for Different Providers","text":""},{"location":"quick-start/#openai","title":"OpenAI","text":"openai_setup.py<pre><code>from clientai import ClientAI\n\n# Initialize the OpenAI client\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Now you can use the client for text generation or chat\n</code></pre>"},{"location":"quick-start/#replicate","title":"Replicate","text":"replicate_setup.py<pre><code>from clientai import ClientAI\n\n# Initialize the Replicate client\nclient = ClientAI('replicate', api_key=\"your-replicate-api-key\")\n\n# Now you can use the client for text generation or chat\n</code></pre>"},{"location":"quick-start/#ollama","title":"Ollama","text":"ollama_setup.py<pre><code>from clientai import ClientAI\n\n# Initialize the Ollama client\nclient = ClientAI('ollama', host=\"your-ollama-host\")\n\n# Now you can use the client for text generation or chat\n</code></pre>"},{"location":"quick-start/#basic-usage","title":"Basic Usage","text":"<p>Once you have initialized the client, you can use it for text generation and chat functionality:</p>"},{"location":"quick-start/#text-generation","title":"Text Generation","text":"text_generation.py<pre><code>from clientai import ClientAI\n\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Generate text\nresponse = client.generate_text(\n    \"Explain the concept of quantum computing\",\n    model=\"gpt-3.5-turbo\",\n    max_tokens=100\n)\nprint(response)\n</code></pre>"},{"location":"quick-start/#chat","title":"Chat","text":"chat.py<pre><code>from clientai import ClientAI\n\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Use chat functionality\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n    {\"role\": \"assistant\", \"content\": \"Machine learning is a branch of artificial intelligence...\"},\n    {\"role\": \"user\", \"content\": \"Can you give an example of its application?\"}\n]\nresponse = client.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    max_tokens=150\n)\nprint(response)\n</code></pre>"},{"location":"quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've seen the basics of ClientAI, you can:</p> <ol> <li>Explore more advanced features like streaming responses and handling full response objects.</li> <li>Check out the Usage Guide for detailed information on all available methods and options.</li> <li>See the API Reference for a complete list of ClientAI's classes and methods.</li> </ol> <p>Remember to handle API keys securely and never expose them in your code or version control systems.</p>"},{"location":"api/ai_provider/","title":"AIProvider Class API Reference","text":"<p>The <code>AIProvider</code> class is an abstract base class that defines the interface for all AI provider implementations in ClientAI. It ensures consistency across different providers.</p>"},{"location":"api/ai_provider/#class-definition","title":"Class Definition","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for AI providers.</p> Source code in <code>clientai/ai_provider.py</code> <pre><code>class AIProvider(ABC):\n    \"\"\"\n    Abstract base class for AI providers.\n    \"\"\"\n\n    @abstractmethod\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs,\n    ) -&gt; GenericResponse:\n        \"\"\"\n        Generate text based on a given prompt.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the AI model to use.\n            return_full_response: If True, return the full response object\n                                  instead of just the generated text.\n            stream: If True, return an iterator for streaming responses.\n            **kwargs: Additional keyword arguments specific to\n                      the provider's API.\n\n        Returns:\n            GenericResponse:\n                The generated text response, full response object,\n                or an iterator for streaming responses.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs,\n    ) -&gt; GenericResponse:\n        \"\"\"\n        Engage in a chat conversation.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the AI model to use.\n            return_full_response: If True, return the full response object\n                                  instead of just the chat content.\n            stream: If True, return an iterator for streaming responses.\n            **kwargs: Additional keyword arguments specific to\n                      the provider's API.\n\n        Returns:\n            GenericResponse:\n                The chat response, either as a string, a dictionary,\n                or an iterator for streaming responses.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/ai_provider/#clientai.ai_provider.AIProvider.chat","title":"<code>chat(messages, model, return_full_response=False, stream=False, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Engage in a chat conversation.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object                   instead of just the chat content.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments specific to       the provider's API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GenericResponse</code> <code>GenericResponse</code> <p>The chat response, either as a string, a dictionary, or an iterator for streaming responses.</p> Source code in <code>clientai/ai_provider.py</code> <pre><code>@abstractmethod\ndef chat(\n    self,\n    messages: List[Message],\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs,\n) -&gt; GenericResponse:\n    \"\"\"\n    Engage in a chat conversation.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the AI model to use.\n        return_full_response: If True, return the full response object\n                              instead of just the chat content.\n        stream: If True, return an iterator for streaming responses.\n        **kwargs: Additional keyword arguments specific to\n                  the provider's API.\n\n    Returns:\n        GenericResponse:\n            The chat response, either as a string, a dictionary,\n            or an iterator for streaming responses.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ai_provider/#clientai.ai_provider.AIProvider.generate_text","title":"<code>generate_text(prompt, model, return_full_response=False, stream=False, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate text based on a given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object                   instead of just the generated text.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments specific to       the provider's API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GenericResponse</code> <code>GenericResponse</code> <p>The generated text response, full response object, or an iterator for streaming responses.</p> Source code in <code>clientai/ai_provider.py</code> <pre><code>@abstractmethod\ndef generate_text(\n    self,\n    prompt: str,\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs,\n) -&gt; GenericResponse:\n    \"\"\"\n    Generate text based on a given prompt.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the AI model to use.\n        return_full_response: If True, return the full response object\n                              instead of just the generated text.\n        stream: If True, return an iterator for streaming responses.\n        **kwargs: Additional keyword arguments specific to\n                  the provider's API.\n\n    Returns:\n        GenericResponse:\n            The generated text response, full response object,\n            or an iterator for streaming responses.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/clientai/","title":"ClientAI Class API Reference","text":"<p>The <code>ClientAI</code> class is the primary interface for interacting with various AI providers in a unified manner. It provides methods for text generation and chat functionality across different AI services.</p>"},{"location":"api/clientai/#class-definition","title":"Class Definition","text":"<p>             Bases: <code>Generic[P, T, S]</code></p> <p>A unified client for interacting with a single AI provider (OpenAI, Replicate, or Ollama).</p> <p>This class provides a consistent interface for common AI operations such as text generation and chat for the chosen AI provider.</p> <p>Type Parameters: P: The type of the AI provider. T: The type of the full response for non-streaming operations. S: The type of each chunk in streaming operations.</p> <p>Attributes:</p> Name Type Description <code>provider</code> <p>The initialized AI provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>The name of the AI provider to use            ('openai', 'replicate', or 'ollama').</p> required <code>**kwargs</code> <p>Provider-specific initialization parameters.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported provider name is given.</p> <code>ImportError</code> <p>If the specified provider is not installed.</p> <p>Examples:</p> <p>Initialize with OpenAI: <pre><code>ai = ClientAI('openai', api_key=\"your-openai-key\")\n</code></pre></p> <p>Initialize with Replicate: <pre><code>ai = ClientAI('replicate', api_key=\"your-replicate-key\")\n</code></pre></p> <p>Initialize with Ollama: <pre><code>ai = ClientAI('ollama', host=\"your-ollama-host\")\n</code></pre></p> Source code in <code>clientai/client_ai.py</code> <pre><code>class ClientAI(Generic[P, T, S]):\n    \"\"\"\n    A unified client for interacting with a single AI provider\n    (OpenAI, Replicate, or Ollama).\n\n    This class provides a consistent interface for common\n    AI operations such as text generation and chat\n    for the chosen AI provider.\n\n    Type Parameters:\n    P: The type of the AI provider.\n    T: The type of the full response for non-streaming operations.\n    S: The type of each chunk in streaming operations.\n\n    Attributes:\n        provider: The initialized AI provider.\n\n    Args:\n        provider_name: The name of the AI provider to use\n                       ('openai', 'replicate', or 'ollama').\n        **kwargs: Provider-specific initialization parameters.\n\n    Raises:\n        ValueError: If an unsupported provider name is given.\n        ImportError: If the specified provider is not installed.\n\n    Examples:\n        Initialize with OpenAI:\n        ```python\n        ai = ClientAI('openai', api_key=\"your-openai-key\")\n        ```\n\n        Initialize with Replicate:\n        ```python\n        ai = ClientAI('replicate', api_key=\"your-replicate-key\")\n        ```\n\n        Initialize with Ollama:\n        ```python\n        ai = ClientAI('ollama', host=\"your-ollama-host\")\n        ```\n    \"\"\"\n\n    def __init__(self, provider_name: str, **kwargs):\n        prov_name = provider_name\n        if prov_name not in [\"openai\", \"replicate\", \"ollama\"]:\n            raise ValueError(f\"Unsupported provider: {prov_name}\")\n\n        if (\n            prov_name == \"openai\"\n            and not OPENAI_INSTALLED\n            or prov_name == \"replicate\"\n            and not REPLICATE_INSTALLED\n            or prov_name == \"ollama\"\n            and not OLLAMA_INSTALLED\n        ):\n            raise ImportError(\n                f\"The {prov_name} provider is not installed. \"\n                f\"Please install it with 'pip install clientai[{prov_name}]'.\"\n            )\n\n        try:\n            provider_module = import_module(\n                f\".{prov_name}.provider\", package=\"clientai\"\n            )\n            provider_class = getattr(provider_module, \"Provider\")\n            if prov_name in [\"openai\", \"replicate\"]:\n                self.provider = cast(\n                    P, provider_class(api_key=kwargs.get(\"api_key\"))\n                )\n            elif prov_name == \"ollama\":\n                self.provider = cast(\n                    P, provider_class(host=kwargs.get(\"host\"))\n                )\n        except ImportError as e:\n            raise ImportError(\n                f\"Error importing {prov_name} provider module: {str(e)}\"\n            ) from e\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs,\n    ) -&gt; AIGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt\n        using the specified AI model and provider.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the AI model to use.\n            return_full_response: If True, returns the full structured response\n                                  If False, returns only the generated text.\n            stream: If True, returns an iterator for streaming responses.\n            **kwargs: Additional keyword arguments specific to\n                      the chosen provider's API.\n\n        Returns:\n            AIGenericResponse:\n                The generated text response, full response structure,\n                or an iterator for streaming responses.\n\n        Examples:\n            Generate text using OpenAI (text only):\n            ```python\n            response = ai.generate_text(\n                \"Tell me a joke\",\n                model=\"gpt-3.5-turbo\",\n            )\n            ```\n\n            Generate text using OpenAI (full response):\n            ```python\n            response = ai.generate_text(\n                \"Tell me a joke\",\n                model=\"gpt-3.5-turbo\",\n                return_full_response=True\n            )\n            ```\n\n            Generate text using OpenAI (streaming):\n            ```python\n            for chunk in ai.generate_text(\n                \"Tell me a joke\",\n                model=\"gpt-3.5-turbo\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n\n            Generate text using Replicate:\n            ```python\n            response = ai.generate_text(\n                \"Explain quantum computing\",\n                model=\"meta/llama-2-70b-chat:latest\",\n            )\n            ```\n\n            Generate text using Ollama:\n            ```python\n            response = ai.generate_text(\n                \"What is the capital of France?\",\n                model=\"llama2\",\n            )\n            ```\n        \"\"\"\n        return self.provider.generate_text(\n            prompt,\n            model,\n            return_full_response=return_full_response,\n            stream=stream,\n            **kwargs,\n        )\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs,\n    ) -&gt; AIGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using\n        the specified AI model and provider.\n\n        Args:\n            messages: A list of message dictionaries, each\n                      containing 'role' and 'content'.\n            model: The name or identifier of the AI model to use.\n            return_full_response: If True, returns the full structured response\n                                  If False, returns the assistant's message.\n            stream: If True, returns an iterator for streaming responses.\n            **kwargs: Additional keyword arguments specific to\n                      the chosen provider's API.\n\n        Returns:\n            AIGenericResponse:\n                The chat response, full response structure,\n                or an iterator for streaming responses.\n\n        Examples:\n            Chat using OpenAI (message content only):\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n                {\"role\": \"assistant\", \"content\": \"Paris.\"},\n                {\"role\": \"user\", \"content\": \"What is its population?\"}\n            ]\n            response = ai.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n            )\n            ```\n\n            Chat using OpenAI (full response):\n            ```python\n            response = ai.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n                return_full_response=True\n            )\n            ```\n\n            Chat using OpenAI (streaming):\n            ```python\n            for chunk in ai.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n\n            Chat using Replicate:\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"Explain the concept of AI.\"}\n            ]\n            response = ai.chat(\n                messages,\n                model=\"meta/llama-2-70b-chat:latest\",\n            )\n            ```\n\n            Chat using Ollama:\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What are the laws of robotics?\"}\n            ]\n            response = ai.chat(messages, model=\"llama2\")\n            ```\n        \"\"\"\n        return self.provider.chat(\n            messages,\n            model,\n            return_full_response=return_full_response,\n            stream=stream,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/clientai/#clientai.ClientAI.chat","title":"<code>chat(messages, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Engage in a chat conversation using the specified AI model and provider.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each       containing 'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, returns the full structured response                   If False, returns the assistant's message.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, returns an iterator for streaming responses.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments specific to       the chosen provider's API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AIGenericResponse</code> <code>AIGenericResponse</code> <p>The chat response, full response structure, or an iterator for streaming responses.</p> <p>Examples:</p> <p>Chat using OpenAI (message content only): <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"Paris.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\nresponse = ai.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n)\n</code></pre></p> <p>Chat using OpenAI (full response): <pre><code>response = ai.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    return_full_response=True\n)\n</code></pre></p> <p>Chat using OpenAI (streaming): <pre><code>for chunk in ai.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> <p>Chat using Replicate: <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"Explain the concept of AI.\"}\n]\nresponse = ai.chat(\n    messages,\n    model=\"meta/llama-2-70b-chat:latest\",\n)\n</code></pre></p> <p>Chat using Ollama: <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What are the laws of robotics?\"}\n]\nresponse = ai.chat(messages, model=\"llama2\")\n</code></pre></p> Source code in <code>clientai/client_ai.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs,\n) -&gt; AIGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using\n    the specified AI model and provider.\n\n    Args:\n        messages: A list of message dictionaries, each\n                  containing 'role' and 'content'.\n        model: The name or identifier of the AI model to use.\n        return_full_response: If True, returns the full structured response\n                              If False, returns the assistant's message.\n        stream: If True, returns an iterator for streaming responses.\n        **kwargs: Additional keyword arguments specific to\n                  the chosen provider's API.\n\n    Returns:\n        AIGenericResponse:\n            The chat response, full response structure,\n            or an iterator for streaming responses.\n\n    Examples:\n        Chat using OpenAI (message content only):\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n            {\"role\": \"assistant\", \"content\": \"Paris.\"},\n            {\"role\": \"user\", \"content\": \"What is its population?\"}\n        ]\n        response = ai.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n        )\n        ```\n\n        Chat using OpenAI (full response):\n        ```python\n        response = ai.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n            return_full_response=True\n        )\n        ```\n\n        Chat using OpenAI (streaming):\n        ```python\n        for chunk in ai.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n\n        Chat using Replicate:\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"Explain the concept of AI.\"}\n        ]\n        response = ai.chat(\n            messages,\n            model=\"meta/llama-2-70b-chat:latest\",\n        )\n        ```\n\n        Chat using Ollama:\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What are the laws of robotics?\"}\n        ]\n        response = ai.chat(messages, model=\"llama2\")\n        ```\n    \"\"\"\n    return self.provider.chat(\n        messages,\n        model,\n        return_full_response=return_full_response,\n        stream=stream,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/clientai/#clientai.ClientAI.generate_text","title":"<code>generate_text(prompt, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Generate text based on a given prompt using the specified AI model and provider.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, returns the full structured response                   If False, returns only the generated text.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, returns an iterator for streaming responses.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments specific to       the chosen provider's API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AIGenericResponse</code> <code>AIGenericResponse</code> <p>The generated text response, full response structure, or an iterator for streaming responses.</p> <p>Examples:</p> <p>Generate text using OpenAI (text only): <pre><code>response = ai.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n)\n</code></pre></p> <p>Generate text using OpenAI (full response): <pre><code>response = ai.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n    return_full_response=True\n)\n</code></pre></p> <p>Generate text using OpenAI (streaming): <pre><code>for chunk in ai.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> <p>Generate text using Replicate: <pre><code>response = ai.generate_text(\n    \"Explain quantum computing\",\n    model=\"meta/llama-2-70b-chat:latest\",\n)\n</code></pre></p> <p>Generate text using Ollama: <pre><code>response = ai.generate_text(\n    \"What is the capital of France?\",\n    model=\"llama2\",\n)\n</code></pre></p> Source code in <code>clientai/client_ai.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs,\n) -&gt; AIGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt\n    using the specified AI model and provider.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the AI model to use.\n        return_full_response: If True, returns the full structured response\n                              If False, returns only the generated text.\n        stream: If True, returns an iterator for streaming responses.\n        **kwargs: Additional keyword arguments specific to\n                  the chosen provider's API.\n\n    Returns:\n        AIGenericResponse:\n            The generated text response, full response structure,\n            or an iterator for streaming responses.\n\n    Examples:\n        Generate text using OpenAI (text only):\n        ```python\n        response = ai.generate_text(\n            \"Tell me a joke\",\n            model=\"gpt-3.5-turbo\",\n        )\n        ```\n\n        Generate text using OpenAI (full response):\n        ```python\n        response = ai.generate_text(\n            \"Tell me a joke\",\n            model=\"gpt-3.5-turbo\",\n            return_full_response=True\n        )\n        ```\n\n        Generate text using OpenAI (streaming):\n        ```python\n        for chunk in ai.generate_text(\n            \"Tell me a joke\",\n            model=\"gpt-3.5-turbo\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n\n        Generate text using Replicate:\n        ```python\n        response = ai.generate_text(\n            \"Explain quantum computing\",\n            model=\"meta/llama-2-70b-chat:latest\",\n        )\n        ```\n\n        Generate text using Ollama:\n        ```python\n        response = ai.generate_text(\n            \"What is the capital of France?\",\n            model=\"llama2\",\n        )\n        ```\n    \"\"\"\n    return self.provider.generate_text(\n        prompt,\n        model,\n        return_full_response=return_full_response,\n        stream=stream,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/ollama_provider/","title":"Ollama Provider API Reference","text":"<p>The <code>OllamaProvider</code> class implements the <code>AIProvider</code> interface for the Ollama service. It provides methods for text generation and chat functionality using locally hosted models through Ollama.</p>"},{"location":"api/ollama_provider/#class-definition","title":"Class Definition","text":"<p>             Bases: <code>AIProvider</code></p> <p>Ollama-specific implementation of the AIProvider abstract base class.</p> <p>This class provides methods to interact with Ollama's models for text generation and chat functionality.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>OllamaClientProtocol</code> <p>The Ollama client used for making API calls.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>Optional[str]</code> <p>The host address for the Ollama server. If not provided, the default Ollama client will be used.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the Ollama package is not installed.</p> <p>Examples:</p> <p>Initialize the Ollama provider: <pre><code>provider = Provider(host=\"http://localhost:11434\")\n</code></pre></p> Source code in <code>clientai/ollama/provider.py</code> <pre><code>class Provider(AIProvider):\n    \"\"\"\n    Ollama-specific implementation of the AIProvider abstract base class.\n\n    This class provides methods to interact with Ollama's models for\n    text generation and chat functionality.\n\n    Attributes:\n        client: The Ollama client used for making API calls.\n\n    Args:\n        host: The host address for the Ollama server.\n            If not provided, the default Ollama client will be used.\n\n    Raises:\n        ImportError: If the Ollama package is not installed.\n\n    Examples:\n        Initialize the Ollama provider:\n        ```python\n        provider = Provider(host=\"http://localhost:11434\")\n        ```\n    \"\"\"\n\n    def __init__(self, host: Optional[str] = None):\n        if not OLLAMA_INSTALLED or Client is None:\n            raise ImportError(\n                \"The ollama package is not installed. \"\n                \"Please install it with 'pip install clientai[ollama]'.\"\n            )\n        self.client: OllamaClientProtocol = cast(\n            OllamaClientProtocol, Client(host=host) if host else ollama\n        )\n\n    def _stream_generate_response(\n        self,\n        stream: Iterator[OllamaStreamResponse],\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, OllamaStreamResponse]]:\n        \"\"\"\n        Process the streaming response from Ollama API for text generation.\n\n        Args:\n            stream: The stream of responses from Ollama API.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, OllamaStreamResponse]: Processed content or\n                                              full response objects.\n        \"\"\"\n        for chunk in stream:\n            if return_full_response:\n                yield chunk\n            else:\n                yield chunk[\"response\"]\n\n    def _stream_chat_response(\n        self,\n        stream: Iterator[OllamaChatResponse],\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, OllamaChatResponse]]:\n        \"\"\"\n        Process the streaming response from Ollama API for chat.\n\n        Args:\n            stream: The stream of responses from Ollama API.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, OllamaChatResponse]: Processed content or\n                                            full response objects.\n        \"\"\"\n        for chunk in stream:\n            if return_full_response:\n                yield chunk\n            else:\n                yield chunk[\"message\"][\"content\"]\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; OllamaGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt using a specified Ollama model.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the Ollama model to use.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            **kwargs: Additional keyword arguments to pass to the Ollama API.\n\n        Returns:\n            OllamaGenericResponse: The generated text, full response object,\n            or an iterator for streaming responses.\n\n        Examples:\n            Generate text (text only):\n            ```python\n            response = provider.generate_text(\n                \"Explain the concept of machine learning\",\n                model=\"llama2\",\n            )\n            print(response)\n            ```\n\n            Generate text (full response):\n            ```python\n            response = provider.generate_text(\n                \"Explain the concept of machine learning\",\n                model=\"llama2\",\n                return_full_response=True\n            )\n            print(response[\"response\"])\n            ```\n\n            Generate text (streaming):\n            ```python\n            for chunk in provider.generate_text(\n                \"Explain the concept of machine learning\",\n                model=\"llama2\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        response = self.client.generate(\n            model=model, prompt=prompt, stream=stream, **kwargs\n        )\n\n        if stream:\n            return cast(\n                OllamaGenericResponse,\n                self._stream_generate_response(\n                    cast(Iterator[OllamaStreamResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(OllamaResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return response[\"response\"]\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; OllamaGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using a specified Ollama model.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the Ollama model to use.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            **kwargs: Additional keyword arguments to pass to the Ollama API.\n\n        Returns:\n            OllamaGenericResponse: The chat response, full response object,\n            or an iterator for streaming responses.\n\n        Examples:\n            Chat (message content only):\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is the capital of Japan?\"},\n                {\"role\": \"assistant\", \"content\": \"The capital is Tokyo.\"},\n                {\"role\": \"user\", \"content\": \"What is its population?\"}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"llama2\",\n            )\n            print(response)\n            ```\n\n            Chat (full response):\n            ```python\n            response = provider.chat(\n                messages,\n                model=\"llama2\",\n                return_full_response=True\n            )\n            print(response[\"message\"][\"content\"])\n            ```\n\n            Chat (streaming):\n            ```python\n            for chunk in provider.chat(\n                messages,\n                model=\"llama2\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        response = self.client.chat(\n            model=model, messages=messages, stream=stream, **kwargs\n        )\n\n        if stream:\n            return cast(\n                OllamaGenericResponse,\n                self._stream_chat_response(\n                    cast(Iterator[OllamaChatResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(OllamaChatResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return response[\"message\"][\"content\"]\n</code></pre>"},{"location":"api/ollama_provider/#clientai.ollama.Provider.chat","title":"<code>chat(messages, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Engage in a chat conversation using a specified Ollama model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Ollama model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the Ollama API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OllamaGenericResponse</code> <code>OllamaGenericResponse</code> <p>The chat response, full response object,</p> <code>OllamaGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Examples:</p> <p>Chat (message content only): <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of Japan?\"},\n    {\"role\": \"assistant\", \"content\": \"The capital is Tokyo.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\nresponse = provider.chat(\n    messages,\n    model=\"llama2\",\n)\nprint(response)\n</code></pre></p> <p>Chat (full response): <pre><code>response = provider.chat(\n    messages,\n    model=\"llama2\",\n    return_full_response=True\n)\nprint(response[\"message\"][\"content\"])\n</code></pre></p> <p>Chat (streaming): <pre><code>for chunk in provider.chat(\n    messages,\n    model=\"llama2\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/ollama/provider.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; OllamaGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using a specified Ollama model.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the Ollama model to use.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the Ollama API.\n\n    Returns:\n        OllamaGenericResponse: The chat response, full response object,\n        or an iterator for streaming responses.\n\n    Examples:\n        Chat (message content only):\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is the capital of Japan?\"},\n            {\"role\": \"assistant\", \"content\": \"The capital is Tokyo.\"},\n            {\"role\": \"user\", \"content\": \"What is its population?\"}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"llama2\",\n        )\n        print(response)\n        ```\n\n        Chat (full response):\n        ```python\n        response = provider.chat(\n            messages,\n            model=\"llama2\",\n            return_full_response=True\n        )\n        print(response[\"message\"][\"content\"])\n        ```\n\n        Chat (streaming):\n        ```python\n        for chunk in provider.chat(\n            messages,\n            model=\"llama2\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    response = self.client.chat(\n        model=model, messages=messages, stream=stream, **kwargs\n    )\n\n    if stream:\n        return cast(\n            OllamaGenericResponse,\n            self._stream_chat_response(\n                cast(Iterator[OllamaChatResponse], response),\n                return_full_response,\n            ),\n        )\n    else:\n        response = cast(OllamaChatResponse, response)\n        if return_full_response:\n            return response\n        else:\n            return response[\"message\"][\"content\"]\n</code></pre>"},{"location":"api/ollama_provider/#clientai.ollama.Provider.generate_text","title":"<code>generate_text(prompt, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Generate text based on a given prompt using a specified Ollama model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Ollama model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the Ollama API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OllamaGenericResponse</code> <code>OllamaGenericResponse</code> <p>The generated text, full response object,</p> <code>OllamaGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Examples:</p> <p>Generate text (text only): <pre><code>response = provider.generate_text(\n    \"Explain the concept of machine learning\",\n    model=\"llama2\",\n)\nprint(response)\n</code></pre></p> <p>Generate text (full response): <pre><code>response = provider.generate_text(\n    \"Explain the concept of machine learning\",\n    model=\"llama2\",\n    return_full_response=True\n)\nprint(response[\"response\"])\n</code></pre></p> <p>Generate text (streaming): <pre><code>for chunk in provider.generate_text(\n    \"Explain the concept of machine learning\",\n    model=\"llama2\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/ollama/provider.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; OllamaGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt using a specified Ollama model.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the Ollama model to use.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the Ollama API.\n\n    Returns:\n        OllamaGenericResponse: The generated text, full response object,\n        or an iterator for streaming responses.\n\n    Examples:\n        Generate text (text only):\n        ```python\n        response = provider.generate_text(\n            \"Explain the concept of machine learning\",\n            model=\"llama2\",\n        )\n        print(response)\n        ```\n\n        Generate text (full response):\n        ```python\n        response = provider.generate_text(\n            \"Explain the concept of machine learning\",\n            model=\"llama2\",\n            return_full_response=True\n        )\n        print(response[\"response\"])\n        ```\n\n        Generate text (streaming):\n        ```python\n        for chunk in provider.generate_text(\n            \"Explain the concept of machine learning\",\n            model=\"llama2\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    response = self.client.generate(\n        model=model, prompt=prompt, stream=stream, **kwargs\n    )\n\n    if stream:\n        return cast(\n            OllamaGenericResponse,\n            self._stream_generate_response(\n                cast(Iterator[OllamaStreamResponse], response),\n                return_full_response,\n            ),\n        )\n    else:\n        response = cast(OllamaResponse, response)\n        if return_full_response:\n            return response\n        else:\n            return response[\"response\"]\n</code></pre>"},{"location":"api/openai_provider/","title":"OpenAI Provider API Reference","text":"<p>The <code>OpenAIProvider</code> class implements the <code>AIProvider</code> interface for the OpenAI service. It provides methods for text generation and chat functionality using OpenAI's models.</p>"},{"location":"api/openai_provider/#class-definition","title":"Class Definition","text":"<p>             Bases: <code>AIProvider</code></p> <p>OpenAI-specific implementation of the AIProvider abstract base class.</p> <p>This class provides methods to interact with OpenAI's models for text generation and chat functionality.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>OpenAIClientProtocol</code> <p>The OpenAI client used for making API calls.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for authenticating with OpenAI.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the OpenAI package is not installed.</p> <p>Examples:</p> <p>Initialize the OpenAI provider: <pre><code>provider = Provider(api_key=\"your-openai-api-key\")\n</code></pre></p> Source code in <code>clientai/openai/provider.py</code> <pre><code>class Provider(AIProvider):\n    \"\"\"\n    OpenAI-specific implementation of the AIProvider abstract base class.\n\n    This class provides methods to interact with OpenAI's\n    models for text generation and chat functionality.\n\n    Attributes:\n        client: The OpenAI client used for making API calls.\n\n    Args:\n        api_key: The API key for authenticating with OpenAI.\n\n    Raises:\n        ImportError: If the OpenAI package is not installed.\n\n    Examples:\n        Initialize the OpenAI provider:\n        ```python\n        provider = Provider(api_key=\"your-openai-api-key\")\n        ```\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        if not OPENAI_INSTALLED or Client is None:\n            raise ImportError(\n                \"The openai package is not installed. \"\n                \"Please install it with 'pip install clientai[openai]'.\"\n            )\n        self.client: OpenAIClientProtocol = cast(\n            OpenAIClientProtocol, Client(api_key=api_key)\n        )\n\n    def _stream_response(\n        self,\n        stream: Iterator[OpenAIStreamResponse],\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, OpenAIStreamResponse]]:\n        \"\"\"\n        Process the streaming response from OpenAI API.\n\n        Args:\n            stream: The stream of responses from OpenAI API.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, OpenAIStreamResponse]: Processed content or full\n                                              response objects.\n        \"\"\"\n        for chunk in stream:\n            if return_full_response:\n                yield chunk\n            else:\n                content = chunk[\"choices\"][0][\"delta\"].get(\"content\")\n                if content:\n                    yield content\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; OpenAIGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt using a specified OpenAI model.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the OpenAI model to use.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            **kwargs: Additional keyword arguments to pass to the OpenAI API.\n\n        Returns:\n            OpenAIGenericResponse: The generated text, full response object,\n            or an iterator for streaming responses.\n\n        Examples:\n            Generate text (text only):\n            ```python\n            response = provider.generate_text(\n                \"Explain the theory of relativity\",\n                model=\"gpt-3.5-turbo\",\n            )\n            print(response)\n            ```\n\n            Generate text (full response):\n            ```python\n            response = provider.generate_text(\n                \"Explain the theory of relativity\",\n                model=\"gpt-3.5-turbo\",\n                return_full_response=True\n            )\n            print(response[\"choices\"][0][\"message\"][\"content\"])\n            ```\n\n            Generate text (streaming):\n            ```python\n            for chunk in provider.generate_text(\n                \"Explain the theory of relativity\",\n                model=\"gpt-3.5-turbo\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            stream=stream,\n            **kwargs,\n        )\n\n        if stream:\n            return cast(\n                OpenAIGenericResponse,\n                self._stream_response(\n                    cast(Iterator[OpenAIStreamResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(OpenAIResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return response[\"choices\"][0][\"message\"][\"content\"]\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; OpenAIGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using a specified OpenAI model.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the OpenAI model to use.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            **kwargs: Additional keyword arguments to pass to the OpenAI API.\n\n        Returns:\n            OpenAIGenericResponse: The chat response, full response object,\n            or an iterator for streaming responses.\n\n        Examples:\n            Chat (message content only):\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n                {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n                {\"role\": \"user\", \"content\": \"What is its population?\"}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n            )\n            print(response)\n            ```\n\n            Chat (full response):\n            ```python\n            response = provider.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n                return_full_response=True\n            )\n            print(response[\"choices\"][0][\"message\"][\"content\"])\n            ```\n\n            Chat (streaming):\n            ```python\n            for chunk in provider.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=model, messages=messages, stream=stream, **kwargs\n        )\n\n        if stream:\n            return cast(\n                OpenAIGenericResponse,\n                self._stream_response(\n                    cast(Iterator[OpenAIStreamResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(OpenAIResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return response[\"choices\"][0][\"message\"][\"content\"]\n</code></pre>"},{"location":"api/openai_provider/#clientai.openai.Provider.chat","title":"<code>chat(messages, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Engage in a chat conversation using a specified OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the OpenAI model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OpenAIGenericResponse</code> <code>OpenAIGenericResponse</code> <p>The chat response, full response object,</p> <code>OpenAIGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Examples:</p> <p>Chat (message content only): <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\nresponse = provider.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n)\nprint(response)\n</code></pre></p> <p>Chat (full response): <pre><code>response = provider.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    return_full_response=True\n)\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</code></pre></p> <p>Chat (streaming): <pre><code>for chunk in provider.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/openai/provider.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; OpenAIGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using a specified OpenAI model.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the OpenAI model to use.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the OpenAI API.\n\n    Returns:\n        OpenAIGenericResponse: The chat response, full response object,\n        or an iterator for streaming responses.\n\n    Examples:\n        Chat (message content only):\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n            {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n            {\"role\": \"user\", \"content\": \"What is its population?\"}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n        )\n        print(response)\n        ```\n\n        Chat (full response):\n        ```python\n        response = provider.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n            return_full_response=True\n        )\n        print(response[\"choices\"][0][\"message\"][\"content\"])\n        ```\n\n        Chat (streaming):\n        ```python\n        for chunk in provider.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    response = self.client.chat.completions.create(\n        model=model, messages=messages, stream=stream, **kwargs\n    )\n\n    if stream:\n        return cast(\n            OpenAIGenericResponse,\n            self._stream_response(\n                cast(Iterator[OpenAIStreamResponse], response),\n                return_full_response,\n            ),\n        )\n    else:\n        response = cast(OpenAIResponse, response)\n        if return_full_response:\n            return response\n        else:\n            return response[\"choices\"][0][\"message\"][\"content\"]\n</code></pre>"},{"location":"api/openai_provider/#clientai.openai.Provider.generate_text","title":"<code>generate_text(prompt, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Generate text based on a given prompt using a specified OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the OpenAI model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OpenAIGenericResponse</code> <code>OpenAIGenericResponse</code> <p>The generated text, full response object,</p> <code>OpenAIGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Examples:</p> <p>Generate text (text only): <pre><code>response = provider.generate_text(\n    \"Explain the theory of relativity\",\n    model=\"gpt-3.5-turbo\",\n)\nprint(response)\n</code></pre></p> <p>Generate text (full response): <pre><code>response = provider.generate_text(\n    \"Explain the theory of relativity\",\n    model=\"gpt-3.5-turbo\",\n    return_full_response=True\n)\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</code></pre></p> <p>Generate text (streaming): <pre><code>for chunk in provider.generate_text(\n    \"Explain the theory of relativity\",\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/openai/provider.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; OpenAIGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt using a specified OpenAI model.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the OpenAI model to use.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the OpenAI API.\n\n    Returns:\n        OpenAIGenericResponse: The generated text, full response object,\n        or an iterator for streaming responses.\n\n    Examples:\n        Generate text (text only):\n        ```python\n        response = provider.generate_text(\n            \"Explain the theory of relativity\",\n            model=\"gpt-3.5-turbo\",\n        )\n        print(response)\n        ```\n\n        Generate text (full response):\n        ```python\n        response = provider.generate_text(\n            \"Explain the theory of relativity\",\n            model=\"gpt-3.5-turbo\",\n            return_full_response=True\n        )\n        print(response[\"choices\"][0][\"message\"][\"content\"])\n        ```\n\n        Generate text (streaming):\n        ```python\n        for chunk in provider.generate_text(\n            \"Explain the theory of relativity\",\n            model=\"gpt-3.5-turbo\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    response = self.client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        stream=stream,\n        **kwargs,\n    )\n\n    if stream:\n        return cast(\n            OpenAIGenericResponse,\n            self._stream_response(\n                cast(Iterator[OpenAIStreamResponse], response),\n                return_full_response,\n            ),\n        )\n    else:\n        response = cast(OpenAIResponse, response)\n        if return_full_response:\n            return response\n        else:\n            return response[\"choices\"][0][\"message\"][\"content\"]\n</code></pre>"},{"location":"api/overview/","title":"API Reference Overview","text":"<p>Welcome to the API Reference section of ClientAI documentation. This section provides detailed information about the various classes, functions, and modules that make up ClientAI. Whether you're looking to integrate ClientAI into your project, extend its functionality, or simply explore its capabilities, this section will guide you through the intricacies of our codebase.</p>"},{"location":"api/overview/#key-components","title":"Key Components","text":"<p>ClientAI's API is comprised of several key components, each serving a specific purpose:</p> <ol> <li> <p>ClientAI Class: This is the main class of our library. It provides a unified interface for interacting with different AI providers and is the primary entry point for using ClientAI.</p> <ul> <li>ClientAI Class Reference</li> </ul> </li> <li> <p>AIProvider Class: An abstract base class that defines the interface for all AI provider implementations. It ensures consistency across different providers.</p> <ul> <li>AIProvider Class Reference</li> </ul> </li> <li> <p>Provider-Specific Classes: These classes implement the AIProvider interface for each supported AI service (OpenAI, Replicate, Ollama).</p> <ul> <li>OpenAI Provider Reference</li> <li>Replicate Provider Reference</li> <li>Ollama Provider Reference</li> </ul> </li> </ol>"},{"location":"api/overview/#usage","title":"Usage","text":"<p>Each component is documented with its own dedicated page, where you can find detailed information about its methods, parameters, return types, and usage examples. These pages are designed to provide you with all the information you need to understand and work with ClientAI effectively.</p>"},{"location":"api/overview/#basic-usage-example","title":"Basic Usage Example","text":"<p>Here's a quick example of how to use the main ClientAI class:</p> <pre><code>from clientai import ClientAI\n\n# Initialize the client\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Generate text\nresponse = client.generate_text(\n    \"Explain quantum computing\",\n    model=\"gpt-3.5-turbo\"\n)\n\nprint(response)\n</code></pre> <p>For more detailed usage instructions and examples, please refer to the Usage Guide.</p>"},{"location":"api/overview/#extending-clientai","title":"Extending ClientAI","text":"<p>If you wish to add support for a new AI provider or extend the functionality of existing providers, you can do so by implementing the AIProvider interface. See the Extending ClientAI Guide for more information.</p>"},{"location":"api/overview/#contribution","title":"Contribution","text":"<p>We welcome contributions to ClientAI! If you're interested in contributing, please refer to our Contributing Guidelines. Contributions can range from bug fixes and documentation improvements to adding support for new AI providers.</p>"},{"location":"api/overview/#feedback","title":"Feedback","text":"<p>Your feedback is crucial in helping us improve ClientAI and its documentation. If you have any suggestions, corrections, or queries, please don't hesitate to reach out to us via GitHub issues or our community channels.</p> <p>Navigate through each section for detailed documentation of ClientAI's API components.</p>"},{"location":"api/replicate_provider/","title":"Replicate Provider API Reference","text":"<p>The <code>ReplicateProvider</code> class implements the <code>AIProvider</code> interface for the Replicate service. It provides methods for text generation and chat functionality using models hosted on Replicate.</p>"},{"location":"api/replicate_provider/#class-definition","title":"Class Definition","text":"<p>             Bases: <code>AIProvider</code></p> <p>Replicate-specific implementation of the AIProvider abstract base class.</p> <p>This class provides methods to interact with Replicate's AI models for text generation and chat functionality.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>ReplicateClientProtocol</code> <p>The Replicate client used for making API calls.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for authenticating with Replicate.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the Replicate package is not installed.</p> <p>Examples:</p> <p>Initialize the Replicate provider: <pre><code>provider = Provider(api_key=\"your-replicate-api-key\")\n</code></pre></p> Source code in <code>clientai/replicate/provider.py</code> <pre><code>class Provider(AIProvider):\n    \"\"\"\n    Replicate-specific implementation of the AIProvider abstract base class.\n\n    This class provides methods to interact with Replicate's AI models for\n    text generation and chat functionality.\n\n    Attributes:\n        client: The Replicate client used for making API calls.\n\n    Args:\n        api_key: The API key for authenticating with Replicate.\n\n    Raises:\n        ImportError: If the Replicate package is not installed.\n\n    Examples:\n        Initialize the Replicate provider:\n        ```python\n        provider = Provider(api_key=\"your-replicate-api-key\")\n        ```\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        if not REPLICATE_INSTALLED or Client is None:\n            raise ImportError(\n                \"The replicate package is not installed. \"\n                \"Please install it with 'pip install clientai[replicate]'.\"\n            )\n        self.client: ReplicateClientProtocol = Client(api_token=api_key)\n\n    def _process_output(self, output: Any) -&gt; str:\n        \"\"\"\n        Process the output from Replicate API into a string format.\n\n        Args:\n            output: The raw output from Replicate API.\n\n        Returns:\n            str: The processed output as a string.\n        \"\"\"\n        if isinstance(output, List):\n            return \"\".join(str(item) for item in output)\n        elif isinstance(output, str):\n            return output\n        else:\n            return str(output)\n\n    def _wait_for_prediction(\n        self, prediction_id: str, max_wait_time: int = 300\n    ) -&gt; ReplicatePredictionProtocol:\n        \"\"\"\n        Wait for a prediction to complete or fail.\n\n        Args:\n            prediction_id: The ID of the prediction to wait for.\n            max_wait_time: Maximum time to wait in seconds. Defaults to 300.\n\n        Returns:\n            ReplicatePredictionProtocol: The completed prediction.\n\n        Raises:\n            TimeoutError: If the prediction doesn't complete within\n                          the max_wait_time.\n            Exception: If the prediction fails.\n        \"\"\"\n        start_time = time.time()\n        while time.time() - start_time &lt; max_wait_time:\n            prediction = self.client.predictions.get(prediction_id)\n            if prediction.status == \"succeeded\":\n                return prediction\n            elif prediction.status == \"failed\":\n                raise Exception(f\"Prediction failed: {prediction.error}\")\n            time.sleep(1)\n        raise TimeoutError(\"Prediction timed out\")\n\n    def _stream_response(\n        self,\n        prediction: ReplicatePredictionProtocol,\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, ReplicateStreamResponse]]:\n        \"\"\"\n        Stream the response from a prediction.\n\n        Args:\n            prediction: The prediction to stream.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, ReplicateStreamResponse]: Processed output or\n                                                 full response objects.\n        \"\"\"\n        metadata = cast(ReplicateStreamResponse, prediction.__dict__.copy())\n        for event in prediction.stream():\n            if return_full_response:\n                metadata[\"output\"] = self._process_output(event)\n                yield metadata\n            else:\n                yield self._process_output(event)\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; ReplicateGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt\n        using a specified Replicate model.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the Replicate model to use.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            **kwargs: Additional keyword arguments\n                      to pass to the Replicate API.\n\n        Returns:\n            ReplicateGenericResponse: The generated text, full response object,\n            or an iterator for streaming responses.\n\n        Examples:\n            Generate text (text only):\n            ```python\n            response = provider.generate_text(\n                \"Explain quantum computing\",\n                model=\"meta/llama-2-70b-chat:latest\",\n            )\n            print(response)\n            ```\n\n            Generate text (full response):\n            ```python\n            response = provider.generate_text(\n                \"Explain quantum computing\",\n                model=\"meta/llama-2-70b-chat:latest\",\n                return_full_response=True\n            )\n            print(response[\"output\"])\n            ```\n\n            Generate text (streaming):\n            ```python\n            for chunk in provider.generate_text(\n                \"Explain quantum computing\",\n                model=\"meta/llama-2-70b-chat:latest\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        prediction = self.client.predictions.create(\n            model=model, input={\"prompt\": prompt}, stream=stream, **kwargs\n        )\n\n        if stream:\n            return self._stream_response(prediction, return_full_response)\n        else:\n            completed_prediction = self._wait_for_prediction(prediction.id)\n            if return_full_response:\n                response = cast(\n                    ReplicateResponse, completed_prediction.__dict__.copy()\n                )\n                response[\"output\"] = self._process_output(\n                    completed_prediction.output\n                )\n                return response\n            else:\n                return self._process_output(completed_prediction.output)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; ReplicateGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using a specified Replicate model.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the Replicate model to use.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            **kwargs: Additional keyword arguments\n                      to pass to the Replicate API.\n\n        Returns:\n            ReplicateGenericResponse: The chat response, full response object,\n            or an iterator for streaming responses.\n\n        Examples:\n            Chat (message content only):\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n                {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n                {\"role\": \"user\", \"content\": \"What is its population?\"}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"meta/llama-2-70b-chat:latest\",\n            )\n            print(response)\n            ```\n\n            Chat (full response):\n            ```python\n            response = provider.chat(\n                messages,\n                model=\"meta/llama-2-70b-chat:latest\",\n                return_full_response=True\n            )\n            print(response[\"output\"])\n            ```\n\n            Chat (streaming):\n            ```python\n            for chunk in provider.chat(\n                messages,\n                model=\"meta/llama-2-70b-chat:latest\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        prompt = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n        prompt += \"\\nassistant: \"\n\n        prediction = self.client.predictions.create(\n            model=model, input={\"prompt\": prompt}, stream=stream, **kwargs\n        )\n\n        if stream:\n            return self._stream_response(prediction, return_full_response)\n        else:\n            completed_prediction = self._wait_for_prediction(prediction.id)\n            if return_full_response:\n                response = cast(\n                    ReplicateResponse, completed_prediction.__dict__.copy()\n                )\n                response[\"output\"] = self._process_output(\n                    completed_prediction.output\n                )\n                return response\n            else:\n                return self._process_output(completed_prediction.output)\n</code></pre>"},{"location":"api/replicate_provider/#clientai.replicate.Provider.chat","title":"<code>chat(messages, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Engage in a chat conversation using a specified Replicate model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Replicate model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments       to pass to the Replicate API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ReplicateGenericResponse</code> <code>ReplicateGenericResponse</code> <p>The chat response, full response object,</p> <code>ReplicateGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Examples:</p> <p>Chat (message content only): <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\nresponse = provider.chat(\n    messages,\n    model=\"meta/llama-2-70b-chat:latest\",\n)\nprint(response)\n</code></pre></p> <p>Chat (full response): <pre><code>response = provider.chat(\n    messages,\n    model=\"meta/llama-2-70b-chat:latest\",\n    return_full_response=True\n)\nprint(response[\"output\"])\n</code></pre></p> <p>Chat (streaming): <pre><code>for chunk in provider.chat(\n    messages,\n    model=\"meta/llama-2-70b-chat:latest\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/replicate/provider.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; ReplicateGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using a specified Replicate model.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the Replicate model to use.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        **kwargs: Additional keyword arguments\n                  to pass to the Replicate API.\n\n    Returns:\n        ReplicateGenericResponse: The chat response, full response object,\n        or an iterator for streaming responses.\n\n    Examples:\n        Chat (message content only):\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n            {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n            {\"role\": \"user\", \"content\": \"What is its population?\"}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"meta/llama-2-70b-chat:latest\",\n        )\n        print(response)\n        ```\n\n        Chat (full response):\n        ```python\n        response = provider.chat(\n            messages,\n            model=\"meta/llama-2-70b-chat:latest\",\n            return_full_response=True\n        )\n        print(response[\"output\"])\n        ```\n\n        Chat (streaming):\n        ```python\n        for chunk in provider.chat(\n            messages,\n            model=\"meta/llama-2-70b-chat:latest\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    prompt = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n    prompt += \"\\nassistant: \"\n\n    prediction = self.client.predictions.create(\n        model=model, input={\"prompt\": prompt}, stream=stream, **kwargs\n    )\n\n    if stream:\n        return self._stream_response(prediction, return_full_response)\n    else:\n        completed_prediction = self._wait_for_prediction(prediction.id)\n        if return_full_response:\n            response = cast(\n                ReplicateResponse, completed_prediction.__dict__.copy()\n            )\n            response[\"output\"] = self._process_output(\n                completed_prediction.output\n            )\n            return response\n        else:\n            return self._process_output(completed_prediction.output)\n</code></pre>"},{"location":"api/replicate_provider/#clientai.replicate.Provider.generate_text","title":"<code>generate_text(prompt, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Generate text based on a given prompt using a specified Replicate model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Replicate model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments       to pass to the Replicate API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ReplicateGenericResponse</code> <code>ReplicateGenericResponse</code> <p>The generated text, full response object,</p> <code>ReplicateGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Examples:</p> <p>Generate text (text only): <pre><code>response = provider.generate_text(\n    \"Explain quantum computing\",\n    model=\"meta/llama-2-70b-chat:latest\",\n)\nprint(response)\n</code></pre></p> <p>Generate text (full response): <pre><code>response = provider.generate_text(\n    \"Explain quantum computing\",\n    model=\"meta/llama-2-70b-chat:latest\",\n    return_full_response=True\n)\nprint(response[\"output\"])\n</code></pre></p> <p>Generate text (streaming): <pre><code>for chunk in provider.generate_text(\n    \"Explain quantum computing\",\n    model=\"meta/llama-2-70b-chat:latest\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/replicate/provider.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; ReplicateGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt\n    using a specified Replicate model.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the Replicate model to use.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        **kwargs: Additional keyword arguments\n                  to pass to the Replicate API.\n\n    Returns:\n        ReplicateGenericResponse: The generated text, full response object,\n        or an iterator for streaming responses.\n\n    Examples:\n        Generate text (text only):\n        ```python\n        response = provider.generate_text(\n            \"Explain quantum computing\",\n            model=\"meta/llama-2-70b-chat:latest\",\n        )\n        print(response)\n        ```\n\n        Generate text (full response):\n        ```python\n        response = provider.generate_text(\n            \"Explain quantum computing\",\n            model=\"meta/llama-2-70b-chat:latest\",\n            return_full_response=True\n        )\n        print(response[\"output\"])\n        ```\n\n        Generate text (streaming):\n        ```python\n        for chunk in provider.generate_text(\n            \"Explain quantum computing\",\n            model=\"meta/llama-2-70b-chat:latest\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    prediction = self.client.predictions.create(\n        model=model, input={\"prompt\": prompt}, stream=stream, **kwargs\n    )\n\n    if stream:\n        return self._stream_response(prediction, return_full_response)\n    else:\n        completed_prediction = self._wait_for_prediction(prediction.id)\n        if return_full_response:\n            response = cast(\n                ReplicateResponse, completed_prediction.__dict__.copy()\n            )\n            response[\"output\"] = self._process_output(\n                completed_prediction.output\n            )\n            return response\n        else:\n            return self._process_output(completed_prediction.output)\n</code></pre>"},{"location":"community/CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"community/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"community/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"community/CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"community/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"community/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at igor.magalhaes.r@gmail.com. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"community/CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"community/CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"community/CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"community/CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"community/CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"community/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"community/CONTRIBUTING/","title":"Contributing to ClientAI","text":"<p>Thank you for your interest in contributing to ClientAI! This guide is meant to make it easy for you to get started.</p>"},{"location":"community/CONTRIBUTING/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":""},{"location":"community/CONTRIBUTING/#cloning-the-repository","title":"Cloning the Repository","text":"<p>Start by forking and cloning the ClientAI repository:</p> <pre><code>git clone https://github.com/YOUR-GITHUB-USERNAME/clientai.git\n</code></pre>"},{"location":"community/CONTRIBUTING/#using-poetry-for-dependency-management","title":"Using Poetry for Dependency Management","text":"<p>ClientAI uses Poetry for managing dependencies. If you don't have Poetry installed, follow the instructions on the official Poetry website.</p> <p>Once Poetry is installed, navigate to the cloned repository and install the dependencies: <pre><code>cd clientai\npoetry install\n</code></pre></p>"},{"location":"community/CONTRIBUTING/#activating-the-virtual-environment","title":"Activating the Virtual Environment","text":"<p>Poetry creates a virtual environment for your project. Activate it using:</p> <pre><code>poetry shell\n</code></pre>"},{"location":"community/CONTRIBUTING/#making-contributions","title":"Making Contributions","text":""},{"location":"community/CONTRIBUTING/#coding-standards","title":"Coding Standards","text":"<ul> <li>Follow PEP 8 guidelines.</li> <li>Write meaningful tests for new features or bug fixes.</li> </ul>"},{"location":"community/CONTRIBUTING/#testing-with-pytest","title":"Testing with Pytest","text":"<p>ClientAI uses pytest for testing. Run tests using: <pre><code>poetry run pytest\n</code></pre></p>"},{"location":"community/CONTRIBUTING/#linting","title":"Linting","text":"<p>Use mypy for type checking: <pre><code>mypy clientai\n</code></pre></p> <p>Use ruff for style: <pre><code>ruff check --fix\nruff format\n</code></pre></p> <p>Ensure your code passes linting before submitting.</p>"},{"location":"community/CONTRIBUTING/#submitting-your-contributions","title":"Submitting Your Contributions","text":""},{"location":"community/CONTRIBUTING/#creating-a-pull-request","title":"Creating a Pull Request","text":"<p>After making your changes:</p> <ul> <li>Push your changes to your fork.</li> <li>Open a pull request with a clear description of your changes.</li> <li>Update the README.md if necessary.</li> </ul>"},{"location":"community/CONTRIBUTING/#code-reviews","title":"Code Reviews","text":"<ul> <li>Address any feedback from code reviews.</li> <li>Once approved, your contributions will be merged into the main branch.</li> </ul>"},{"location":"community/CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Please adhere to our Code of Conduct to maintain a welcoming and inclusive environment.</p> <p>Thank you for contributing to ClientAI\ud83d\ude80</p>"},{"location":"community/LICENSE/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2024 Igor Benav</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"community/overview/","title":"Community Overview","text":"<p>Welcome to the project's community hub. Here, you'll find essential resources and guidelines that are crucial for contributing to and participating in the project. Please take the time to familiarize yourself with the following documents:</p>"},{"location":"community/overview/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Contributing</li> <li>Code of Conduct</li> <li>License</li> </ul>"},{"location":"community/overview/#contributing","title":"Contributing","text":"<p>View the Contributing Guidelines</p> <p>Interested in contributing to the project? Great! The contributing guidelines will provide you with all the information you need to get started. This includes how to submit issues, propose changes, and the process for submitting pull requests.</p>"},{"location":"community/overview/#code-of-conduct","title":"Code of Conduct","text":"<p>View the Code of Conduct</p> <p>The Code of Conduct outlines the standards and behaviors expected of our community members. It's crucial to ensure a welcoming and inclusive environment for everyone. Please take the time to read and adhere to these guidelines.</p>"},{"location":"community/overview/#license","title":"License","text":"<p>View the License</p> <p>The license document outlines the terms under which our project can be used, modified, and distributed. Understanding the licensing is important for both users and contributors of the project.</p> <p>Thank you for being a part of our community and for contributing to our project's success!</p>"}]}