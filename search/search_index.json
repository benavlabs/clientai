{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ClientAI","text":"<p> A unified client for seamless interaction with multiple AI providers. </p> <p> </p> <p> ClientAI is a Python package that provides a unified interface for interacting with multiple AI providers, including OpenAI, Replicate, and Ollama. It offers seamless integration and consistent methods for text generation and chat functionality across different AI platforms. </p>"},{"location":"#features","title":"Features","text":"<ul> <li>Unified Interface: Consistent methods for text generation and chat across multiple AI providers.</li> <li>Multiple Providers: Support for OpenAI, Replicate, and Ollama, with easy extensibility for future providers.</li> <li>Streaming Support: Efficient streaming of responses for real-time applications.</li> <li>Flexible Configuration: Easy setup with provider-specific configurations.</li> <li>Customizable: Extensible design for adding new providers or customizing existing ones.</li> <li>Type Hinting: Comprehensive type annotations for better development experience.</li> <li>Provider Isolation: Optional installation of provider-specific dependencies to keep your environment lean.</li> </ul>"},{"location":"#minimal-example","title":"Minimal Example","text":"<p>Here's a quick example to get you started with ClientAI:</p> <pre><code>from clientai import ClientAI\n\n# Initialize with OpenAI\nclient = ClientAI('openai', api_key=\"your-openai-key\")\n\n# Generate text\nresponse = client.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n)\n\nprint(response)\n\n# Chat functionality\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"Paris.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\n\nresponse = client.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n)\n\nprint(response)\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<p>Before installing ClientAI, ensure you have the following prerequisites:</p> <ul> <li>Python: Version 3.9 or newer.</li> <li>Dependencies: The core ClientAI package has minimal dependencies. Provider-specific packages (e.g., <code>openai</code>, <code>replicate</code>, <code>ollama</code>) are optional and can be installed separately.</li> </ul>"},{"location":"#installing","title":"Installing","text":"<p>To install ClientAI with all providers, run:</p> <pre><code>pip install clientai[all]\n</code></pre> <p>Or, if you prefer to install only specific providers:</p> <pre><code>pip install clientai[openai]  # For OpenAI support\npip install clientai[replicate]  # For Replicate support\npip install clientai[ollama]  # For Ollama support\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>ClientAI offers a consistent way to interact with different AI providers:</p> <ol> <li>Initialize the client with your chosen provider and credentials.</li> <li>Use the <code>generate_text</code> method for text generation tasks.</li> <li>Use the <code>chat</code> method for conversational interactions.</li> </ol> <p>Both methods support streaming responses and returning full response objects.</p> <p>For more detailed usage examples and advanced features, please refer to the Usage section of this documentation.</p>"},{"location":"#license","title":"License","text":"<p><code>MIT</code></p>"},{"location":"extending/","title":"Extending ClientAI: Adding a New Provider","text":"<p>This guide will walk you through the process of adding support for a new AI provider to the ClientAI package. By following these steps, you'll be able to integrate a new provider seamlessly into the existing structure.</p>"},{"location":"extending/#overview","title":"Overview","text":"<p>To add a new provider, you'll need to:</p> <ol> <li>Create a new directory for the provider</li> <li>Implement the provider-specific types</li> <li>Implement the provider class</li> <li>Update the main ClientAI class</li> <li>Update the package constants</li> <li>Add tests for the new provider</li> </ol> <p>Let's go through each step in detail.</p>"},{"location":"extending/#step-1-create-a-new-directory","title":"Step 1: Create a New Directory","text":"<p>First, create a new directory for your provider in the <code>clientai</code> folder. For example, if you're adding support for a provider called \"NewAI\", create a directory named <code>newai</code>:</p> <pre><code>clientai/\n    newai/\n        __init__.py\n        _typing.py\n        provider.py\n</code></pre>"},{"location":"extending/#step-2-implement-provider-specific-types","title":"Step 2: Implement Provider-Specific Types","text":"<p>In the <code>_typing.py</code> file, define the types specific to your provider. This should include response types, client types, and any other necessary types. Here's an example structure:</p> <pre><code># clientai/newai/_typing.py\n\nfrom typing import Any, Dict, Iterator, Protocol, TypedDict, Union\nfrom .._common_types import GenericResponse\n\nclass NewAIResponse(TypedDict):\n    # Define the structure of a full response from NewAI\n    pass\n\nclass NewAIStreamResponse(TypedDict):\n    # Define the structure of a streaming response chunk from NewAI\n    pass\n\nclass NewAIClientProtocol(Protocol):\n    # Define the expected interface for the NewAI client\n    pass\n\nNewAIGenericResponse = GenericResponse[\n    str, NewAIResponse, NewAIStreamResponse\n]\n\n# Add any other necessary types\n</code></pre>"},{"location":"extending/#step-3-implement-the-provider-class","title":"Step 3: Implement the Provider Class","text":"<p>In the <code>provider.py</code> file, implement the <code>Provider</code> class that inherits from <code>AIProvider</code>. This class should implement the <code>generate_text</code> and <code>chat</code> methods:</p> <pre><code># clientai/newai/provider.py\n\nfrom ..ai_provider import AIProvider\nfrom ._typing import NewAIClientProtocol, NewAIGenericResponse\nfrom typing import List\nfrom .._common_types import Message\n\nclass Provider(AIProvider):\n    def __init__(self, api_key: str):\n        # Initialize the NewAI client\n        pass\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs\n    ) -&gt; NewAIGenericResponse:\n        # Implement text generation logic\n        pass\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs\n    ) -&gt; NewAIGenericResponse:\n        # Implement chat logic\n        pass\n\n    # Implement any helper methods as needed\n</code></pre> <p>Make sure to handle both streaming and non-streaming responses, as well as the <code>return_full_response</code> option.</p>"},{"location":"extending/#step-4-update-the-main-clientai-class","title":"Step 4: Update the Main ClientAI Class","text":"<p>Update the <code>clientai/client_ai.py</code> file to include support for your new provider:</p> <ol> <li> <p>Add an import for your new provider:    <pre><code>from .newai import NEWAI_INSTALLED\n</code></pre></p> </li> <li> <p>Update the <code>__init__</code> method of the <code>ClientAI</code> class to handle the new provider:    <pre><code> def __init__(self, provider_name: str, **kwargs):\n     prov_name = provider_name\n     # ----- add \"newai\" here -----\n     if prov_name not in [\"openai\", \"replicate\", \"ollama\", \"newai\"]:\n         raise ValueError(f\"Unsupported provider: {prov_name}\")\n\n     if (\n         prov_name == \"openai\"\n         and not OPENAI_INSTALLED\n         ...\n         # ----- also add \"newai\" here -----\n         or prov_name == \"newai\" \n         and not NEWAI_INSTALLED\n     ):\n         raise ImportError(\n             f\"The {prov_name} provider is not installed. \"\n             f\"Please install it with 'pip install clientai[{prov_name}]'.\"\n         )\n</code></pre></p> </li> <li> <p>Add the new provider to the provider initialization logic:     <pre><code>...\ntry:\n    provider_module = import_module(\n        f\".{prov_name}.provider\", package=\"clientai\"\n    )\n    provider_class = getattr(provider_module, \"Provider\")\n\n    # ----- add \"newai\" here -----\n    if prov_name in [\"openai\", \"replicate\", \"newai\"]:\n        self.provider = cast(\n            P, provider_class(api_key=kwargs.get(\"api_key\"))\n        )\n    ...\n</code></pre></p> </li> </ol>"},{"location":"extending/#step-5-update-package-constants-and-dependencies","title":"Step 5: Update Package Constants and Dependencies","text":"<ol> <li>In the <code>clientai/_constants.py</code> file, add a constant for your new provider:</li> </ol> <pre><code>NEWAI_INSTALLED = find_spec(\"newai\") is not None\n</code></pre> <ol> <li>Update the <code>clientai/__init__.py</code> file to export the new constant:</li> </ol> <pre><code>from ._constants import NEWAI_INSTALLED\n__all__ = [\n    # ... existing exports ...\n    \"NEWAI_INSTALLED\",\n]\n</code></pre> <ol> <li>Update the <code>pyproject.toml</code> file to include the new provider as an optional dependency:</li> </ol> <pre><code>[tool.poetry.dependencies]\npython = \"^3.9\"\npydantic = \"^2.9.2\"\nopenai = {version = \"^1.50.2\", optional = true}\nreplicate = {version = \"^0.34.1\", optional = true}\nollama = {version = \"^0.3.3\", optional = true}\nnewai-package = {version = \"^1.0.0\", optional = true}  # Add this line\n</code></pre> <ol> <li>Define an optional group for the new provider:</li> </ol> <pre><code>[tool.poetry.group.newai]\noptional = true\n\n[tool.poetry.group.newai.dependencies]\nnewai-package = \"^1.0.0\"\n</code></pre> <ol> <li>Include the new provider in the development dependencies:</li> </ol> <pre><code>[tool.poetry.group.dev.dependencies]\nruff = \"^0.6.8\"\npytest = \"^8.3.3\"\nmypy = \"1.9.0\"\nopenai = \"^1.50.2\"\nreplicate = \"^0.34.1\"\nollama = \"^0.3.3\"\nnewai-package = \"^1.0.0\"  # Add this line\n</code></pre> <ol> <li>Run <code>poetry update</code> to update the <code>poetry.lock</code> file with the new dependencies.</li> </ol> <p>These changes allow users to install the new provider's dependencies using Poetry:</p> <pre><code>poetry install --with newai\n</code></pre> <p>If users are not using Poetry and are installing the package via pip, they can still use the extras syntax:</p> <pre><code>pip install clientai[newai]\n</code></pre>"},{"location":"extending/#step-6-add-tests","title":"Step 6: Add Tests","text":"<p>Create a new test file for your provider in the <code>tests</code> directory:</p> <pre><code>tests/\n    newai/\n        __init__.py\n        test_provider.py\n</code></pre> <p>Implement tests for your new provider, ensuring that you cover both the <code>generate_text</code> and <code>chat</code> methods, as well as streaming and non-streaming scenarios.</p>"},{"location":"extending/#step-7-update-documentation","title":"Step 7: Update Documentation","text":"<p>Don't forget to update the documentation to include information about the new provider:</p> <ol> <li> <p>Add a new file <code>docs/api/newai_provider.md</code> with the following template for the NewAI provider. <pre><code># NewAI Provider API Reference\n\nThe `NewAI` class implements the `AIProvider` interface for the NewAI service. It provides methods for text generation and chat functionality using NewAI's models.\n\n## Class Definition\n\n::: clientai.newai.Provider\n    rendering:\n      show_if_no_docstring: true\n</code></pre></p> </li> <li> <p>Update <code>docs/index.md</code> to add a reference to the new provider.</p> </li> <li>Update <code>docs/quick-start.md</code> to include an example of how to use the new provider.</li> <li>Update <code>docs/api/overview.md</code> to include a link to the new provider's documentation.</li> </ol>"},{"location":"extending/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>After implementing your new provider, it's important to ensure that your code meets the project's quality standards and follows the contribution guidelines. Please refer to our Contributing Guide for detailed information on how to contribute to ClientAI.</p>"},{"location":"extending/#quick-summary-of-development-tools","title":"Quick Summary of Development Tools","text":"<p>ClientAI uses the following tools for maintaining code quality:</p> <ol> <li> <p>pytest: For running tests    <pre><code>poetry run pytest\n</code></pre></p> </li> <li> <p>mypy: For type checking    <pre><code>mypy clientai\n</code></pre></p> </li> <li> <p>ruff: For code linting and formatting    <pre><code>ruff check --fix\nruff format\n</code></pre></p> </li> </ol> <p>Make sure to run these tools and address any issues before submitting your contribution.</p>"},{"location":"extending/#conclusion","title":"Conclusion","text":"<p>By following these steps, you can successfully add support for a new AI provider to the ClientAI package. Remember to maintain consistency with the existing code style and structure, and to thoroughly test your implementation.</p> <p>If you're contributing this new provider to the main ClientAI repository, make sure to follow the contribution guidelines and submit a pull request with your changes. Your contribution will help expand the capabilities of ClientAI and benefit the entire community.</p> <p>Thank you for your interest in extending ClientAI!</p>"},{"location":"installing/","title":"Installing","text":""},{"location":"installing/#requirements","title":"Requirements","text":"<p>Before installing ClientAI, ensure you have the following prerequisites:</p> <ul> <li>Python: Version 3.9 or newer.</li> <li>Core Dependencies: ClientAI has minimal core dependencies, which will be automatically installed.</li> <li>Provider-Specific Libraries: Depending on which AI providers you plan to use, you may need to install additional libraries:<ul> <li>For OpenAI: <code>openai</code> library</li> <li>For Replicate: <code>replicate</code> library</li> <li>For Ollama: <code>ollama</code> library</li> </ul> </li> </ul>"},{"location":"installing/#installing_1","title":"Installing","text":"<p>ClientAI offers flexible installation options to suit your needs:</p>"},{"location":"installing/#basic-installation","title":"Basic Installation","text":"<p>To install the core ClientAI package without any provider-specific dependencies:</p> <pre><code>pip install clientai\n</code></pre> <p>Or, if using poetry:</p> <pre><code>poetry add clientai\n</code></pre>"},{"location":"installing/#installation-with-specific-providers","title":"Installation with Specific Providers","text":"<p>To install ClientAI with support for specific providers:</p> <pre><code>pip install clientai[openai]  # For OpenAI support\npip install clientai[replicate]  # For Replicate support\npip install clientai[ollama]  # For Ollama support\n</code></pre> <p>Or with poetry:</p> <pre><code>poetry add clientai[openai]\npoetry add clientai[replicate]\npoetry add clientai[ollama]\n</code></pre>"},{"location":"installing/#full-installation","title":"Full Installation","text":"<p>To install ClientAI with support for all providers:</p> <pre><code>pip install clientai[all]\n</code></pre> <p>Or with poetry:</p> <pre><code>poetry add clientai[all]\n</code></pre>"},{"location":"quick-start/","title":"Quickstart","text":"<p>This guide will help you get started with ClientAI quickly. We'll cover the basic setup and usage for each supported AI provider.</p>"},{"location":"quick-start/#minimal-example","title":"Minimal Example","text":"<p>Here's a minimal example to get you started with ClientAI:</p> quickstart.py<pre><code>from clientai import ClientAI\n\n# Initialize the client (example with OpenAI)\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Generate text\nresponse = client.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n)\nprint(response)\n\n# Use chat functionality\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n    {\"role\": \"user\", \"content\": \"What's its population?\"}\n]\nresponse = client.chat(messages, model=\"gpt-3.5-turbo\")\nprint(response)\n</code></pre>"},{"location":"quick-start/#setup-for-different-providers","title":"Setup for Different Providers","text":""},{"location":"quick-start/#openai","title":"OpenAI","text":"openai_setup.py<pre><code>from clientai import ClientAI\n\n# Initialize the OpenAI client\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Now you can use the client for text generation or chat\n</code></pre>"},{"location":"quick-start/#replicate","title":"Replicate","text":"replicate_setup.py<pre><code>from clientai import ClientAI\n\n# Initialize the Replicate client\nclient = ClientAI('replicate', api_key=\"your-replicate-api-key\")\n\n# Now you can use the client for text generation or chat\n</code></pre>"},{"location":"quick-start/#ollama","title":"Ollama","text":"ollama_setup.py<pre><code>from clientai import ClientAI\n\n# Initialize the Ollama client\nclient = ClientAI('ollama', host=\"your-ollama-host\")\n\n# Now you can use the client for text generation or chat\n</code></pre>"},{"location":"quick-start/#basic-usage","title":"Basic Usage","text":"<p>Once you have initialized the client, you can use it for text generation and chat functionality:</p>"},{"location":"quick-start/#text-generation","title":"Text Generation","text":"text_generation.py<pre><code>from clientai import ClientAI\n\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Generate text\nresponse = client.generate_text(\n    \"Explain the concept of quantum computing\",\n    model=\"gpt-3.5-turbo\",\n    max_tokens=100\n)\nprint(response)\n</code></pre>"},{"location":"quick-start/#chat","title":"Chat","text":"chat.py<pre><code>from clientai import ClientAI\n\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Use chat functionality\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n    {\"role\": \"assistant\", \"content\": \"Machine learning is a branch of artificial intelligence...\"},\n    {\"role\": \"user\", \"content\": \"Can you give an example of its application?\"}\n]\nresponse = client.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    max_tokens=150\n)\nprint(response)\n</code></pre>"},{"location":"quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've seen the basics of ClientAI, you can:</p> <ol> <li>Explore more advanced features like streaming responses and handling full response objects.</li> <li>Check out the Usage Guide for detailed information on all available methods and options.</li> <li>See the API Reference for a complete list of ClientAI's classes and methods.</li> </ol> <p>Remember to handle API keys securely and never expose them in your code or version control systems.</p>"},{"location":"api/ai_provider/","title":"AIProvider Class API Reference","text":"<p>The <code>AIProvider</code> class is an abstract base class that defines the interface for all AI provider implementations in ClientAI. It ensures consistency across different providers.</p>"},{"location":"api/ai_provider/#class-definition","title":"Class Definition","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for AI providers.</p> Source code in <code>clientai/ai_provider.py</code> <pre><code>class AIProvider(ABC):\n    \"\"\"\n    Abstract base class for AI providers.\n    \"\"\"\n\n    @abstractmethod\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; GenericResponse:\n        \"\"\"\n        Generate text based on a given prompt.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the AI model to use.\n            return_full_response: If True, return the full response object\n                                  instead of just the generated text.\n            stream: If True, return an iterator for streaming responses.\n            **kwargs: Additional keyword arguments specific to\n                      the provider's API.\n\n        Returns:\n            GenericResponse:\n                The generated text response, full response object,\n                or an iterator for streaming responses.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; GenericResponse:\n        \"\"\"\n        Engage in a chat conversation.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the AI model to use.\n            return_full_response: If True, return the full response object\n                                  instead of just the chat content.\n            stream: If True, return an iterator for streaming responses.\n            **kwargs: Additional keyword arguments specific to\n                      the provider's API.\n\n        Returns:\n            GenericResponse:\n                The chat response, either as a string, a dictionary,\n                or an iterator for streaming responses.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/ai_provider/#clientai.ai_provider.AIProvider.chat","title":"<code>chat(messages, model, return_full_response=False, stream=False, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Engage in a chat conversation.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object                   instead of just the chat content.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to       the provider's API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GenericResponse</code> <code>GenericResponse</code> <p>The chat response, either as a string, a dictionary, or an iterator for streaming responses.</p> Source code in <code>clientai/ai_provider.py</code> <pre><code>@abstractmethod\ndef chat(\n    self,\n    messages: List[Message],\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; GenericResponse:\n    \"\"\"\n    Engage in a chat conversation.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the AI model to use.\n        return_full_response: If True, return the full response object\n                              instead of just the chat content.\n        stream: If True, return an iterator for streaming responses.\n        **kwargs: Additional keyword arguments specific to\n                  the provider's API.\n\n    Returns:\n        GenericResponse:\n            The chat response, either as a string, a dictionary,\n            or an iterator for streaming responses.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ai_provider/#clientai.ai_provider.AIProvider.generate_text","title":"<code>generate_text(prompt, model, return_full_response=False, stream=False, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate text based on a given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object                   instead of just the generated text.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to       the provider's API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GenericResponse</code> <code>GenericResponse</code> <p>The generated text response, full response object, or an iterator for streaming responses.</p> Source code in <code>clientai/ai_provider.py</code> <pre><code>@abstractmethod\ndef generate_text(\n    self,\n    prompt: str,\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; GenericResponse:\n    \"\"\"\n    Generate text based on a given prompt.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the AI model to use.\n        return_full_response: If True, return the full response object\n                              instead of just the generated text.\n        stream: If True, return an iterator for streaming responses.\n        **kwargs: Additional keyword arguments specific to\n                  the provider's API.\n\n    Returns:\n        GenericResponse:\n            The generated text response, full response object,\n            or an iterator for streaming responses.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/clientai/","title":"ClientAI Class API Reference","text":"<p>The <code>ClientAI</code> class is the primary interface for interacting with various AI providers in a unified manner. It provides methods for text generation and chat functionality across different AI services.</p>"},{"location":"api/clientai/#class-definition","title":"Class Definition","text":"<p>             Bases: <code>Generic[P, T, S]</code></p> <p>A unified client for interacting with a single AI provider (OpenAI, Replicate, or Ollama).</p> <p>This class provides a consistent interface for common AI operations such as text generation and chat for the chosen AI provider.</p> <p>Type Parameters: P: The type of the AI provider. T: The type of the full response for non-streaming operations. S: The type of each chunk in streaming operations.</p> <p>Attributes:</p> Name Type Description <code>provider</code> <p>The initialized AI provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>The name of the AI provider to use            ('openai', 'replicate', or 'ollama').</p> required <code>**kwargs</code> <code>Any</code> <p>Provider-specific initialization parameters.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported provider name is given.</p> <code>ImportError</code> <p>If the specified provider is not installed.</p> <p>Examples:</p> <p>Initialize with OpenAI: <pre><code>ai = ClientAI('openai', api_key=\"your-openai-key\")\n</code></pre></p> <p>Initialize with Replicate: <pre><code>ai = ClientAI('replicate', api_key=\"your-replicate-key\")\n</code></pre></p> <p>Initialize with Ollama: <pre><code>ai = ClientAI('ollama', host=\"your-ollama-host\")\n</code></pre></p> Source code in <code>clientai/client_ai.py</code> <pre><code>class ClientAI(Generic[P, T, S]):\n    \"\"\"\n    A unified client for interacting with a single AI provider\n    (OpenAI, Replicate, or Ollama).\n\n    This class provides a consistent interface for common\n    AI operations such as text generation and chat\n    for the chosen AI provider.\n\n    Type Parameters:\n    P: The type of the AI provider.\n    T: The type of the full response for non-streaming operations.\n    S: The type of each chunk in streaming operations.\n\n    Attributes:\n        provider: The initialized AI provider.\n\n    Args:\n        provider_name: The name of the AI provider to use\n                       ('openai', 'replicate', or 'ollama').\n        **kwargs (Any): Provider-specific initialization parameters.\n\n    Raises:\n        ValueError: If an unsupported provider name is given.\n        ImportError: If the specified provider is not installed.\n\n    Examples:\n        Initialize with OpenAI:\n        ```python\n        ai = ClientAI('openai', api_key=\"your-openai-key\")\n        ```\n\n        Initialize with Replicate:\n        ```python\n        ai = ClientAI('replicate', api_key=\"your-replicate-key\")\n        ```\n\n        Initialize with Ollama:\n        ```python\n        ai = ClientAI('ollama', host=\"your-ollama-host\")\n        ```\n    \"\"\"\n\n    def __init__(self, provider_name: str, **kwargs):\n        prov_name = provider_name\n        if prov_name not in [\"openai\", \"replicate\", \"ollama\"]:\n            raise ValueError(f\"Unsupported provider: {prov_name}\")\n\n        if (\n            prov_name == \"openai\"\n            and not OPENAI_INSTALLED\n            or prov_name == \"replicate\"\n            and not REPLICATE_INSTALLED\n            or prov_name == \"ollama\"\n            and not OLLAMA_INSTALLED\n        ):\n            raise ImportError(\n                f\"The {prov_name} provider is not installed. \"\n                f\"Please install it with 'pip install clientai[{prov_name}]'.\"\n            )\n\n        try:\n            provider_module = import_module(\n                f\".{prov_name}.provider\", package=\"clientai\"\n            )\n            provider_class = getattr(provider_module, \"Provider\")\n            if prov_name in [\"openai\", \"replicate\"]:\n                self.provider = cast(\n                    P, provider_class(api_key=kwargs.get(\"api_key\"))\n                )\n            elif prov_name == \"ollama\":\n                self.provider = cast(\n                    P, provider_class(host=kwargs.get(\"host\"))\n                )\n        except ImportError as e:\n            raise ImportError(\n                f\"Error importing {prov_name} provider module: {str(e)}\"\n            ) from e\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; AIGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt\n        using the specified AI model and provider.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the AI model to use.\n            return_full_response: If True, returns the full structured response\n                                  If False, returns only the generated text.\n            stream: If True, returns an iterator for streaming responses.\n            **kwargs: Additional keyword arguments specific to\n                      the chosen provider's API.\n\n        Returns:\n            AIGenericResponse:\n                The generated text response, full response structure,\n                or an iterator for streaming responses.\n\n        Examples:\n            Generate text using OpenAI (text only):\n            ```python\n            response = ai.generate_text(\n                \"Tell me a joke\",\n                model=\"gpt-3.5-turbo\",\n            )\n            ```\n\n            Generate text using OpenAI (full response):\n            ```python\n            response = ai.generate_text(\n                \"Tell me a joke\",\n                model=\"gpt-3.5-turbo\",\n                return_full_response=True\n            )\n            ```\n\n            Generate text using OpenAI (streaming):\n            ```python\n            for chunk in ai.generate_text(\n                \"Tell me a joke\",\n                model=\"gpt-3.5-turbo\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n\n            Generate text using Replicate:\n            ```python\n            response = ai.generate_text(\n                \"Explain quantum computing\",\n                model=\"meta/llama-2-70b-chat:latest\",\n            )\n            ```\n\n            Generate text using Ollama:\n            ```python\n            response = ai.generate_text(\n                \"What is the capital of France?\",\n                model=\"llama2\",\n            )\n            ```\n        \"\"\"\n        return self.provider.generate_text(\n            prompt,\n            model,\n            return_full_response=return_full_response,\n            stream=stream,\n            **kwargs,\n        )\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; AIGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using\n        the specified AI model and provider.\n\n        Args:\n            messages: A list of message dictionaries, each\n                      containing 'role' and 'content'.\n            model: The name or identifier of the AI model to use.\n            return_full_response: If True, returns the full structured response\n                                  If False, returns the assistant's message.\n            stream: If True, returns an iterator for streaming responses.\n            **kwargs: Additional keyword arguments specific to\n                      the chosen provider's API.\n\n        Returns:\n            AIGenericResponse:\n                The chat response, full response structure,\n                or an iterator for streaming responses.\n\n        Examples:\n            Chat using OpenAI (message content only):\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n                {\"role\": \"assistant\", \"content\": \"Paris.\"},\n                {\"role\": \"user\", \"content\": \"What is its population?\"}\n            ]\n            response = ai.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n            )\n            ```\n\n            Chat using OpenAI (full response):\n            ```python\n            response = ai.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n                return_full_response=True\n            )\n            ```\n\n            Chat using OpenAI (streaming):\n            ```python\n            for chunk in ai.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n\n            Chat using Replicate:\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"Explain the concept of AI.\"}\n            ]\n            response = ai.chat(\n                messages,\n                model=\"meta/llama-2-70b-chat:latest\",\n            )\n            ```\n\n            Chat using Ollama:\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What are the laws of robotics?\"}\n            ]\n            response = ai.chat(messages, model=\"llama2\")\n            ```\n        \"\"\"\n        return self.provider.chat(\n            messages,\n            model,\n            return_full_response=return_full_response,\n            stream=stream,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/clientai/#clientai.ClientAI.chat","title":"<code>chat(messages, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Engage in a chat conversation using the specified AI model and provider.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each       containing 'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, returns the full structured response                   If False, returns the assistant's message.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, returns an iterator for streaming responses.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to       the chosen provider's API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AIGenericResponse</code> <code>AIGenericResponse</code> <p>The chat response, full response structure, or an iterator for streaming responses.</p> <p>Examples:</p> <p>Chat using OpenAI (message content only): <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"Paris.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\nresponse = ai.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n)\n</code></pre></p> <p>Chat using OpenAI (full response): <pre><code>response = ai.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    return_full_response=True\n)\n</code></pre></p> <p>Chat using OpenAI (streaming): <pre><code>for chunk in ai.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> <p>Chat using Replicate: <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"Explain the concept of AI.\"}\n]\nresponse = ai.chat(\n    messages,\n    model=\"meta/llama-2-70b-chat:latest\",\n)\n</code></pre></p> <p>Chat using Ollama: <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What are the laws of robotics?\"}\n]\nresponse = ai.chat(messages, model=\"llama2\")\n</code></pre></p> Source code in <code>clientai/client_ai.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; AIGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using\n    the specified AI model and provider.\n\n    Args:\n        messages: A list of message dictionaries, each\n                  containing 'role' and 'content'.\n        model: The name or identifier of the AI model to use.\n        return_full_response: If True, returns the full structured response\n                              If False, returns the assistant's message.\n        stream: If True, returns an iterator for streaming responses.\n        **kwargs: Additional keyword arguments specific to\n                  the chosen provider's API.\n\n    Returns:\n        AIGenericResponse:\n            The chat response, full response structure,\n            or an iterator for streaming responses.\n\n    Examples:\n        Chat using OpenAI (message content only):\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n            {\"role\": \"assistant\", \"content\": \"Paris.\"},\n            {\"role\": \"user\", \"content\": \"What is its population?\"}\n        ]\n        response = ai.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n        )\n        ```\n\n        Chat using OpenAI (full response):\n        ```python\n        response = ai.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n            return_full_response=True\n        )\n        ```\n\n        Chat using OpenAI (streaming):\n        ```python\n        for chunk in ai.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n\n        Chat using Replicate:\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"Explain the concept of AI.\"}\n        ]\n        response = ai.chat(\n            messages,\n            model=\"meta/llama-2-70b-chat:latest\",\n        )\n        ```\n\n        Chat using Ollama:\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What are the laws of robotics?\"}\n        ]\n        response = ai.chat(messages, model=\"llama2\")\n        ```\n    \"\"\"\n    return self.provider.chat(\n        messages,\n        model,\n        return_full_response=return_full_response,\n        stream=stream,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/clientai/#clientai.ClientAI.generate_text","title":"<code>generate_text(prompt, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Generate text based on a given prompt using the specified AI model and provider.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the AI model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, returns the full structured response                   If False, returns only the generated text.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, returns an iterator for streaming responses.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to       the chosen provider's API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AIGenericResponse</code> <code>AIGenericResponse</code> <p>The generated text response, full response structure, or an iterator for streaming responses.</p> <p>Examples:</p> <p>Generate text using OpenAI (text only): <pre><code>response = ai.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n)\n</code></pre></p> <p>Generate text using OpenAI (full response): <pre><code>response = ai.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n    return_full_response=True\n)\n</code></pre></p> <p>Generate text using OpenAI (streaming): <pre><code>for chunk in ai.generate_text(\n    \"Tell me a joke\",\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> <p>Generate text using Replicate: <pre><code>response = ai.generate_text(\n    \"Explain quantum computing\",\n    model=\"meta/llama-2-70b-chat:latest\",\n)\n</code></pre></p> <p>Generate text using Ollama: <pre><code>response = ai.generate_text(\n    \"What is the capital of France?\",\n    model=\"llama2\",\n)\n</code></pre></p> Source code in <code>clientai/client_ai.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; AIGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt\n    using the specified AI model and provider.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the AI model to use.\n        return_full_response: If True, returns the full structured response\n                              If False, returns only the generated text.\n        stream: If True, returns an iterator for streaming responses.\n        **kwargs: Additional keyword arguments specific to\n                  the chosen provider's API.\n\n    Returns:\n        AIGenericResponse:\n            The generated text response, full response structure,\n            or an iterator for streaming responses.\n\n    Examples:\n        Generate text using OpenAI (text only):\n        ```python\n        response = ai.generate_text(\n            \"Tell me a joke\",\n            model=\"gpt-3.5-turbo\",\n        )\n        ```\n\n        Generate text using OpenAI (full response):\n        ```python\n        response = ai.generate_text(\n            \"Tell me a joke\",\n            model=\"gpt-3.5-turbo\",\n            return_full_response=True\n        )\n        ```\n\n        Generate text using OpenAI (streaming):\n        ```python\n        for chunk in ai.generate_text(\n            \"Tell me a joke\",\n            model=\"gpt-3.5-turbo\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n\n        Generate text using Replicate:\n        ```python\n        response = ai.generate_text(\n            \"Explain quantum computing\",\n            model=\"meta/llama-2-70b-chat:latest\",\n        )\n        ```\n\n        Generate text using Ollama:\n        ```python\n        response = ai.generate_text(\n            \"What is the capital of France?\",\n            model=\"llama2\",\n        )\n        ```\n    \"\"\"\n    return self.provider.generate_text(\n        prompt,\n        model,\n        return_full_response=return_full_response,\n        stream=stream,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/ollama_provider/","title":"Ollama Provider API Reference","text":"<p>The <code>OllamaProvider</code> class implements the <code>AIProvider</code> interface for the Ollama service. It provides methods for text generation and chat functionality using locally hosted models through Ollama.</p>"},{"location":"api/ollama_provider/#class-definition","title":"Class Definition","text":"<p>             Bases: <code>AIProvider</code></p> <p>Ollama-specific implementation of the AIProvider abstract base class.</p> <p>This class provides methods to interact with Ollama's models for text generation and chat functionality.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>OllamaClientProtocol</code> <p>The Ollama client used for making API calls.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>Optional[str]</code> <p>The host address for the Ollama server. If not provided, the default Ollama client will be used.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the Ollama package is not installed.</p> <p>Examples:</p> <p>Initialize the Ollama provider: <pre><code>provider = Provider(host=\"http://localhost:11434\")\n</code></pre></p> Source code in <code>clientai/ollama/provider.py</code> <pre><code>class Provider(AIProvider):\n    \"\"\"\n    Ollama-specific implementation of the AIProvider abstract base class.\n\n    This class provides methods to interact with Ollama's models for\n    text generation and chat functionality.\n\n    Attributes:\n        client: The Ollama client used for making API calls.\n\n    Args:\n        host: The host address for the Ollama server.\n            If not provided, the default Ollama client will be used.\n\n    Raises:\n        ImportError: If the Ollama package is not installed.\n\n    Examples:\n        Initialize the Ollama provider:\n        ```python\n        provider = Provider(host=\"http://localhost:11434\")\n        ```\n    \"\"\"\n\n    def __init__(self, host: Optional[str] = None):\n        if not OLLAMA_INSTALLED or Client is None:\n            raise ImportError(\n                \"The ollama package is not installed. \"\n                \"Please install it with 'pip install clientai[ollama]'.\"\n            )\n        self.client: OllamaClientProtocol = cast(\n            OllamaClientProtocol, Client(host=host) if host else ollama\n        )\n\n    def _stream_generate_response(\n        self,\n        stream: Iterator[OllamaStreamResponse],\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, OllamaStreamResponse]]:\n        \"\"\"\n        Process the streaming response from Ollama API for text generation.\n\n        Args:\n            stream: The stream of responses from Ollama API.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, OllamaStreamResponse]: Processed content or\n                                              full response objects.\n        \"\"\"\n        for chunk in stream:\n            if return_full_response:\n                yield chunk\n            else:\n                yield chunk[\"response\"]\n\n    def _stream_chat_response(\n        self,\n        stream: Iterator[OllamaChatResponse],\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, OllamaChatResponse]]:\n        \"\"\"\n        Process the streaming response from Ollama API for chat.\n\n        Args:\n            stream: The stream of responses from Ollama API.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, OllamaChatResponse]: Processed content or\n                                            full response objects.\n        \"\"\"\n        for chunk in stream:\n            if return_full_response:\n                yield chunk\n            else:\n                yield chunk[\"message\"][\"content\"]\n\n    def _map_exception_to_clientai_error(self, e: Exception) -&gt; ClientAIError:\n        \"\"\"\n        Maps an Ollama exception to the appropriate ClientAI exception.\n\n        Args:\n            e (Exception): The exception caught during the API call.\n\n        Returns:\n            ClientAIError: An instance of the appropriate ClientAI exception.\n        \"\"\"\n        message = str(e)\n\n        if isinstance(e, ollama.RequestError):\n            if \"authentication\" in message.lower():\n                return AuthenticationError(message, original_error=e)\n            elif \"rate limit\" in message.lower():\n                return RateLimitError(message, original_error=e)\n            elif \"not found\" in message.lower():\n                return ModelError(message, original_error=e)\n            else:\n                return InvalidRequestError(message, original_error=e)\n        elif isinstance(e, ollama.ResponseError):\n            if \"timeout\" in message.lower() or \"timed out\" in message.lower():\n                return TimeoutError(message, original_error=e)\n            else:\n                return APIError(message, original_error=e)\n        else:\n            return ClientAIError(message, original_error=e)\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; OllamaGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt using a specified Ollama model.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the Ollama model to use.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            **kwargs: Additional keyword arguments to pass to the Ollama API.\n\n        Returns:\n            OllamaGenericResponse: The generated text, full response object,\n            or an iterator for streaming responses.\n\n        Examples:\n            Generate text (text only):\n            ```python\n            response = provider.generate_text(\n                \"Explain the concept of machine learning\",\n                model=\"llama2\",\n            )\n            print(response)\n            ```\n\n            Generate text (full response):\n            ```python\n            response = provider.generate_text(\n                \"Explain the concept of machine learning\",\n                model=\"llama2\",\n                return_full_response=True\n            )\n            print(response[\"response\"])\n            ```\n\n            Generate text (streaming):\n            ```python\n            for chunk in provider.generate_text(\n                \"Explain the concept of machine learning\",\n                model=\"llama2\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        try:\n            response = self.client.generate(\n                model=model, prompt=prompt, stream=stream, **kwargs\n            )\n\n            if stream:\n                return cast(\n                    OllamaGenericResponse,\n                    self._stream_generate_response(\n                        cast(Iterator[OllamaStreamResponse], response),\n                        return_full_response,\n                    ),\n                )\n            else:\n                response = cast(OllamaResponse, response)\n                if return_full_response:\n                    return response\n                else:\n                    return response[\"response\"]\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; OllamaGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using a specified Ollama model.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the Ollama model to use.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            **kwargs: Additional keyword arguments to pass to the Ollama API.\n\n        Returns:\n            OllamaGenericResponse: The chat response, full response object,\n            or an iterator for streaming responses.\n\n        Examples:\n            Chat (message content only):\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is the capital of Japan?\"},\n                {\"role\": \"assistant\", \"content\": \"The capital is Tokyo.\"},\n                {\"role\": \"user\", \"content\": \"What is its population?\"}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"llama2\",\n            )\n            print(response)\n            ```\n\n            Chat (full response):\n            ```python\n            response = provider.chat(\n                messages,\n                model=\"llama2\",\n                return_full_response=True\n            )\n            print(response[\"message\"][\"content\"])\n            ```\n\n            Chat (streaming):\n            ```python\n            for chunk in provider.chat(\n                messages,\n                model=\"llama2\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        try:\n            response = self.client.chat(\n                model=model, messages=messages, stream=stream, **kwargs\n            )\n\n            if stream:\n                return cast(\n                    OllamaGenericResponse,\n                    self._stream_chat_response(\n                        cast(Iterator[OllamaChatResponse], response),\n                        return_full_response,\n                    ),\n                )\n            else:\n                response = cast(OllamaChatResponse, response)\n                if return_full_response:\n                    return response\n                else:\n                    return response[\"message\"][\"content\"]\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/ollama_provider/#clientai.ollama.Provider.chat","title":"<code>chat(messages, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Engage in a chat conversation using a specified Ollama model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Ollama model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the Ollama API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OllamaGenericResponse</code> <code>OllamaGenericResponse</code> <p>The chat response, full response object,</p> <code>OllamaGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Examples:</p> <p>Chat (message content only): <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of Japan?\"},\n    {\"role\": \"assistant\", \"content\": \"The capital is Tokyo.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\nresponse = provider.chat(\n    messages,\n    model=\"llama2\",\n)\nprint(response)\n</code></pre></p> <p>Chat (full response): <pre><code>response = provider.chat(\n    messages,\n    model=\"llama2\",\n    return_full_response=True\n)\nprint(response[\"message\"][\"content\"])\n</code></pre></p> <p>Chat (streaming): <pre><code>for chunk in provider.chat(\n    messages,\n    model=\"llama2\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/ollama/provider.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; OllamaGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using a specified Ollama model.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the Ollama model to use.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the Ollama API.\n\n    Returns:\n        OllamaGenericResponse: The chat response, full response object,\n        or an iterator for streaming responses.\n\n    Examples:\n        Chat (message content only):\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is the capital of Japan?\"},\n            {\"role\": \"assistant\", \"content\": \"The capital is Tokyo.\"},\n            {\"role\": \"user\", \"content\": \"What is its population?\"}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"llama2\",\n        )\n        print(response)\n        ```\n\n        Chat (full response):\n        ```python\n        response = provider.chat(\n            messages,\n            model=\"llama2\",\n            return_full_response=True\n        )\n        print(response[\"message\"][\"content\"])\n        ```\n\n        Chat (streaming):\n        ```python\n        for chunk in provider.chat(\n            messages,\n            model=\"llama2\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    try:\n        response = self.client.chat(\n            model=model, messages=messages, stream=stream, **kwargs\n        )\n\n        if stream:\n            return cast(\n                OllamaGenericResponse,\n                self._stream_chat_response(\n                    cast(Iterator[OllamaChatResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(OllamaChatResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return response[\"message\"][\"content\"]\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/ollama_provider/#clientai.ollama.Provider.generate_text","title":"<code>generate_text(prompt, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Generate text based on a given prompt using a specified Ollama model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Ollama model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the Ollama API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OllamaGenericResponse</code> <code>OllamaGenericResponse</code> <p>The generated text, full response object,</p> <code>OllamaGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Examples:</p> <p>Generate text (text only): <pre><code>response = provider.generate_text(\n    \"Explain the concept of machine learning\",\n    model=\"llama2\",\n)\nprint(response)\n</code></pre></p> <p>Generate text (full response): <pre><code>response = provider.generate_text(\n    \"Explain the concept of machine learning\",\n    model=\"llama2\",\n    return_full_response=True\n)\nprint(response[\"response\"])\n</code></pre></p> <p>Generate text (streaming): <pre><code>for chunk in provider.generate_text(\n    \"Explain the concept of machine learning\",\n    model=\"llama2\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/ollama/provider.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; OllamaGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt using a specified Ollama model.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the Ollama model to use.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the Ollama API.\n\n    Returns:\n        OllamaGenericResponse: The generated text, full response object,\n        or an iterator for streaming responses.\n\n    Examples:\n        Generate text (text only):\n        ```python\n        response = provider.generate_text(\n            \"Explain the concept of machine learning\",\n            model=\"llama2\",\n        )\n        print(response)\n        ```\n\n        Generate text (full response):\n        ```python\n        response = provider.generate_text(\n            \"Explain the concept of machine learning\",\n            model=\"llama2\",\n            return_full_response=True\n        )\n        print(response[\"response\"])\n        ```\n\n        Generate text (streaming):\n        ```python\n        for chunk in provider.generate_text(\n            \"Explain the concept of machine learning\",\n            model=\"llama2\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    try:\n        response = self.client.generate(\n            model=model, prompt=prompt, stream=stream, **kwargs\n        )\n\n        if stream:\n            return cast(\n                OllamaGenericResponse,\n                self._stream_generate_response(\n                    cast(Iterator[OllamaStreamResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(OllamaResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return response[\"response\"]\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/openai_provider/","title":"OpenAI Provider API Reference","text":"<p>The <code>OpenAIProvider</code> class implements the <code>AIProvider</code> interface for the OpenAI service. It provides methods for text generation and chat functionality using OpenAI's models.</p>"},{"location":"api/openai_provider/#class-definition","title":"Class Definition","text":"<p>             Bases: <code>AIProvider</code></p> <p>OpenAI-specific implementation of the AIProvider abstract base class.</p> <p>This class provides methods to interact with OpenAI's models for text generation and chat functionality.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>OpenAIClientProtocol</code> <p>The OpenAI client used for making API calls.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for authenticating with OpenAI.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the OpenAI package is not installed.</p> <p>Examples:</p> <p>Initialize the OpenAI provider: <pre><code>provider = Provider(api_key=\"your-openai-api-key\")\n</code></pre></p> Source code in <code>clientai/openai/provider.py</code> <pre><code>class Provider(AIProvider):\n    \"\"\"\n    OpenAI-specific implementation of the AIProvider abstract base class.\n\n    This class provides methods to interact with OpenAI's\n    models for text generation and chat functionality.\n\n    Attributes:\n        client: The OpenAI client used for making API calls.\n\n    Args:\n        api_key: The API key for authenticating with OpenAI.\n\n    Raises:\n        ImportError: If the OpenAI package is not installed.\n\n    Examples:\n        Initialize the OpenAI provider:\n        ```python\n        provider = Provider(api_key=\"your-openai-api-key\")\n        ```\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        if not OPENAI_INSTALLED or Client is None:\n            raise ImportError(\n                \"The openai package is not installed. \"\n                \"Please install it with 'pip install clientai[openai]'.\"\n            )\n        self.client: OpenAIClientProtocol = cast(\n            OpenAIClientProtocol, Client(api_key=api_key)\n        )\n\n    def _stream_response(\n        self,\n        stream: Iterator[OpenAIStreamResponse],\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, OpenAIStreamResponse]]:\n        \"\"\"\n        Process the streaming response from OpenAI API.\n\n        Args:\n            stream: The stream of responses from OpenAI API.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, OpenAIStreamResponse]: Processed content or full\n                                              response objects.\n        \"\"\"\n        for chunk in stream:\n            if return_full_response:\n                yield chunk\n            else:\n                content = chunk.choices[0].delta.content\n                if content:\n                    yield content\n\n    def _map_exception_to_clientai_error(self, e: Exception) -&gt; ClientAIError:\n        \"\"\"\n        Maps an OpenAI exception to the appropriate ClientAI exception.\n\n        Args:\n            e (Exception): The exception caught during the API call.\n\n        Raises:\n            ClientAIError: An instance of the appropriate ClientAI exception.\n        \"\"\"\n        error_message = str(e)\n        status_code = None\n\n        if hasattr(e, \"status_code\"):\n            status_code = e.status_code\n        else:\n            try:\n                status_code = int(\n                    error_message.split(\"Error code: \")[1].split(\" -\")[0]\n                )\n            except (IndexError, ValueError):\n                pass\n\n        if (\n            isinstance(e, OpenAIAuthenticationError)\n            or \"incorrect api key\" in error_message.lower()\n        ):\n            return AuthenticationError(\n                error_message, status_code, original_error=e\n            )\n        elif (\n            isinstance(e, openai.OpenAIError)\n            or \"error code:\" in error_message.lower()\n        ):\n            if status_code == 429 or \"rate limit\" in error_message.lower():\n                return RateLimitError(\n                    error_message, status_code, original_error=e\n                )\n            elif status_code == 404 or \"not found\" in error_message.lower():\n                return ModelError(error_message, status_code, original_error=e)\n            elif status_code == 400 or \"invalid\" in error_message.lower():\n                return InvalidRequestError(\n                    error_message, status_code, original_error=e\n                )\n            elif status_code == 408 or \"timeout\" in error_message.lower():\n                return TimeoutError(\n                    error_message, status_code, original_error=e\n                )\n            elif status_code and status_code &gt;= 500:\n                return APIError(error_message, status_code, original_error=e)\n\n        return ClientAIError(error_message, status_code, original_error=e)\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; OpenAIGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt using a specified OpenAI model.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the OpenAI model to use.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            **kwargs: Additional keyword arguments to pass to the OpenAI API.\n\n        Returns:\n            OpenAIGenericResponse: The generated text, full response object,\n            or an iterator for streaming responses.\n\n        Raises:\n            ClientAIError: If an error occurs during the API call.\n\n        Examples:\n            Generate text (text only):\n            ```python\n            response = provider.generate_text(\n                \"Explain the theory of relativity\",\n                model=\"gpt-3.5-turbo\",\n            )\n            print(response)\n            ```\n\n            Generate text (full response):\n            ```python\n            response = provider.generate_text(\n                \"Explain the theory of relativity\",\n                model=\"gpt-3.5-turbo\",\n                return_full_response=True\n            )\n            print(response.choices[0].message.content)\n            ```\n\n            Generate text (streaming):\n            ```python\n            for chunk in provider.generate_text(\n                \"Explain the theory of relativity\",\n                model=\"gpt-3.5-turbo\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        try:\n            response = self.client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                stream=stream,\n                **kwargs,\n            )\n\n            if stream:\n                return cast(\n                    OpenAIGenericResponse,\n                    self._stream_response(\n                        cast(Iterator[OpenAIStreamResponse], response),\n                        return_full_response,\n                    ),\n                )\n            else:\n                response = cast(OpenAIResponse, response)\n                if return_full_response:\n                    return response\n                else:\n                    return response.choices[0].message.content\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; OpenAIGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using a specified OpenAI model.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the OpenAI model to use.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            **kwargs: Additional keyword arguments to pass to the OpenAI API.\n\n        Returns:\n            OpenAIGenericResponse: The chat response, full response object,\n            or an iterator for streaming responses.\n\n        Raises:\n            ClientAIError: If an error occurs during the API call.\n\n        Examples:\n            Chat (message content only):\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n                {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n                {\"role\": \"user\", \"content\": \"What is its population?\"}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n            )\n            print(response)\n            ```\n\n            Chat (full response):\n            ```python\n            response = provider.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n                return_full_response=True\n            )\n            print(response.choices[0].message.content)\n            ```\n\n            Chat (streaming):\n            ```python\n            for chunk in provider.chat(\n                messages,\n                model=\"gpt-3.5-turbo\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        try:\n            response = self.client.chat.completions.create(\n                model=model, messages=messages, stream=stream, **kwargs\n            )\n\n            if stream:\n                return cast(\n                    OpenAIGenericResponse,\n                    self._stream_response(\n                        cast(Iterator[OpenAIStreamResponse], response),\n                        return_full_response,\n                    ),\n                )\n            else:\n                response = cast(OpenAIResponse, response)\n                if return_full_response:\n                    return response\n                else:\n                    return response.choices[0].message.content\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/openai_provider/#clientai.openai.Provider.chat","title":"<code>chat(messages, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Engage in a chat conversation using a specified OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the OpenAI model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OpenAIGenericResponse</code> <code>OpenAIGenericResponse</code> <p>The chat response, full response object,</p> <code>OpenAIGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Raises:</p> Type Description <code>ClientAIError</code> <p>If an error occurs during the API call.</p> <p>Examples:</p> <p>Chat (message content only): <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\nresponse = provider.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n)\nprint(response)\n</code></pre></p> <p>Chat (full response): <pre><code>response = provider.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    return_full_response=True\n)\nprint(response.choices[0].message.content)\n</code></pre></p> <p>Chat (streaming): <pre><code>for chunk in provider.chat(\n    messages,\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/openai/provider.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; OpenAIGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using a specified OpenAI model.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the OpenAI model to use.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the OpenAI API.\n\n    Returns:\n        OpenAIGenericResponse: The chat response, full response object,\n        or an iterator for streaming responses.\n\n    Raises:\n        ClientAIError: If an error occurs during the API call.\n\n    Examples:\n        Chat (message content only):\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n            {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n            {\"role\": \"user\", \"content\": \"What is its population?\"}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n        )\n        print(response)\n        ```\n\n        Chat (full response):\n        ```python\n        response = provider.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n            return_full_response=True\n        )\n        print(response.choices[0].message.content)\n        ```\n\n        Chat (streaming):\n        ```python\n        for chunk in provider.chat(\n            messages,\n            model=\"gpt-3.5-turbo\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    try:\n        response = self.client.chat.completions.create(\n            model=model, messages=messages, stream=stream, **kwargs\n        )\n\n        if stream:\n            return cast(\n                OpenAIGenericResponse,\n                self._stream_response(\n                    cast(Iterator[OpenAIStreamResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(OpenAIResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return response.choices[0].message.content\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/openai_provider/#clientai.openai.Provider.generate_text","title":"<code>generate_text(prompt, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Generate text based on a given prompt using a specified OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the OpenAI model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OpenAIGenericResponse</code> <code>OpenAIGenericResponse</code> <p>The generated text, full response object,</p> <code>OpenAIGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Raises:</p> Type Description <code>ClientAIError</code> <p>If an error occurs during the API call.</p> <p>Examples:</p> <p>Generate text (text only): <pre><code>response = provider.generate_text(\n    \"Explain the theory of relativity\",\n    model=\"gpt-3.5-turbo\",\n)\nprint(response)\n</code></pre></p> <p>Generate text (full response): <pre><code>response = provider.generate_text(\n    \"Explain the theory of relativity\",\n    model=\"gpt-3.5-turbo\",\n    return_full_response=True\n)\nprint(response.choices[0].message.content)\n</code></pre></p> <p>Generate text (streaming): <pre><code>for chunk in provider.generate_text(\n    \"Explain the theory of relativity\",\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/openai/provider.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; OpenAIGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt using a specified OpenAI model.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the OpenAI model to use.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the OpenAI API.\n\n    Returns:\n        OpenAIGenericResponse: The generated text, full response object,\n        or an iterator for streaming responses.\n\n    Raises:\n        ClientAIError: If an error occurs during the API call.\n\n    Examples:\n        Generate text (text only):\n        ```python\n        response = provider.generate_text(\n            \"Explain the theory of relativity\",\n            model=\"gpt-3.5-turbo\",\n        )\n        print(response)\n        ```\n\n        Generate text (full response):\n        ```python\n        response = provider.generate_text(\n            \"Explain the theory of relativity\",\n            model=\"gpt-3.5-turbo\",\n            return_full_response=True\n        )\n        print(response.choices[0].message.content)\n        ```\n\n        Generate text (streaming):\n        ```python\n        for chunk in provider.generate_text(\n            \"Explain the theory of relativity\",\n            model=\"gpt-3.5-turbo\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    try:\n        response = self.client.chat.completions.create(\n            model=model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            stream=stream,\n            **kwargs,\n        )\n\n        if stream:\n            return cast(\n                OpenAIGenericResponse,\n                self._stream_response(\n                    cast(Iterator[OpenAIStreamResponse], response),\n                    return_full_response,\n                ),\n            )\n        else:\n            response = cast(OpenAIResponse, response)\n            if return_full_response:\n                return response\n            else:\n                return response.choices[0].message.content\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/overview/","title":"API Reference Overview","text":"<p>Welcome to the API Reference section of ClientAI documentation. This section provides detailed information about the various classes, functions, and modules that make up ClientAI. Whether you're looking to integrate ClientAI into your project, extend its functionality, or simply explore its capabilities, this section will guide you through the intricacies of our codebase.</p>"},{"location":"api/overview/#key-components","title":"Key Components","text":"<p>ClientAI's API is comprised of several key components, each serving a specific purpose:</p> <ol> <li> <p>ClientAI Class: This is the main class of our library. It provides a unified interface for interacting with different AI providers and is the primary entry point for using ClientAI.</p> <ul> <li>ClientAI Class Reference</li> </ul> </li> <li> <p>AIProvider Class: An abstract base class that defines the interface for all AI provider implementations. It ensures consistency across different providers.</p> <ul> <li>AIProvider Class Reference</li> </ul> </li> <li> <p>Provider-Specific Classes: These classes implement the AIProvider interface for each supported AI service (OpenAI, Replicate, Ollama).</p> <ul> <li>OpenAI Provider Reference</li> <li>Replicate Provider Reference</li> <li>Ollama Provider Reference</li> </ul> </li> </ol>"},{"location":"api/overview/#usage","title":"Usage","text":"<p>Each component is documented with its own dedicated page, where you can find detailed information about its methods, parameters, return types, and usage examples. These pages are designed to provide you with all the information you need to understand and work with ClientAI effectively.</p>"},{"location":"api/overview/#basic-usage-example","title":"Basic Usage Example","text":"<p>Here's a quick example of how to use the main ClientAI class:</p> <pre><code>from clientai import ClientAI\n\n# Initialize the client\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Generate text\nresponse = client.generate_text(\n    \"Explain quantum computing\",\n    model=\"gpt-3.5-turbo\"\n)\n\nprint(response)\n</code></pre> <p>For more detailed usage instructions and examples, please refer to the Usage Guide (\ud83d\udea7 Under Construction, come back soon \ud83d\udea7).</p>"},{"location":"api/overview/#extending-clientai","title":"Extending ClientAI","text":"<p>If you wish to add support for a new AI provider or extend the functionality of existing providers, you can do so by implementing the AIProvider interface. See the Extending ClientAI Guide for more information.</p>"},{"location":"api/overview/#contribution","title":"Contribution","text":"<p>We welcome contributions to ClientAI! If you're interested in contributing, please refer to our Contributing Guidelines. Contributions can range from bug fixes and documentation improvements to adding support for new AI providers.</p>"},{"location":"api/overview/#feedback","title":"Feedback","text":"<p>Your feedback is crucial in helping us improve ClientAI and its documentation. If you have any suggestions, corrections, or queries, please don't hesitate to reach out to us via GitHub issues or our community channels.</p> <p>Navigate through each section for detailed documentation of ClientAI's API components.</p>"},{"location":"api/replicate_provider/","title":"Replicate Provider API Reference","text":"<p>The <code>ReplicateProvider</code> class implements the <code>AIProvider</code> interface for the Replicate service. It provides methods for text generation and chat functionality using models hosted on Replicate.</p>"},{"location":"api/replicate_provider/#class-definition","title":"Class Definition","text":"<p>             Bases: <code>AIProvider</code></p> <p>Replicate-specific implementation of the AIProvider abstract base class.</p> <p>This class provides methods to interact with Replicate's AI models for text generation and chat functionality.</p> <p>Attributes:</p> Name Type Description <code>client</code> <code>ReplicateClientProtocol</code> <p>The Replicate client used for making API calls.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for authenticating with Replicate.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the Replicate package is not installed.</p> <p>Examples:</p> <p>Initialize the Replicate provider: <pre><code>provider = Provider(api_key=\"your-replicate-api-key\")\n</code></pre></p> Source code in <code>clientai/replicate/provider.py</code> <pre><code>class Provider(AIProvider):\n    \"\"\"\n    Replicate-specific implementation of the AIProvider abstract base class.\n\n    This class provides methods to interact with Replicate's AI models for\n    text generation and chat functionality.\n\n    Attributes:\n        client: The Replicate client used for making API calls.\n\n    Args:\n        api_key: The API key for authenticating with Replicate.\n\n    Raises:\n        ImportError: If the Replicate package is not installed.\n\n    Examples:\n        Initialize the Replicate provider:\n        ```python\n        provider = Provider(api_key=\"your-replicate-api-key\")\n        ```\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        if not REPLICATE_INSTALLED or Client is None:\n            raise ImportError(\n                \"The replicate package is not installed. \"\n                \"Please install it with 'pip install clientai[replicate]'.\"\n            )\n        self.client: ReplicateClientProtocol = Client(api_token=api_key)\n\n    def _process_output(self, output: Any) -&gt; str:\n        \"\"\"\n        Process the output from Replicate API into a string format.\n\n        Args:\n            output: The raw output from Replicate API.\n\n        Returns:\n            str: The processed output as a string.\n        \"\"\"\n        if isinstance(output, List):\n            return \"\".join(str(item) for item in output)\n        elif isinstance(output, str):\n            return output\n        else:\n            return str(output)\n\n    def _wait_for_prediction(\n        self, prediction_id: str, max_wait_time: int = 300\n    ) -&gt; ReplicatePredictionProtocol:\n        \"\"\"\n        Wait for a prediction to complete or fail.\n\n        Args:\n            prediction_id: The ID of the prediction to wait for.\n            max_wait_time: Maximum time to wait in seconds. Defaults to 300.\n\n        Returns:\n            ReplicatePredictionProtocol: The completed prediction.\n\n        Raises:\n            TimeoutError: If the prediction doesn't complete within\n                          the max_wait_time.\n            APIError: If the prediction fails.\n        \"\"\"\n        start_time = time.time()\n        while time.time() - start_time &lt; max_wait_time:\n            prediction = self.client.predictions.get(prediction_id)\n            if prediction.status == \"succeeded\":\n                return prediction\n            elif prediction.status == \"failed\":\n                raise self._map_exception_to_clientai_error(\n                    Exception(f\"Prediction failed: {prediction.error}\")\n                )\n            time.sleep(1)\n\n        raise self._map_exception_to_clientai_error(\n            Exception(\"Prediction timed out\"), status_code=408\n        )\n\n    def _stream_response(\n        self,\n        prediction: ReplicatePredictionProtocol,\n        return_full_response: bool,\n    ) -&gt; Iterator[Union[str, ReplicateStreamResponse]]:\n        \"\"\"\n        Stream the response from a prediction.\n\n        Args:\n            prediction: The prediction to stream.\n            return_full_response: If True, yield full response objects.\n\n        Yields:\n            Union[str, ReplicateStreamResponse]: Processed output or\n                                                 full response objects.\n        \"\"\"\n        metadata = cast(ReplicateStreamResponse, prediction.__dict__.copy())\n        for event in prediction.stream():\n            if return_full_response:\n                metadata[\"output\"] = self._process_output(event)\n                yield metadata\n            else:\n                yield self._process_output(event)\n\n    def _map_exception_to_clientai_error(\n        self, e: Exception, status_code: Optional[int] = None\n    ) -&gt; ClientAIError:\n        \"\"\"\n        Maps a Replicate exception to the appropriate ClientAI exception.\n\n        Args:\n            e (Exception): The exception caught during the API call.\n            status_code (int, optional): The HTTP status code, if available.\n\n        Returns:\n            ClientAIError: An instance of the appropriate ClientAI exception.\n        \"\"\"\n        error_message = str(e)\n        status_code = status_code or getattr(e, \"status_code\", None)\n\n        if (\n            \"authentication\" in error_message.lower()\n            or \"unauthorized\" in error_message.lower()\n        ):\n            return AuthenticationError(\n                error_message, status_code, original_error=e\n            )\n        elif \"rate limit\" in error_message.lower():\n            return RateLimitError(error_message, status_code, original_error=e)\n        elif \"not found\" in error_message.lower():\n            return ModelError(error_message, status_code, original_error=e)\n        elif \"invalid\" in error_message.lower():\n            return InvalidRequestError(\n                error_message, status_code, original_error=e\n            )\n        elif \"timeout\" in error_message.lower() or status_code == 408:\n            return TimeoutError(error_message, status_code, original_error=e)\n        elif status_code == 400:\n            return InvalidRequestError(\n                error_message, status_code, original_error=e\n            )\n        else:\n            return APIError(error_message, status_code, original_error=e)\n\n    def generate_text(\n        self,\n        prompt: str,\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; ReplicateGenericResponse:\n        \"\"\"\n        Generate text based on a given prompt\n        using a specified Replicate model.\n\n        Args:\n            prompt: The input prompt for text generation.\n            model: The name or identifier of the Replicate model to use.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            **kwargs: Additional keyword arguments\n                      to pass to the Replicate API.\n\n        Returns:\n            ReplicateGenericResponse: The generated text, full response object,\n            or an iterator for streaming responses.\n\n        Examples:\n            Generate text (text only):\n            ```python\n            response = provider.generate_text(\n                \"Explain quantum computing\",\n                model=\"meta/llama-2-70b-chat:latest\",\n            )\n            print(response)\n            ```\n\n            Generate text (full response):\n            ```python\n            response = provider.generate_text(\n                \"Explain quantum computing\",\n                model=\"meta/llama-2-70b-chat:latest\",\n                return_full_response=True\n            )\n            print(response[\"output\"])\n            ```\n\n            Generate text (streaming):\n            ```python\n            for chunk in provider.generate_text(\n                \"Explain quantum computing\",\n                model=\"meta/llama-2-70b-chat:latest\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        try:\n            prediction = self.client.predictions.create(\n                model=model, input={\"prompt\": prompt}, stream=stream, **kwargs\n            )\n\n            if stream:\n                return self._stream_response(prediction, return_full_response)\n            else:\n                completed_prediction = self._wait_for_prediction(prediction.id)\n                if return_full_response:\n                    response = cast(\n                        ReplicateResponse, completed_prediction.__dict__.copy()\n                    )\n                    response[\"output\"] = self._process_output(\n                        completed_prediction.output\n                    )\n                    return response\n                else:\n                    return self._process_output(completed_prediction.output)\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: str,\n        return_full_response: bool = False,\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; ReplicateGenericResponse:\n        \"\"\"\n        Engage in a chat conversation using a specified Replicate model.\n\n        Args:\n            messages: A list of message dictionaries, each containing\n                      'role' and 'content'.\n            model: The name or identifier of the Replicate model to use.\n            return_full_response: If True, return the full response object.\n                If False, return only the generated text. Defaults to False.\n            stream: If True, return an iterator for streaming responses.\n                Defaults to False.\n            **kwargs: Additional keyword arguments\n                      to pass to the Replicate API.\n\n        Returns:\n            ReplicateGenericResponse: The chat response, full response object,\n            or an iterator for streaming responses.\n\n        Examples:\n            Chat (message content only):\n            ```python\n            messages = [\n                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n                {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n                {\"role\": \"user\", \"content\": \"What is its population?\"}\n            ]\n            response = provider.chat(\n                messages,\n                model=\"meta/llama-2-70b-chat:latest\",\n            )\n            print(response)\n            ```\n\n            Chat (full response):\n            ```python\n            response = provider.chat(\n                messages,\n                model=\"meta/llama-2-70b-chat:latest\",\n                return_full_response=True\n            )\n            print(response[\"output\"])\n            ```\n\n            Chat (streaming):\n            ```python\n            for chunk in provider.chat(\n                messages,\n                model=\"meta/llama-2-70b-chat:latest\",\n                stream=True\n            ):\n                print(chunk, end=\"\", flush=True)\n            ```\n        \"\"\"\n        try:\n            prompt = \"\\n\".join(\n                [f\"{m['role']}: {m['content']}\" for m in messages]\n            )\n            prompt += \"\\nassistant: \"\n\n            prediction = self.client.predictions.create(\n                model=model, input={\"prompt\": prompt}, stream=stream, **kwargs\n            )\n\n            if stream:\n                return self._stream_response(prediction, return_full_response)\n            else:\n                completed_prediction = self._wait_for_prediction(prediction.id)\n                if return_full_response:\n                    response = cast(\n                        ReplicateResponse, completed_prediction.__dict__.copy()\n                    )\n                    response[\"output\"] = self._process_output(\n                        completed_prediction.output\n                    )\n                    return response\n                else:\n                    return self._process_output(completed_prediction.output)\n\n        except Exception as e:\n            raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/replicate_provider/#clientai.replicate.Provider.chat","title":"<code>chat(messages, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Engage in a chat conversation using a specified Replicate model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of message dictionaries, each containing       'role' and 'content'.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Replicate model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments       to pass to the Replicate API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ReplicateGenericResponse</code> <code>ReplicateGenericResponse</code> <p>The chat response, full response object,</p> <code>ReplicateGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Examples:</p> <p>Chat (message content only): <pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n    {\"role\": \"user\", \"content\": \"What is its population?\"}\n]\nresponse = provider.chat(\n    messages,\n    model=\"meta/llama-2-70b-chat:latest\",\n)\nprint(response)\n</code></pre></p> <p>Chat (full response): <pre><code>response = provider.chat(\n    messages,\n    model=\"meta/llama-2-70b-chat:latest\",\n    return_full_response=True\n)\nprint(response[\"output\"])\n</code></pre></p> <p>Chat (streaming): <pre><code>for chunk in provider.chat(\n    messages,\n    model=\"meta/llama-2-70b-chat:latest\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/replicate/provider.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; ReplicateGenericResponse:\n    \"\"\"\n    Engage in a chat conversation using a specified Replicate model.\n\n    Args:\n        messages: A list of message dictionaries, each containing\n                  'role' and 'content'.\n        model: The name or identifier of the Replicate model to use.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        **kwargs: Additional keyword arguments\n                  to pass to the Replicate API.\n\n    Returns:\n        ReplicateGenericResponse: The chat response, full response object,\n        or an iterator for streaming responses.\n\n    Examples:\n        Chat (message content only):\n        ```python\n        messages = [\n            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n            {\"role\": \"assistant\", \"content\": \"The capital is Paris.\"},\n            {\"role\": \"user\", \"content\": \"What is its population?\"}\n        ]\n        response = provider.chat(\n            messages,\n            model=\"meta/llama-2-70b-chat:latest\",\n        )\n        print(response)\n        ```\n\n        Chat (full response):\n        ```python\n        response = provider.chat(\n            messages,\n            model=\"meta/llama-2-70b-chat:latest\",\n            return_full_response=True\n        )\n        print(response[\"output\"])\n        ```\n\n        Chat (streaming):\n        ```python\n        for chunk in provider.chat(\n            messages,\n            model=\"meta/llama-2-70b-chat:latest\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    try:\n        prompt = \"\\n\".join(\n            [f\"{m['role']}: {m['content']}\" for m in messages]\n        )\n        prompt += \"\\nassistant: \"\n\n        prediction = self.client.predictions.create(\n            model=model, input={\"prompt\": prompt}, stream=stream, **kwargs\n        )\n\n        if stream:\n            return self._stream_response(prediction, return_full_response)\n        else:\n            completed_prediction = self._wait_for_prediction(prediction.id)\n            if return_full_response:\n                response = cast(\n                    ReplicateResponse, completed_prediction.__dict__.copy()\n                )\n                response[\"output\"] = self._process_output(\n                    completed_prediction.output\n                )\n                return response\n            else:\n                return self._process_output(completed_prediction.output)\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"api/replicate_provider/#clientai.replicate.Provider.generate_text","title":"<code>generate_text(prompt, model, return_full_response=False, stream=False, **kwargs)</code>","text":"<p>Generate text based on a given prompt using a specified Replicate model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The input prompt for text generation.</p> required <code>model</code> <code>str</code> <p>The name or identifier of the Replicate model to use.</p> required <code>return_full_response</code> <code>bool</code> <p>If True, return the full response object. If False, return only the generated text. Defaults to False.</p> <code>False</code> <code>stream</code> <code>bool</code> <p>If True, return an iterator for streaming responses. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments       to pass to the Replicate API.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ReplicateGenericResponse</code> <code>ReplicateGenericResponse</code> <p>The generated text, full response object,</p> <code>ReplicateGenericResponse</code> <p>or an iterator for streaming responses.</p> <p>Examples:</p> <p>Generate text (text only): <pre><code>response = provider.generate_text(\n    \"Explain quantum computing\",\n    model=\"meta/llama-2-70b-chat:latest\",\n)\nprint(response)\n</code></pre></p> <p>Generate text (full response): <pre><code>response = provider.generate_text(\n    \"Explain quantum computing\",\n    model=\"meta/llama-2-70b-chat:latest\",\n    return_full_response=True\n)\nprint(response[\"output\"])\n</code></pre></p> <p>Generate text (streaming): <pre><code>for chunk in provider.generate_text(\n    \"Explain quantum computing\",\n    model=\"meta/llama-2-70b-chat:latest\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre></p> Source code in <code>clientai/replicate/provider.py</code> <pre><code>def generate_text(\n    self,\n    prompt: str,\n    model: str,\n    return_full_response: bool = False,\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; ReplicateGenericResponse:\n    \"\"\"\n    Generate text based on a given prompt\n    using a specified Replicate model.\n\n    Args:\n        prompt: The input prompt for text generation.\n        model: The name or identifier of the Replicate model to use.\n        return_full_response: If True, return the full response object.\n            If False, return only the generated text. Defaults to False.\n        stream: If True, return an iterator for streaming responses.\n            Defaults to False.\n        **kwargs: Additional keyword arguments\n                  to pass to the Replicate API.\n\n    Returns:\n        ReplicateGenericResponse: The generated text, full response object,\n        or an iterator for streaming responses.\n\n    Examples:\n        Generate text (text only):\n        ```python\n        response = provider.generate_text(\n            \"Explain quantum computing\",\n            model=\"meta/llama-2-70b-chat:latest\",\n        )\n        print(response)\n        ```\n\n        Generate text (full response):\n        ```python\n        response = provider.generate_text(\n            \"Explain quantum computing\",\n            model=\"meta/llama-2-70b-chat:latest\",\n            return_full_response=True\n        )\n        print(response[\"output\"])\n        ```\n\n        Generate text (streaming):\n        ```python\n        for chunk in provider.generate_text(\n            \"Explain quantum computing\",\n            model=\"meta/llama-2-70b-chat:latest\",\n            stream=True\n        ):\n            print(chunk, end=\"\", flush=True)\n        ```\n    \"\"\"\n    try:\n        prediction = self.client.predictions.create(\n            model=model, input={\"prompt\": prompt}, stream=stream, **kwargs\n        )\n\n        if stream:\n            return self._stream_response(prediction, return_full_response)\n        else:\n            completed_prediction = self._wait_for_prediction(prediction.id)\n            if return_full_response:\n                response = cast(\n                    ReplicateResponse, completed_prediction.__dict__.copy()\n                )\n                response[\"output\"] = self._process_output(\n                    completed_prediction.output\n                )\n                return response\n            else:\n                return self._process_output(completed_prediction.output)\n\n    except Exception as e:\n        raise self._map_exception_to_clientai_error(e)\n</code></pre>"},{"location":"community/CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"community/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"community/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"community/CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"community/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"community/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at igor.magalhaes.r@gmail.com. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"community/CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"community/CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"community/CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"community/CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"community/CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"community/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"community/CONTRIBUTING/","title":"Contributing to ClientAI","text":"<p>Thank you for your interest in contributing to ClientAI! This guide is meant to make it easy for you to get started.</p>"},{"location":"community/CONTRIBUTING/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":""},{"location":"community/CONTRIBUTING/#cloning-the-repository","title":"Cloning the Repository","text":"<p>Start by forking and cloning the ClientAI repository:</p> <pre><code>git clone https://github.com/YOUR-GITHUB-USERNAME/clientai.git\n</code></pre>"},{"location":"community/CONTRIBUTING/#using-poetry-for-dependency-management","title":"Using Poetry for Dependency Management","text":"<p>ClientAI uses Poetry for managing dependencies. If you don't have Poetry installed, follow the instructions on the official Poetry website.</p> <p>Once Poetry is installed, navigate to the cloned repository and install the dependencies: <pre><code>cd clientai\npoetry install\n</code></pre></p>"},{"location":"community/CONTRIBUTING/#activating-the-virtual-environment","title":"Activating the Virtual Environment","text":"<p>Poetry creates a virtual environment for your project. Activate it using:</p> <pre><code>poetry shell\n</code></pre>"},{"location":"community/CONTRIBUTING/#making-contributions","title":"Making Contributions","text":""},{"location":"community/CONTRIBUTING/#coding-standards","title":"Coding Standards","text":"<ul> <li>Follow PEP 8 guidelines.</li> <li>Write meaningful tests for new features or bug fixes.</li> </ul>"},{"location":"community/CONTRIBUTING/#testing-with-pytest","title":"Testing with Pytest","text":"<p>ClientAI uses pytest for testing. Run tests using: <pre><code>poetry run pytest\n</code></pre></p>"},{"location":"community/CONTRIBUTING/#linting","title":"Linting","text":"<p>Use mypy for type checking: <pre><code>mypy clientai\n</code></pre></p> <p>Use ruff for style: <pre><code>ruff check --fix\nruff format\n</code></pre></p> <p>Ensure your code passes linting before submitting.</p>"},{"location":"community/CONTRIBUTING/#submitting-your-contributions","title":"Submitting Your Contributions","text":""},{"location":"community/CONTRIBUTING/#creating-a-pull-request","title":"Creating a Pull Request","text":"<p>After making your changes:</p> <ul> <li>Push your changes to your fork.</li> <li>Open a pull request with a clear description of your changes.</li> <li>Update the README.md if necessary.</li> </ul>"},{"location":"community/CONTRIBUTING/#code-reviews","title":"Code Reviews","text":"<ul> <li>Address any feedback from code reviews.</li> <li>Once approved, your contributions will be merged into the main branch.</li> </ul>"},{"location":"community/CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Please adhere to our Code of Conduct to maintain a welcoming and inclusive environment.</p> <p>Thank you for contributing to ClientAI\ud83d\ude80</p>"},{"location":"community/LICENSE/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2024 Igor Benav</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"community/overview/","title":"Community Overview","text":"<p>Welcome to the project's community hub. Here, you'll find essential resources and guidelines that are crucial for contributing to and participating in the project. Please take the time to familiarize yourself with the following documents:</p>"},{"location":"community/overview/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Contributing</li> <li>Code of Conduct</li> <li>License</li> </ul>"},{"location":"community/overview/#contributing","title":"Contributing","text":"<p>View the Contributing Guidelines</p> <p>Interested in contributing to the project? Great! The contributing guidelines will provide you with all the information you need to get started. This includes how to submit issues, propose changes, and the process for submitting pull requests.</p>"},{"location":"community/overview/#code-of-conduct","title":"Code of Conduct","text":"<p>View the Code of Conduct</p> <p>The Code of Conduct outlines the standards and behaviors expected of our community members. It's crucial to ensure a welcoming and inclusive environment for everyone. Please take the time to read and adhere to these guidelines.</p>"},{"location":"community/overview/#license","title":"License","text":"<p>View the License</p> <p>The license document outlines the terms under which our project can be used, modified, and distributed. Understanding the licensing is important for both users and contributors of the project.</p> <p>Thank you for being a part of our community and for contributing to our project's success!</p>"},{"location":"examples/ai_dungeon_master/","title":"ClientAI Tutorial: Building an AI Dungeon Master","text":"<p>In this tutorial, we'll walk through the process of creating an AI-powered Dungeon Master using the ClientAI package. We'll explain each concept in detail and build our game step-by-step, providing context for every decision we make, both technical and gameplay-related.</p>"},{"location":"examples/ai_dungeon_master/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Setting Up the Project  2.5 Creating the Project Structure</li> <li>Creating the Game Structure</li> <li>Integrating Multiple AI Providers</li> <li>Developing the Enhanced AI Dungeon Master</li> <li>Main Script that Runs the Game</li> <li>Running the Game</li> <li>Conclusion and Further Improvements</li> </ol>"},{"location":"examples/ai_dungeon_master/#1-introduction","title":"1. Introduction","text":"<p>ClientAI is a Python package that provides a unified interface for interacting with multiple AI providers. In this tutorial, we'll use ClientAI to create an AI Dungeon Master that can generate story elements, NPC dialogues, and dynamic environments using different AI models.</p> <p>Our AI Dungeon Master will be a text-based role-playing game (RPG) where the game's content is dynamically generated by AI. This approach allows for infinite replayability and unique experiences for each player.</p> <p>We'll focus on explaining both technical decisions (such as class structures and AI interactions) and gameplay decisions (like character creation and game mechanics).</p> <p>The final result is available in this github repo.</p>"},{"location":"examples/ai_dungeon_master/#2-setting-up-the-project","title":"2. Setting Up the Project","text":"<p>First, let's set up our project and install the necessary dependencies.</p> <ol> <li>Create a new directory for your project:</li> </ol> <pre><code>mkdir ai_dungeon_master\ncd ai_dungeon_master\n</code></pre> <ol> <li> <p>Install ClientAI and its dependencies:</p> <p>If you want to use poetry, you may skip this part.</p> </li> </ol> <pre><code>pip install clientai[all]\n</code></pre> <p>This command installs ClientAI with support for all providers. If you only need specific providers, you can install them individually (e.g., <code>pip install clientai[openai]</code> for just OpenAI support).</p> <ol> <li> <p>Install additional dependencies:</p> <p>If you want to use poetry, you may also skip this part.</p> </li> </ol> <p>We'll need some additional packages for our project.</p> <pre><code>pip install requests\n</code></pre> <ul> <li> <p><code>requests</code>: For making HTTP requests to check if the local AI servers are running.</p> </li> <li> <p>Install Ollama:</p> </li> </ul> <p>Ollama is a local AI model server that we'll use to run the Llama 3 model. Follow these steps to install Ollama:</p> <ul> <li> <p>For macOS or Linux:   <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre></p> </li> <li> <p>For Windows:   Download the installer from the Ollama GitHub releases page and follow the installation instructions.</p> </li> <li> <p>Pull the Llama 3 model from Ollama:</p> </li> </ul> <p>After installing Ollama, you need to download the Llama 3 model. Run the following command:</p> <pre><code>ollama pull llama3\n</code></pre> <p>This command will download and set up the Llama 3 model for use with Ollama. The download might take some time depending on your internet connection.</p> <p>These imports will be used throughout our project:</p> <ul> <li><code>random</code>: For generating random numbers and making random choices.</li> <li><code>subprocess</code>: For starting and managing subprocesses like local AI servers.</li> <li><code>time</code>: For adding delays and managing timeouts.</li> <li><code>requests</code>: For making HTTP requests to check server availability.</li> <li><code>logging</code>: For logging information and errors.</li> <li><code>ClientAI</code>: The main class from the ClientAI package that we'll use to interact with AI providers.</li> </ul>"},{"location":"examples/ai_dungeon_master/#25-creating-the-project-structure","title":"2.5 Creating the Project Structure","text":"<p>Before we dive into the code, let's set up a proper project structure. This will help us organize our code and make it easier to maintain and expand in the future.</p> <ol> <li>Create the following directory structure:</li> </ol> <pre><code>clientai_dungeon_master/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 ai_dungeon_master/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 main.py\n    \u251c\u2500\u2500 game/\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 character.py\n    \u2502   \u251c\u2500\u2500 game_state.py\n    \u2502   \u2514\u2500\u2500 dungeon_master.py\n    \u251c\u2500\u2500 ai/\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 ai_providers.py\n    \u2502   \u2514\u2500\u2500 ollama_server.py\n    \u2514\u2500\u2500 utils/\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 text_utils.py\n</code></pre> <ol> <li> <p>Create a <code>pyproject.toml</code> file in the root directory with the following content:</p> <p>If you're using pip directly, you may skip this part</p> </li> </ol> <pre><code>[tool.poetry]\nname = \"clientai-dungeon-master\"\nversion = \"0.1.0\"\ndescription = \"An AI-powered dungeon master for text-based RPG adventures\"\nauthors = [\"Your Name &lt;your.email@example.com&gt;\"]\nreadme = \"README.md\"\npackages = [{include = \"clientai_dungeon_master\"}]\n\n[tool.poetry.dependencies]\npython = \"^3.11\"\nclientai = \"^0.1.2\"\nrequests = \"^2.32.3\"\npython-decouple = \"^3.8\"\n\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n</code></pre> <p>and run</p> <pre><code>poetry install\n</code></pre> <ol> <li>Create a <code>.gitignore</code> file in the root directory with the following content:</li> </ol> <pre><code># Python\n__pycache__/\n*.py[cod]\n*.pyo\n*.pyd\n.Python\nenv/\nvenv/\nENV/\n\n# Poetry\n.venv/\ndist/\n\n# Environment variables\n.env\n\n# IDEs\n.vscode/\n.idea/\n\n# Logs\n*.log\n\n# OS generated files\n.DS_Store\n.DS_Store?\n._*\n.Spotlight-V100\n.Trashes\nehthumbs.db\nThumbs.db\n</code></pre> <ol> <li>Create a <code>.env</code> file in the root directory to store your API keys:</li> </ol> <pre><code>OPENAI_API_KEY=your_openai_api_key_here\nREPLICATE_API_KEY=your_replicate_api_key_here\n</code></pre> <p>Remember to replace <code>your_openai_api_key_here</code> and <code>your_replicate_api_key_here</code> with your actual API keys.</p> <ol> <li>Move the relevant code into the appropriate files based on the new structure.</li> </ol> <p>This structure separates concerns, making the code more modular and easier to maintain. It also sets up the project for potential future expansion, such as adding more game features or integrating additional AI providers.</p>"},{"location":"examples/ai_dungeon_master/#3-creating-the-game-structure","title":"3. Creating the Game Structure","text":"<p>Before integrating AI, we'll create the basic structure of our game. This includes classes to represent the character, game state, and AI providers.</p>"},{"location":"examples/ai_dungeon_master/#character-class","title":"Character Class","text":"<p>The <code>Character</code> class represents the player's character in the game. It stores essential character information like name, race, class, background story, and stats.</p> ai_dungeon_master/game/character.py<pre><code>class Character:\n    def __init__(self, name: str, race: str, class_type: str, background: str, stats: dict):\n        self.name = name\n        self.race = race\n        self.class_type = class_type\n        self.background = background\n        self.stats = stats\n\n    def __str__(self):\n        return f\"Name: {self.name}, Race: {self.race}, Class: {self.class_type}, Background: {self.background}, Stats: {self.stats}\"\n</code></pre> <p>Here we define a character with attributes like a name, race, class, background and stats (like Strength, Intelligence, Wisdom). This is really simple, but will be enough to customize what happens in the story.</p> <p>We'll also define the <code>__str__</code> method to be able to print the character's details easily.</p>"},{"location":"examples/ai_dungeon_master/#gamestate-class","title":"GameState Class","text":"<p>The <code>GameState</code> class keeps track of the game's current state, including the character's status, location, inventory, health, experience, and quests.</p> ai_dungeon_master/game/game_state.py<pre><code>from typing import Optional\n\nfrom .character import Character\n\nclass GameState:\n    def __init__(self, character: Character):\n        self.character = character\n        self.location = \"entrance\"\n        self.inventory = []\n        self.health = 100\n        self.experience = 0\n        self.quests = []\n\n    def update(self, location: Optional[str] = None, item: Optional[str] = None, health_change: int = 0, exp_gain: int = 0, quest: Optional[str] = None):\n        if location:\n            self.location = location\n        if item:\n            self.inventory.append(item)\n        self.health = max(0, min(100, self.health + health_change))\n        self.experience += exp_gain\n        if quest:\n            self.quests.append(quest)\n\n    def __str__(self):\n        return f\"{str(self.character)}\\nLocation: {self.location}, Health: {self.health}, XP: {self.experience}, Inventory: {', '.join(self.inventory)}, Quests: {', '.join(self.quests)}\"\n</code></pre> <p>We keep track of the state to keep a more consistent experience, we can't expect this to be always generated by the llm. We need to pass the game state as a guide to generate the content.</p> <p>The update method allows easy updates to the game state, we'll keep health within 0 to 100, and add an inventory and quests to add more depth to the game.</p>"},{"location":"examples/ai_dungeon_master/#4-integrating-multiple-ai-providers","title":"4. Integrating Multiple AI Providers","text":"<p>We'll use ClientAI to create a class that manages interactions with different AI providers. This abstraction allows us to switch between providers seamlessly.</p>"},{"location":"examples/ai_dungeon_master/#aiproviders-class","title":"AIProviders Class","text":"ai_dungeon_master/ai/ai_providers.py<pre><code>from typing import List\n\nfrom clientai import ClientAI\n\nclass AIProviders:\n    def __init__(self):\n        self.openai = ClientAI('openai', api_key=openai_token)\n        self.replicate = ClientAI('replicate', api_key=replicate_token)\n        self.ollama = ClientAI('ollama', host=\"http://localhost:11434\")\n\n    def chat(\n        self,\n        messages: List[dict],\n        provider: str = 'openai',\n        openai_model=\"gpt-4o-mini\",\n        replicate_model=\"meta/meta-llama-3-8b-instruct\",\n        ollama_model=\"llama3\",\n    ):\n        if provider == 'openai':\n            return self.openai.chat(messages, model=openai_model, stream=True)\n        elif provider == 'replicate':\n            return self.replicate.chat(messages, model=replicate_model, stream=True)\n        elif provider == 'ollama':\n            return self.ollama.chat(messages, model=ollama_model, stream=True)\n        else:\n            raise ValueError(f\"Unknown provider: {provider}\")\n</code></pre> <p>We create instances of ClientAI for each provider with the necessary API keys or host information, then abstract the chat method to allow for easy switching between AI providers.</p> <p>We are going to use ClientAI to use multiple AI models from different providers, since we want to find what is the best model for each task balancing performance and costs.</p>"},{"location":"examples/ai_dungeon_master/#managing-api-keys-with-python-decouple-and-a-env-file","title":"Managing API Keys with python-decouple and a .env File","text":"<p>To securely handle your API keys without exposing them in your codebase, you can use the python-decouple package and store your keys in a .env file. This approach keeps sensitive information out of your code and version control.</p> <ol> <li>Install python-decouple:    You may skip this if you used poetry</li> </ol> <pre><code>pip install python-decouple\n</code></pre> <ol> <li>Create a .env File:    In your project's root directory, make sure the <code>.env</code> has your API keys:</li> </ol> <pre><code>OPENAI_API_KEY=your_openai_api_key_here\nREPLICATE_API_KEY=your_replicate_api_key_here\n</code></pre> <p>Replace <code>your_openai_api_key_here</code> and <code>your_replicate_api_key_here</code> with your actual API keys.</p> <ol> <li>Ensure .env is added to .gitignore:    To prevent the .env file from being tracked by version control, ensure it is in your .gitignore file:</li> </ol> <pre><code># .gitignore\n.env\n</code></pre> <p>This ensures your API keys remain private and aren't pushed to repositories like GitHub.</p> <ol> <li>Access the API Keys in Your Code:    Import <code>config</code> from decouple and retrieve the API keys:</li> </ol> ai_dungeon_master/ai/ai_providers.py<pre><code>from decouple import config\n\nopenai_token = config('OPENAI_API_KEY')\nreplicate_token = config('REPLICATE_API_KEY')\n</code></pre> <p>Now, you can use these variables when initializing your AI providers.</p> <ol> <li>Update the AIProviders Class:    ai_dungeon_master/ai/ai_providers.py<pre><code>from typing import List\n\nfrom clientai import ClientAI\nfrom decouple import config\n\nopenai_token = config('OPENAI_API_KEY')\nreplicate_token = config('REPLICATE_API_KEY')he ol\n\nclass AIProviders:\n    def __init__(self):\n        self.openai = ClientAI('openai', api_key=openai_token)\n        self.replicate = ClientAI('replicate', api_key=replicate_token)\n        self.ollama = ClientAI('ollama', host=\"http://localhost:11434\")\n\n ...\n</code></pre></li> </ol>"},{"location":"examples/ai_dungeon_master/#managing-ai-servers","title":"Managing AI Servers","text":"<p>We need to ensure that local AI servers (like Ollama) are running before the game starts, so let's define a function to start ollama.</p> ai_dungeon_master/ai/ollama_server.py<pre><code>import subprocess\nimport time\nimport requests\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef start_ollama_server(timeout: int = 30, check_interval: float = 1.0):\n    \"\"\"\n    Start the Ollama server and wait for it to be ready.\n    \"\"\"\n    logging.info(\"Starting Ollama server...\")\n\n    try:\n        process = subprocess.Popen(\n            ['ollama', 'serve'],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n    except subprocess.SubprocessError as e:\n        logging.error(f\"Failed to start Ollama process: {e}\")\n        raise\n\n    start_time = time.time()\n    while time.time() - start_time &lt; timeout:\n        try:\n            response = requests.get('http://localhost:11434', timeout=5)\n            if response.status_code == 200:\n                logging.info(\"Ollama server is ready.\")\n                return process\n        except requests.ConnectionError:\n            pass\n        except requests.RequestException as e:\n            logging.error(f\"Unexpected error when checking Ollama server: {e}\")\n            process.terminate()\n            raise\n\n        if process.poll() is not None:\n            stdout, stderr = process.communicate()\n            logging.error(f\"Ollama process terminated unexpectedly. stdout: {stdout}, stderr: {stderr}\")\n            raise subprocess.SubprocessError(\"Ollama process terminated unexpectedly\")\n\n        time.sleep(check_interval)\n\n    process.terminate()\n    raise TimeoutError(f\"Ollama server did not start within {timeout} seconds\")\n</code></pre> <p>By managing the server startup within the code, we reduce the setup burden on the player.</p>"},{"location":"examples/ai_dungeon_master/#5-developing-the-enhanced-ai-dungeon-master","title":"5. Developing the Enhanced AI Dungeon Master","text":"<p>Now we'll develop the main class that controls the game logic and interactions with AI models.</p>"},{"location":"examples/ai_dungeon_master/#enhancedaidungeonmaster-class","title":"EnhancedAIDungeonMaster Class","text":"ai_dungeon_master/game/dungeon_master.py<pre><code>from typing import Tuple, List\nimport random\nimport time\n\nfrom ai.ai_providers import AIProviders\nfrom utils.text_utils import print_separator\nfrom game.character import Character\nfrom game.game_state import GameState\n\nclass EnhancedAIDungeonMaster:\n    def __init__(self):\n        self.ai = AIProviders()\n        self.conversation_history = []\n        self.game_state = None\n\n    # Methods will be added here...\n</code></pre>"},{"location":"examples/ai_dungeon_master/#creating-the-character","title":"Creating the Character","text":"<p>We need a method to create the player's character. We'll use AI to do this automatically for us:</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def create_character(self):\n        print(\"Let's create your character!\")\n        name = input(\"What is your character's name? \")\n\n        # We start by defining a prompt\n        character_prompt = f\"\"\"\n        Create a character for a fantasy RPG with the following details:\n        Name: {name}\n\n        Please provide:\n        1. A suitable race (e.g., Human, Elf, Dwarf, etc.)\n        2. A class (e.g., Warrior, Mage, Rogue, etc.)\n        3. A brief background story (2-3 sentences)\n        4. Basic stats (Strength, Dexterity, Constitution, Intelligence, Wisdom, Charisma) on a scale of 1-20\n\n        Format the response as follows:\n        Race: [race]\n        Class: [class]\n        Background: [background story]\n        Stats:\n        - Strength: [value]\n        - Dexterity: [value]\n        - Constitution: [value]\n        - Intelligence: [value]\n        - Wisdom: [value]\n        - Charisma: [value]\n        \"\"\"\n\n        # And we add this prompt to our chat history\n        self.add_to_history(\"user\", character_prompt)\n        character_info = self.print_stream(self.ai.chat(self.conversation_history, provider='openai'))\n\n        # Parse the character info\n        lines = character_info.strip().split('\\n')\n        race = class_type = background = \"\"\n        stats = {}\n\n        for line in lines:\n            if line.startswith(\"Race:\"):\n                race = line.split(\": \", 1)[1].strip()\n            elif line.startswith(\"Class:\"):\n                class_type = line.split(\": \", 1)[1].strip()\n            elif line.startswith(\"Background:\"):\n                background = line.split(\": \", 1)[1].strip()\n            elif \":\" in line and not line.startswith(\"Stats:\"):\n                key, value = line.split(\":\", 1)\n                key = key.strip(\"- \")\n                try:\n                    stats[key] = int(value.strip())\n                except ValueError:\n                    stats[key] = random.randint(1, 20)\n\n        # Just in case, let's ensure it'the player has stats\n        # If any stat is missing, assign a random value\n        for stat in [\"Strength\", \"Dexterity\", \"Constitution\", \"Intelligence\", \"Wisdom\", \"Charisma\"]:\n            if stat not in stats:\n                stats[stat] = random.randint(1, 20)\n\n        # And let's also ensure other required attributes are assigned\n        # If race, class, or background is empty, assign default values\n        race = race or \"Human\"\n        class_type = class_type or \"Adventurer\"\n        background = background or \"A mysterious traveler with an unknown past.\"\n\n        return Character(name, race, class_type, background, stats)\n</code></pre> <p>We'll use GPT 4o mini to create initial stuff we need, like the race, class, background etc, and extract the information from the generated content to handle errors.</p> <p>Note that since we are leaving this information to the LLM, the name will influence the attributes. If you need a more consistently random generation, do it in the python code and just pass it to the prompt.</p>"},{"location":"examples/ai_dungeon_master/#maintaining-conversation-history","title":"Maintaining Conversation History","text":"<p>To provide context to the AI, we maintain a conversation history.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def add_to_history(self, role: str, content: str):\n        if not self.conversation_history or self.conversation_history[-1]['content'] != content:\n            self.conversation_history.append({\"role\": role, \"content\": content})\n            if len(self.conversation_history) &gt; 10:\n                self.conversation_history = self.conversation_history[-10:]\n</code></pre> <p>Here we will ensure we don't add the same message twice. Plus, we are limiting the conversation history to 10 messages to prevent exceeding token limits.</p>"},{"location":"examples/ai_dungeon_master/#generating-the-environment","title":"Generating the Environment","text":"<p>Next, let's create detailed environments to enhance the imersion.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def generate_environment(self):\n        if not hasattr(self, 'current_environment'):\n            prompt = f\"\"\"\n            The character {self.game_state.character.name} is a {self.game_state.character.race} {self.game_state.character.class_type} \n            currently in the {self.game_state.location}.\n\n            Describe the current environment in detail, focusing on:\n            1. The physical setting and atmosphere\n            2. Any notable NPCs present\n            3. Interesting objects or features\n\n            Do not create a new character or change any existing character details.\n            Do not include any actions or dialogue for {self.game_state.character.name}.\n\n            End your description with one of these tags if appropriate:\n            [INTERACT_OPPORTUNITY] - if there's a chance for the player to interact with someone or something\n            [QUEST_OPPORTUNITY] - if there's a potential quest or mission available\n            \"\"\"\n            self.add_to_history(\"user\", prompt)\n            self.current_environment = self.ai.chat(self.conversation_history, provider='openai')\n        return self.current_environment\n</code></pre> <p>Here we instruct the AI to provide specific details, and we use tags for opportunities. We'll parse these tags <code>INTERACT_OPPORTUNITY</code> and <code>QUEST_OPPORTUNITY</code> later to perform other actions.</p> <p>We'll also store the environment description to avoid regenerating it unnecessarily.</p>"},{"location":"examples/ai_dungeon_master/#handling-player-actions","title":"Handling Player Actions","text":"<p>Now let's process the player's actions and generate outcomes. We'll run this one locally with ollama.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def handle_player_action(self, action):\n        prompt = f\"\"\"\n        The player ({self.game_state.character.name}, a {self.game_state.character.race} {self.game_state.character.class_type}) \n        attempts to {action} in {self.game_state.location}. \n        Describe the immediate result of this action, focusing on the environment and NPCs' reactions.\n        Do not generate any further actions or dialogue for {self.game_state.character.name}.\n        If the player is trying to interact with an NPC, end your response with [NPC_INTERACTION: &lt;npc_name&gt;].\n        \"\"\"\n        self.add_to_history(\"user\", prompt)\n        return self.ai.chat(self.conversation_history, provider='ollama')\n</code></pre> <p>Here we pass what the player wants to do to the AI and generate the outcomes for the players actions. We are also using a tag here for interactions, so we can process those in a different way.</p>"},{"location":"examples/ai_dungeon_master/#generating-npc-dialogue","title":"Generating NPC Dialogue","text":"<p>Next, let's create a function to generate a dialogue with an npc. We'll use replicate with llama3 8b for this.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def generate_npc_dialogue(self, npc_name: str, player_input: str):\n        prompt = f\"\"\"\n        The player ({self.game_state.character.name}) said to {npc_name}: \"{player_input}\"\n        Generate a single, natural response from {npc_name}, addressing the player's input directly.\n        If the player is asking about items for sale, list 2-3 specific items with brief descriptions and prices.\n        Do not include any actions or responses from the player character.\n        Keep the response concise and relevant to the player's input.\n        Do not include any formatting tags, headers, or quotation marks in your response.\n        Respond as if you are {npc_name} speaking directly to the player.\n        \"\"\"\n        self.add_to_history(\"user\", prompt)\n        return self.ai.chat(self.conversation_history, provider='replicate')\n</code></pre> <p>Note that in the prompt we ensure the AI provides responses that are in character and appropriate, so we can pass this directly to the player.</p>"},{"location":"examples/ai_dungeon_master/#handling-conversations","title":"Handling Conversations","text":"<p>We manage conversations with NPCs in a separate method. We start with a conversation loop, to allow the player to have a back-and-forth dialogue with an NPC, and we reset the conversation history to focus the AI on the dialogue.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def handle_conversation(self, npc_name):\n        print(f\"\\nYou are now in conversation with {npc_name}.\")\n        self.conversation_history = [\n            {\"role\": \"system\", \"content\": f\"You are {npc_name}, speaking directly to the player. Respond naturally and in character.\"}\n        ]\n        while True:\n            player_input = input(f\"\\nWhat do you say to {npc_name}? (or type 'end conversation' to stop): \")\n            if player_input.lower() == \"end conversation\":\n                print(f\"\\nYou end your conversation with {npc_name}.\")\n                break\n\n            print(f\"\\n{npc_name}:\")\n            self.print_stream(self.generate_npc_dialogue(npc_name, player_input))\n</code></pre> <p>We also add the possibility for the player to end the conversation at any time.</p>"},{"location":"examples/ai_dungeon_master/#updating-the-game-state","title":"Updating the Game State","text":"<p>We update the game state based on the outcomes provided by the AI.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def update_game_state(self, outcome):\n        if \"found\" in outcome.lower():\n            item = outcome.split(\"found\")[1].split(\".\")[0].strip()\n            self.game_state.update(item=item)\n        if \"new area\" in outcome.lower():\n            new_location = outcome.split(\"new area\")[1].split(\".\")[0].strip()\n            self.game_state.update(location=new_location)\n        if \"damage\" in outcome.lower():\n            self.game_state.update(health_change=-10)\n        if \"healed\" in outcome.lower():\n            self.game_state.update(health_change=10)\n        if \"quest\" in outcome.lower():\n            quest = outcome.split(\"quest\")[1].split(\".\")[0].strip()\n            self.game_state.update(quest=quest)\n        self.game_state.update(exp_gain=5)\n</code></pre> <p>This is a simpler way to do it, but we will just look for keywords in the AI's response to determine what changes to make. This isn't the most consistent way to do it, but is easy to do and will easily allow the game to respond to the player's actions, making the experience feel more dynamic.</p>"},{"location":"examples/ai_dungeon_master/#processing-story-elements","title":"Processing Story Elements","text":"<p>Let's process the AI-generated story to extract content and any special flags.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def process_story(self, story_generator) -&gt; Tuple[str, List[str]]:\n        story = self.print_stream(story_generator, print_output=True)\n        story_lines = story.split('\\n')\n\n        flags = []\n        for line in reversed(story_lines):\n            if line.strip().startswith('[') and line.strip().endswith(']'):\n                flags.append(line.strip('[').strip(']'))\n                story_lines.remove(line)\n            else:\n                break\n\n        story_content = '\\n'.join(story_lines).strip()\n\n        if any(flag.startswith(\"NPC_INTERACTION:\") for flag in flags):\n            npc_name = next(flag.split(':')[1].strip() for flag in flags if flag.startswith(\"NPC_INTERACTION:\"))\n            return story_content, npc_name\n        else:\n            return story_content, flags\n</code></pre> <p>Here is where we'll actually separates the special tags we defined earlier from the story content and ensure the player sees a coherent story without tags.</p>"},{"location":"examples/ai_dungeon_master/#printing-streamed-content","title":"Printing Streamed Content","text":"<p>We also don't want to wait until the whole content is generated to print, so let's define a function to display the AI's response in real-time, simulating typing.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def print_stream(self, stream, print_output=True) -&gt; str:\n        full_text = \"\"\n        for chunk in stream:\n            if print_output:\n                print(chunk, end='', flush=True)\n            full_text += chunk\n            time.sleep(0.03)\n        if print_output:\n            print()\n        return full_text\n</code></pre>"},{"location":"examples/ai_dungeon_master/#main-game-loop","title":"Main Game Loop","text":"<p>Finally, we bring everything together in the play_game method.</p> ai_dungeon_master/game/dungeon_master.py<pre><code>class EnhancedAIDungeonMaster:\n    ...\n    def play_game(self):\n        print(\"Welcome to the Dungeon!\")\n        character = self.create_character()\n        self.game_state = GameState(character)\n\n        print(\"\\nYour adventure begins...\")\n        while True:\n            print_separator()\n            environment_description, env_flags = self.process_story(self.generate_environment())\n\n            if \"INTERACT_OPPORTUNITY\" in env_flags:\n                print(\"\\nThere seems to be an opportunity to interact.\")\n            if \"QUEST_OPPORTUNITY\" in env_flags:\n                print(\"\\nThere might be a quest available.\")\n\n            action = input(\"\\nWhat do you do? \")\n            if action.lower() == \"quit\":\n                break\n\n            print(\"\\nOutcome:\")\n            outcome, npc_interaction = self.process_story(self.handle_player_action(action))\n\n            self.update_game_state(outcome)\n\n            if npc_interaction:\n                self.handle_conversation(npc_interaction)\n\n            print_separator()\n            print(f\"Current state: {str(self.game_state)}\")\n\n            if self.game_state.health &lt;= 0:\n                print(\"Game Over! Your health reached 0.\")\n                break\n\n            if hasattr(self, 'current_environment'):\n                del self.current_environment\n</code></pre> <p>The game loop continuously processes player actions and updates the game state, new environments are generated to keep the game dynamic and the player is allowed to quit whenever they want.</p> <p>Plus, the game is over if health reaches zero.</p>"},{"location":"examples/ai_dungeon_master/#helper-methods","title":"Helper Methods","text":"<p>Let's also create some methods for improved user experience, we want to separate content to make it easier to see and also create a print_slowly to simulate streamed content in important messages.</p> ai_dungeon_master/utils/text_utils.py<pre><code>import time\n\ndef print_separator(self):\n    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n\ndef print_slowly(text, delay=0.03):\n    for char in text:\n        print(char, end='', flush=True)\n        time.sleep(delay)\n    print()\n</code></pre>"},{"location":"examples/ai_dungeon_master/#6-main-script-that-runs-the-game","title":"6. Main Script that Runs the Game","text":"<p>At our main script, we initialize and start the game.</p> ai_dungeon_master/main.py<pre><code>from game.dungeon_master import EnhancedAIDungeonMaster\nfrom utils.text_utils import print_slowly\nfrom ai.ollama_server import start_ollama_server\n\ndef main():\n    print_slowly(\"Welcome to the AI Dungeon Master!\")\n    print_slowly(\"Prepare for an adventure guided by multiple AI models.\")\n    print_slowly(\"Type 'quit' at any time to exit the game.\")\n    print()\n\n    # Start the Ollama server before the game begins\n    ollama_process = start_ollama_server()\n\n    game = EnhancedAIDungeonMaster()\n    game.play_game()\n\n    print_slowly(\"Thank you for playing AI Dungeon Master!\")\n\n    # Terminate the Ollama server when the game ends\n    if ollama_process:\n        ollama_process.terminate()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/ai_dungeon_master/#7-running-the-game","title":"7. Running the Game","text":"<ol> <li> <p>Ensure you're in the root directory of the project.</p> </li> <li> <p>Run the game using Poetry:</p> </li> </ol> <pre><code>poetry run python ai_dungeon_master/main.py\n</code></pre> <p>Or directly if you used pip:</p> <pre><code>python ai_dungeon_master/main.py\n</code></pre> <p>This command will execute the <code>main.py</code> file, which should contain the game initialization and main loop.</p>"},{"location":"examples/ai_dungeon_master/#8-conclusion-and-further-improvements","title":"8. Conclusion and Further Improvements","text":"<p>Congratulations! You've now created an AI Dungeon Master using the ClientAI package. This project demonstrates how to integrate multiple AI providers and manage game logic to create a dynamic and engaging text-based RPG.</p>"},{"location":"examples/ai_dungeon_master/#potential-improvements","title":"Potential Improvements:","text":"<ol> <li>Error Handling: Implement try-except blocks to handle exceptions and improve robustness.</li> <li>Saving and Loading: Add functionality to save and load game states.</li> <li>Combat System: Develop a combat system that uses character stats and AI to determine outcomes.</li> <li>Quest Management: Create a more complex quest system with objectives and rewards.</li> <li>Multiplayer: Explore options for multiplayer interactions.</li> <li>User Interface: Develop a GUI for a more user-friendly experience.</li> <li>AI Fine-Tuning: Customize AI models for more consistent and relevant responses.</li> </ol> <p>By implementing these improvements, you can further enhance the gameplay experience and create an even more immersive and engaging AI-driven RPG.</p>"},{"location":"examples/overview/","title":"Examples Overview","text":"<p>Welcome to the Examples section of the ClientAI documentation. This section provides practical, real-world examples of how to use ClientAI in various applications. Whether you're a beginner looking to get started or an experienced developer seeking inspiration for more complex projects, these examples will demonstrate the versatility and power of ClientAI.</p>"},{"location":"examples/overview/#featured-examples","title":"Featured Examples","text":"<p>Our examples cover a range of applications, from simple text generation to more complex AI-driven systems. Here's an overview of what you'll find in this section:</p> <ol> <li> <p>AI Dungeon Master: A text-based RPG that uses multiple AI providers to create an interactive storytelling experience.</p> <ul> <li>AI Dungeon Master Tutorial</li> </ul> </li> <li> <p>Chatbot Assistant: A simple chatbot that can answer questions and engage in conversation using ClientAI.</p> <ul> <li>Soon</li> </ul> </li> <li> <p>Sentiment Analyzer: An application that analyzes the sentiment of given text using different AI models.</p> <ul> <li>Soon</li> </ul> </li> </ol>"},{"location":"examples/overview/#usage","title":"Usage","text":"<p>Each example is documented on its own page, where you'll find:</p> <ul> <li>A detailed explanation of the example's purpose and functionality</li> <li>Step-by-step instructions for implementing the example</li> <li>Code snippets and full source code</li> <li>Explanations of key ClientAI features used in the example</li> <li>Tips for customizing and extending the example</li> </ul>"},{"location":"examples/overview/#quick-start-example","title":"Quick Start Example","text":"<p>Here's a simple example to get you started with ClientAI:</p> <pre><code>from clientai import ClientAI\n\n# Initialize the client\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Generate a short story\nprompt = \"Write a short story about a robot learning to paint.\"\nresponse = client.generate_text(prompt, model=\"gpt-3.5-turbo\")\n\nprint(response)\n</code></pre> <p>For more general usage instructions, please refer to our Quickstart Guide.</p>"},{"location":"examples/overview/#customizing-examples","title":"Customizing Examples","text":"<p>Feel free to use these examples as starting points for your own projects. You can modify and extend them to suit your specific needs. If you create an interesting project using ClientAI, we'd love to hear about it!</p>"},{"location":"examples/overview/#contributing","title":"Contributing","text":"<p>We welcome contributions to our examples collection! If you've created an example that you think would be valuable to others, please consider submitting it. Check out our Contributing Guidelines for more information on how to contribute.</p>"},{"location":"examples/overview/#feedback","title":"Feedback","text":"<p>Your feedback helps us improve our examples and documentation. If you have suggestions for new examples, improvements to existing ones, or any other feedback, please let us know through GitHub issues or our community channels.</p> <p>Explore each example to see ClientAI in action and learn how to implement AI-driven features in your own projects.</p>"},{"location":"usage/chat_functionality/","title":"Chat Functionality in ClientAI","text":"<p>This guide covers how to leverage ClientAI's chat functionality. You'll learn about creating chat conversations, managing context, and handling chat-specific features across supported providers.</p>"},{"location":"usage/chat_functionality/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Chat Interaction</li> <li>Managing Conversation Context</li> <li>Advanced Chat Features</li> <li>Provider-Specific Chat Capabilities</li> <li>Best Practices</li> </ol>"},{"location":"usage/chat_functionality/#basic-chat-interaction","title":"Basic Chat Interaction","text":"<p>To use the chat functionality in ClientAI, use the <code>chat</code> method:</p> <pre><code>from clientai import ClientAI\n\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hello, who are you?\"}\n]\n\nresponse = client.chat(messages, model=\"gpt-3.5-turbo\")\nprint(response)\n\n# Continue the conversation\nmessages.append({\"role\": \"assistant\", \"content\": response})\nmessages.append({\"role\": \"user\", \"content\": \"What can you help me with?\"})\n\nresponse = client.chat(messages, model=\"gpt-3.5-turbo\")\nprint(response)\n</code></pre> <p>This example demonstrates a simple back-and-forth conversation.</p>"},{"location":"usage/chat_functionality/#managing-conversation-context","title":"Managing Conversation Context","text":"<p>Effective context management is crucial for coherent conversations:</p> <pre><code>conversation = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in Python programming.\"},\n    {\"role\": \"user\", \"content\": \"How do I use list comprehensions in Python?\"}\n]\n\nresponse = client.chat(conversation, model=\"gpt-3.5-turbo\")\nprint(response)\n\nconversation.append({\"role\": \"assistant\", \"content\": response})\nconversation.append({\"role\": \"user\", \"content\": \"Can you give an example?\"})\n\nresponse = client.chat(conversation, model=\"gpt-3.5-turbo\")\nprint(response)\n</code></pre> <p>This example shows how to maintain context across multiple exchanges, including a system message to set the assistant's role.</p>"},{"location":"usage/chat_functionality/#advanced-chat-features","title":"Advanced Chat Features","text":""},{"location":"usage/chat_functionality/#streaming-chat-responses","title":"Streaming Chat Responses","text":"<p>For real-time conversation, you can stream chat responses:</p> <pre><code>conversation = [\n    {\"role\": \"user\", \"content\": \"Tell me a long story about space exploration\"}\n]\n\nfor chunk in client.chat(conversation, model=\"gpt-3.5-turbo\", stream=True):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"usage/chat_functionality/#temperature-and-top-p-sampling","title":"Temperature and Top-p Sampling","text":"<p>Adjust the creativity and randomness of responses:</p> <pre><code>response = client.chat(\n    conversation,\n    model=\"gpt-3.5-turbo\",\n    temperature=0.7,\n    top_p=0.9\n)\n</code></pre>"},{"location":"usage/chat_functionality/#provider-specific-chat-capabilities","title":"Provider-Specific Chat Capabilities","text":"<p>Different providers may offer unique chat features:</p>"},{"location":"usage/chat_functionality/#openai","title":"OpenAI","text":"<pre><code>openai_client = ClientAI('openai', api_key=\"your-openai-api-key\")\n\nresponse = openai_client.chat(\n    [{\"role\": \"user\", \"content\": \"Translate 'Hello, world!' to Japanese\"}],\n    model=\"gpt-4\"\n)\n</code></pre>"},{"location":"usage/chat_functionality/#replicate","title":"Replicate","text":"<pre><code>replicate_client = ClientAI('replicate', api_key=\"your-replicate-api-key\")\n\nresponse = replicate_client.chat(\n    [{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n    model=\"meta/llama-2-70b-chat:latest\"\n)\n</code></pre>"},{"location":"usage/chat_functionality/#ollama","title":"Ollama","text":"<pre><code>ollama_client = ClientAI('ollama', host=\"http://localhost:11434\")\n\nresponse = ollama_client.chat(\n    [{\"role\": \"user\", \"content\": \"What are the three laws of robotics?\"}],\n    model=\"llama2\"\n)\n</code></pre>"},{"location":"usage/chat_functionality/#best-practices","title":"Best Practices","text":"<ol> <li>Context Management: Keep track of the conversation history, but be mindful of token limits.</li> </ol> <pre><code>max_context_length = 10\nif len(conversation) &gt; max_context_length:\n    conversation = conversation[-max_context_length:]\n</code></pre> <ol> <li>Error Handling: Implement robust error handling for chat interactions:</li> </ol> <pre><code>try:\n    response = client.chat(conversation, model=\"gpt-3.5-turbo\")\nexcept Exception as e:\n    print(f\"An error occurred during chat: {e}\")\n    response = \"I'm sorry, I encountered an error. Could you please try again?\"\n</code></pre> <ol> <li>User Input Validation: Validate and sanitize user inputs to prevent potential issues:</li> </ol> <pre><code>def sanitize_input(user_input):\n    # Implement appropriate sanitization logic\n    return user_input.strip()\n\nuser_message = sanitize_input(input(\"Your message: \"))\nconversation.append({\"role\": \"user\", \"content\": user_message})\n</code></pre> <ol> <li>Graceful Fallbacks: Implement fallback mechanisms for when the AI doesn't understand or can't provide a suitable response:</li> </ol> <pre><code>if not response or response.lower() == \"i don't know\":\n    response = \"I'm not sure about that. Could you please rephrase or ask something else?\"\n</code></pre> <ol> <li>Model Selection: Choose appropriate models based on the complexity of your chat application:</li> </ol> <pre><code>model = \"gpt-4\" if complex_conversation else \"gpt-3.5-turbo\"\nresponse = client.chat(conversation, model=model)\n</code></pre> <ol> <li>Conversation Resetting: Provide options to reset or start new conversations:</li> </ol> <pre><code>def reset_conversation():\n    return [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n\n# Usage\nconversation = reset_conversation()\n</code></pre> <p>By following these guidelines and exploring the various features available, you can create sophisticated chat applications using ClientAI across different AI providers.</p>"},{"location":"usage/error_handling/","title":"Error Handling in ClientAI","text":"<p>ClientAI provides a robust error handling system that unifies exceptions across different AI providers. This guide covers how to handle potential errors when using ClientAI.</p>"},{"location":"usage/error_handling/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Exception Hierarchy</li> <li>Handling Errors</li> <li>Provider-Specific Error Mapping</li> <li>Best Practices</li> </ol>"},{"location":"usage/error_handling/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>ClientAI uses a custom exception hierarchy to provide consistent error handling across different AI providers:</p> <pre><code>from clientai.exceptions import (\n    ClientAIError,\n    AuthenticationError,\n    RateLimitError,\n    InvalidRequestError,\n    ModelError,\n    TimeoutError,\n    APIError\n)\n</code></pre> <ul> <li><code>ClientAIError</code>: Base exception class for all ClientAI errors.</li> <li><code>AuthenticationError</code>: Raised when there's an authentication problem with the AI provider.</li> <li><code>RateLimitError</code>: Raised when the AI provider's rate limit is exceeded.</li> <li><code>InvalidRequestError</code>: Raised when the request to the AI provider is invalid.</li> <li><code>ModelError</code>: Raised when there's an issue with the specified model.</li> <li><code>TimeoutError</code>: Raised when a request to the AI provider times out.</li> <li><code>APIError</code>: Raised when there's an API-related error from the AI provider.</li> </ul>"},{"location":"usage/error_handling/#handling-errors","title":"Handling Errors","text":"<p>Here's how to handle potential errors when using ClientAI:</p> <pre><code>from clientai import ClientAI\nfrom clientai.exceptions import (\n    ClientAIError,\n    AuthenticationError,\n    RateLimitError,\n    InvalidRequestError,\n    ModelError,\n    TimeoutError,\n    APIError\n)\n\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\ntry:\n    response = client.generate_text(\"Tell me a joke\", model=\"gpt-3.5-turbo\")\n    print(f\"Generated text: {response}\")\nexcept AuthenticationError as e:\n    print(f\"Authentication error: {e}\")\nexcept RateLimitError as e:\n    print(f\"Rate limit exceeded: {e}\")\nexcept InvalidRequestError as e:\n    print(f\"Invalid request: {e}\")\nexcept ModelError as e:\n    print(f\"Model error: {e}\")\nexcept TimeoutError as e:\n    print(f\"Request timed out: {e}\")\nexcept APIError as e:\n    print(f\"API error: {e}\")\nexcept ClientAIError as e:\n    print(f\"An unexpected ClientAI error occurred: {e}\")\n</code></pre>"},{"location":"usage/error_handling/#provider-specific-error-mapping","title":"Provider-Specific Error Mapping","text":"<p>ClientAI maps provider-specific errors to its custom exception hierarchy. For example:</p>"},{"location":"usage/error_handling/#openai","title":"OpenAI","text":"<pre><code>def _map_exception_to_clientai_error(self, e: Exception) -&gt; None:\n    error_message = str(e)\n    status_code = getattr(e, 'status_code', None)\n\n    if isinstance(e, OpenAIAuthenticationError) or \"incorrect api key\" in error_message.lower():\n        raise AuthenticationError(error_message, status_code, original_error=e)\n    elif status_code == 429 or \"rate limit\" in error_message.lower():\n        raise RateLimitError(error_message, status_code, original_error=e)\n    elif status_code == 404 or \"not found\" in error_message.lower():\n        raise ModelError(error_message, status_code, original_error=e)\n    elif status_code == 400 or \"invalid\" in error_message.lower():\n        raise InvalidRequestError(error_message, status_code, original_error=e)\n    elif status_code == 408 or \"timeout\" in error_message.lower():\n        raise TimeoutError(error_message, status_code, original_error=e)\n    elif status_code and status_code &gt;= 500:\n        raise APIError(error_message, status_code, original_error=e)\n\n    raise ClientAIError(error_message, status_code, original_error=e)\n</code></pre>"},{"location":"usage/error_handling/#replicate","title":"Replicate","text":"<pre><code>def _map_exception_to_clientai_error(self, e: Exception, status_code: int = None) -&gt; ClientAIError:\n    error_message = str(e)\n    status_code = status_code or getattr(e, 'status_code', None)\n\n    if \"authentication\" in error_message.lower() or \"unauthorized\" in error_message.lower():\n        return AuthenticationError(error_message, status_code, original_error=e)\n    elif \"rate limit\" in error_message.lower():\n        return RateLimitError(error_message, status_code, original_error=e)\n    elif \"not found\" in error_message.lower():\n        return ModelError(error_message, status_code, original_error=e)\n    elif \"invalid\" in error_message.lower():\n        return InvalidRequestError(error_message, status_code, original_error=e)\n    elif \"timeout\" in error_message.lower() or status_code == 408:\n        return TimeoutError(error_message, status_code, original_error=e)\n    elif status_code == 400:\n        return InvalidRequestError(error_message, status_code, original_error=e)\n    else:\n        return APIError(error_message, status_code, original_error=e)\n</code></pre>"},{"location":"usage/error_handling/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Specific Exception Handling: Catch specific exceptions when you need to handle them differently.</p> </li> <li> <p>Logging: Log errors for debugging and monitoring purposes.</p> </li> </ol> <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    response = client.generate_text(\"Tell me a joke\", model=\"gpt-3.5-turbo\")\nexcept ClientAIError as e:\n    logger.error(f\"An error occurred: {e}\", exc_info=True)\n</code></pre> <ol> <li>Retry Logic: Implement retry logic for transient errors like rate limiting.</li> </ol> <pre><code>import time\nfrom clientai.exceptions import RateLimitError\n\ndef retry_generate(prompt, model, max_retries=3, delay=1):\n    for attempt in range(max_retries):\n        try:\n            return client.generate_text(prompt, model=model)\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n            wait_time = e.retry_after if hasattr(e, 'retry_after') else delay * (2 ** attempt)\n            logger.warning(f\"Rate limit reached. Waiting for {wait_time} seconds...\")\n            time.sleep(wait_time)\n</code></pre> <ol> <li>Graceful Degradation: Implement fallback options when errors occur.</li> </ol> <pre><code>def generate_with_fallback(prompt, primary_client, fallback_client):\n    try:\n        return primary_client.generate_text(prompt, model=\"gpt-3.5-turbo\")\n    except ClientAIError as e:\n        logger.warning(f\"Primary client failed: {e}. Falling back to secondary client.\")\n        return fallback_client.generate_text(prompt, model=\"llama-2-70b-chat\")\n</code></pre> <p>By following these practices and utilizing ClientAI's unified error handling system, you can create more robust and maintainable applications that gracefully handle errors across different AI providers.</p>"},{"location":"usage/initialization/","title":"Initializing ClientAI","text":"<p>This guide covers the process of initializing ClientAI with different AI providers. You'll learn how to set up ClientAI for use with OpenAI, Replicate, and Ollama.</p>"},{"location":"usage/initialization/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites</li> <li>OpenAI Initialization</li> <li>Replicate Initialization</li> <li>Ollama Initialization</li> <li>Multiple Provider Initialization</li> <li>Best Practices</li> </ol>"},{"location":"usage/initialization/#prerequisites","title":"Prerequisites","text":"<p>Before initializing ClientAI, ensure you have:</p> <ol> <li>Installed ClientAI: <code>pip install clientai[all]</code></li> <li>Obtained necessary API keys for the providers you plan to use</li> <li>Basic understanding of Python and asynchronous programming</li> </ol>"},{"location":"usage/initialization/#openai-initialization","title":"OpenAI Initialization","text":"<p>To initialize ClientAI with OpenAI:</p> <pre><code>from clientai import ClientAI\n\nopenai_client = ClientAI('openai', api_key=\"your-openai-api-key\")\n</code></pre> <p>Replace <code>\"your-openai-api-key\"</code> with your actual OpenAI API key.</p>"},{"location":"usage/initialization/#replicate-initialization","title":"Replicate Initialization","text":"<p>To initialize ClientAI with Replicate:</p> <pre><code>from clientai import ClientAI\n\nreplicate_client = ClientAI('replicate', api_key=\"your-replicate-api-key\")\n</code></pre> <p>Replace <code>\"your-replicate-api-key\"</code> with your actual Replicate API key.</p>"},{"location":"usage/initialization/#ollama-initialization","title":"Ollama Initialization","text":"<p>To initialize ClientAI with Ollama:</p> <pre><code>from clientai import ClientAI\n\nollama_client = ClientAI('ollama', host=\"http://localhost:11434\")\n</code></pre> <p>Ensure that you have Ollama running locally on the specified host.</p>"},{"location":"usage/initialization/#multiple-provider-initialization","title":"Multiple Provider Initialization","text":"<p>You can initialize multiple providers in the same script:</p> <pre><code>from clientai import ClientAI\n\nopenai_client = ClientAI('openai', api_key=\"your-openai-api-key\")\nreplicate_client = ClientAI('replicate', api_key=\"your-replicate-api-key\")\nollama_client = ClientAI('ollama', host=\"http://localhost:11434\")\n</code></pre>"},{"location":"usage/initialization/#best-practices","title":"Best Practices","text":"<ol> <li>Environment Variables: Store API keys in environment variables instead of hardcoding them in your script:</li> </ol> <pre><code>import os\nfrom clientai import ClientAI\n\nopenai_client = ClientAI('openai', api_key=os.getenv('OPENAI_API_KEY'))\n</code></pre> <ol> <li>Error Handling: Wrap initialization in a try-except block to handle potential errors:</li> </ol> <pre><code>try:\n    client = ClientAI('openai', api_key=\"your-openai-api-key\")\nexcept ValueError as e:\n    print(f\"Error initializing ClientAI: {e}\")\n</code></pre> <ol> <li>Configuration Files: For projects with multiple providers, consider using a configuration file:</li> </ol> <pre><code>import json\nfrom clientai import ClientAI\n\nwith open('config.json') as f:\n    config = json.load(f)\n\nopenai_client = ClientAI('openai', **config['openai'])\nreplicate_client = ClientAI('replicate', **config['replicate'])\n</code></pre> <ol> <li>Lazy Initialization: If you're not sure which provider you'll use, initialize clients only when needed:</li> </ol> <pre><code>def get_client(provider):\n    if provider == 'openai':\n        return ClientAI('openai', api_key=\"your-openai-api-key\")\n    elif provider == 'replicate':\n        return ClientAI('replicate', api_key=\"your-replicate-api-key\")\n    # ... other providers ...\n\n# Use the client when needed\nclient = get_client('openai')\n</code></pre> <p>By following these initialization guidelines, you'll be well-prepared to start using ClientAI with various AI providers in your projects.</p>"},{"location":"usage/multiple_providers/","title":"Working with Multiple Providers in ClientAI","text":"<p>This guide explores techniques for effectively using multiple AI providers within a single project using ClientAI. You'll learn how to switch between providers and leverage their unique strengths.</p>"},{"location":"usage/multiple_providers/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Setting Up Multiple Providers</li> <li>Switching Between Providers</li> <li>Leveraging Provider Strengths</li> <li>Load Balancing and Fallback Strategies</li> <li>Best Practices</li> </ol>"},{"location":"usage/multiple_providers/#setting-up-multiple-providers","title":"Setting Up Multiple Providers","text":"<p>First, initialize ClientAI with multiple providers:</p> <pre><code>from clientai import ClientAI\n\nopenai_client = ClientAI('openai', api_key=\"your-openai-api-key\")\nreplicate_client = ClientAI('replicate', api_key=\"your-replicate-api-key\")\nollama_client = ClientAI('ollama', host=\"http://localhost:11434\")\n</code></pre>"},{"location":"usage/multiple_providers/#switching-between-providers","title":"Switching Between Providers","text":"<p>Create a function to switch between providers based on your requirements:</p> <pre><code>def get_provider(task):\n    if task == \"translation\":\n        return openai_client\n    elif task == \"code_generation\":\n        return replicate_client\n    elif task == \"local_inference\":\n        return ollama_client\n    else:\n        return openai_client  # Default to OpenAI\n\n# Usage\ntask = \"translation\"\nprovider = get_provider(task)\nresponse = provider.generate_text(\"Translate 'Hello' to French\", model=\"gpt-3.5-turbo\")\n</code></pre> <p>This approach allows you to dynamically select the most appropriate provider for each task.</p>"},{"location":"usage/multiple_providers/#leveraging-provider-strengths","title":"Leveraging Provider Strengths","text":"<p>Different providers excel in different areas. Here's how you can leverage their strengths:</p> <pre><code>def translate_text(text, target_language):\n    return openai_client.generate_text(\n        f\"Translate '{text}' to {target_language}\",\n        model=\"gpt-3.5-turbo\"\n    )\n\ndef generate_code(prompt):\n    return replicate_client.generate_text(\n        prompt,\n        model=\"meta/llama-2-70b-chat:latest\"\n    )\n\ndef local_inference(prompt):\n    return ollama_client.generate_text(\n        prompt,\n        model=\"llama2\"\n    )\n\n# Usage\nfrench_text = translate_text(\"Hello, world!\", \"French\")\npython_code = generate_code(\"Write a Python function to calculate the Fibonacci sequence\")\nquick_response = local_inference(\"What's the capital of France?\")\n</code></pre>"},{"location":"usage/multiple_providers/#load-balancing-and-fallback-strategies","title":"Load Balancing and Fallback Strategies","text":"<p>Implement load balancing and fallback strategies to ensure reliability:</p> <pre><code>import random\n\nproviders = [openai_client, replicate_client, ollama_client]\n\ndef load_balanced_generate(prompt, max_retries=3):\n    for _ in range(max_retries):\n        try:\n            provider = random.choice(providers)\n            return provider.generate_text(prompt, model=provider.default_model)\n        except Exception as e:\n            print(f\"Error with provider {provider.__class__.__name__}: {e}\")\n    raise Exception(\"All providers failed after max retries\")\n\n# Usage\ntry:\n    response = load_balanced_generate(\"Explain the concept of machine learning\")\n    print(response)\nexcept Exception as e:\n    print(f\"Failed to generate text: {e}\")\n</code></pre> <p>This function randomly selects a provider and falls back to others if there's an error.</p>"},{"location":"usage/multiple_providers/#best-practices","title":"Best Practices","text":"<ol> <li>Provider Selection Logic: Develop clear criteria for selecting providers based on task requirements, cost, and performance.</li> </ol> <pre><code>def select_provider(task, complexity, budget):\n    if complexity == \"high\" and budget == \"high\":\n        return openai_client  # Assuming OpenAI has more advanced models\n    elif task == \"code\" and budget == \"medium\":\n        return replicate_client\n    else:\n        return ollama_client  # Assuming Ollama is the most cost-effective\n</code></pre> <ol> <li>Consistent Interface: Create wrapper functions to provide a consistent interface across providers:</li> </ol> <pre><code>def unified_generate(prompt, provider=None):\n    if provider is None:\n        provider = get_default_provider()\n    return provider.generate_text(prompt, model=provider.default_model)\n\n# Usage\nresponse = unified_generate(\"Explain quantum computing\")\n</code></pre> <ol> <li>Error Handling and Logging: Implement comprehensive error handling and logging when working with multiple providers:</li> </ol> <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef safe_generate(prompt, provider):\n    try:\n        return provider.generate_text(prompt, model=provider.default_model)\n    except Exception as e:\n        logger.error(f\"Error with {provider.__class__.__name__}: {e}\")\n        return None\n</code></pre> <ol> <li>Performance Monitoring: Track the performance of different providers to optimize selection:</li> </ol> <pre><code>import time\n\ndef timed_generate(prompt, provider):\n    start_time = time.time()\n    result = provider.generate_text(prompt, model=provider.default_model)\n    elapsed_time = time.time() - start_time\n    logger.info(f\"{provider.__class__.__name__} took {elapsed_time:.2f} seconds\")\n    return result\n</code></pre> <ol> <li>Configuration Management: Use configuration files or environment variables to manage provider settings:</li> </ol> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nopenai_client = ClientAI('openai', api_key=os.getenv('OPENAI_API_KEY'))\nreplicate_client = ClientAI('replicate', api_key=os.getenv('REPLICATE_API_KEY'))\nollama_client = ClientAI('ollama', host=os.getenv('OLLAMA_HOST'))\n</code></pre> <ol> <li>Caching: Implement caching to reduce redundant API calls and improve response times:</li> </ol> <pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=100)\ndef cached_generate(prompt, provider_name):\n    provider = get_provider(provider_name)\n    return provider.generate_text(prompt, model=provider.default_model)\n\n# Usage\nresponse = cached_generate(\"What is the speed of light?\", \"openai\")\n</code></pre> <p>By following these practices and leveraging the strengths of multiple providers, you can create more robust, efficient, and versatile applications with ClientAI.</p>"},{"location":"usage/overview/","title":"Usage Overview","text":"<p>This Usage section provides comprehensive guides on how to effectively use the key features of ClientAI. Each topic focuses on a specific aspect of usage, ensuring you have all the information needed to leverage the full potential of ClientAI in your projects.</p>"},{"location":"usage/overview/#key-topics","title":"Key Topics","text":""},{"location":"usage/overview/#1-initializing-clientai","title":"1. Initializing ClientAI","text":"<p>This guide covers the process of initializing ClientAI with different AI providers. It provides a step-by-step approach to setting up ClientAI for use with OpenAI, Replicate, and Ollama.</p> <ul> <li>Initializing ClientAI Guide</li> </ul>"},{"location":"usage/overview/#2-text-generation-with-clientai","title":"2. Text Generation with ClientAI","text":"<p>Learn how to use ClientAI for text generation tasks. This guide explores the various options and parameters available for generating text across different AI providers.</p> <ul> <li>Text Generation Guide</li> </ul>"},{"location":"usage/overview/#3-chat-functionality-in-clientai","title":"3. Chat Functionality in ClientAI","text":"<p>Discover how to leverage ClientAI's chat functionality. This guide covers creating chat conversations, managing context, and handling chat-specific features across supported providers.</p> <ul> <li>Chat Functionality Guide</li> </ul>"},{"location":"usage/overview/#4-working-with-multiple-providers","title":"4. Working with Multiple Providers","text":"<p>Explore techniques for effectively using multiple AI providers within a single project. This guide demonstrates how to switch between providers and leverage their unique strengths.</p> <ul> <li>Multiple Providers Guide</li> </ul>"},{"location":"usage/overview/#5-handling-responses-and-errors","title":"5. Handling Responses and Errors","text":"<p>Learn best practices for handling responses from AI providers and managing potential errors. This guide covers response parsing, error handling, and retry strategies.</p> <ul> <li>Soon</li> </ul>"},{"location":"usage/overview/#getting-started","title":"Getting Started","text":"<p>To make the most of these guides, we recommend familiarizing yourself with basic Python programming and asynchronous programming concepts, as ClientAI leverages these extensively.</p>"},{"location":"usage/overview/#quick-start-example","title":"Quick Start Example","text":"<p>Here's a simple example to get you started with ClientAI:</p> <pre><code>from clientai import ClientAI\n\n# Initialize the client\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\n# Generate text\nresponse = client.generate_text(\n    \"Explain the concept of machine learning in simple terms.\",\n    model=\"gpt-3.5-turbo\"\n)\n\nprint(response)\n</code></pre> <p>For more detailed examples and explanations, refer to the specific guides linked above.</p>"},{"location":"usage/overview/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/overview/#streaming-responses","title":"Streaming Responses","text":"<p>ClientAI supports streaming responses for compatible providers. Here's a basic example:</p> <pre><code>for chunk in client.generate_text(\n    \"Tell me a long story about space exploration\",\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"usage/overview/#using-different-models","title":"Using Different Models","text":"<p>ClientAI allows you to specify different models for each provider. For example:</p> <pre><code># Using GPT-4 with OpenAI\nopenai_response = openai_client.generate_text(\n    \"Explain quantum computing\",\n    model=\"gpt-4\"\n)\n\n# Using Llama 2 with Replicate\nreplicate_response = replicate_client.generate_text(\n    \"Describe the process of photosynthesis\",\n    model=\"meta/llama-2-70b-chat:latest\"\n)\n</code></pre>"},{"location":"usage/overview/#best-practices","title":"Best Practices","text":"<ol> <li>API Key Management: Always store your API keys securely, preferably as environment variables.</li> <li>Error Handling: Implement proper error handling to manage potential API failures or rate limiting issues.</li> <li>Model Selection: Choose appropriate models based on your task requirements and budget considerations.</li> <li>Context Management: For chat applications, manage conversation context efficiently to get the best results.</li> </ol>"},{"location":"usage/overview/#contribution","title":"Contribution","text":"<p>If you have suggestions or contributions to these guides, please refer to our Contributing Guidelines. We appreciate your input in improving our documentation and making ClientAI more accessible to all users.</p>"},{"location":"usage/text_generation/","title":"Text Generation with ClientAI","text":"<p>This guide explores how to use ClientAI for text generation tasks across different AI providers. You'll learn about the various options and parameters available for generating text.</p>"},{"location":"usage/text_generation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Text Generation</li> <li>Advanced Parameters</li> <li>Streaming Responses</li> <li>Provider-Specific Features</li> <li>Best Practices</li> </ol>"},{"location":"usage/text_generation/#basic-text-generation","title":"Basic Text Generation","text":"<p>To generate text using ClientAI, use the <code>generate_text</code> method:</p> <pre><code>from clientai import ClientAI\n\nclient = ClientAI('openai', api_key=\"your-openai-api-key\")\n\nresponse = client.generate_text(\n    \"Write a short story about a robot learning to paint.\",\n    model=\"gpt-3.5-turbo\"\n)\n\nprint(response)\n</code></pre> <p>This will generate a short story based on the given prompt.</p>"},{"location":"usage/text_generation/#advanced-parameters","title":"Advanced Parameters","text":"<p>ClientAI supports various parameters to fine-tune text generation:</p> <pre><code>response = client.generate_text(\n    \"Explain the theory of relativity\",\n    model=\"gpt-4\",\n    max_tokens=150,\n    temperature=0.7,\n    top_p=0.9,\n    presence_penalty=0.1,\n    frequency_penalty=0.1\n)\n</code></pre> <ul> <li><code>max_tokens</code>: Maximum number of tokens to generate</li> <li><code>temperature</code>: Controls randomness (0.0 to 1.0)</li> <li><code>top_p</code>: Nucleus sampling parameter</li> <li><code>presence_penalty</code>: Penalizes new tokens based on their presence in the text so far</li> <li><code>frequency_penalty</code>: Penalizes new tokens based on their frequency in the text so far</li> </ul> <p>Note: Available parameters may vary depending on the provider.</p>"},{"location":"usage/text_generation/#streaming-responses","title":"Streaming Responses","text":"<p>For long-form content, you can use streaming to get partial responses as they're generated:</p> <pre><code>for chunk in client.generate_text(\n    \"Write a comprehensive essay on climate change\",\n    model=\"gpt-3.5-turbo\",\n    stream=True\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre> <p>This allows for real-time display of generated text, which can be useful for user interfaces or long-running generations.</p>"},{"location":"usage/text_generation/#provider-specific-features","title":"Provider-Specific Features","text":"<p>Different providers may offer unique features. Here are some examples:</p>"},{"location":"usage/text_generation/#openai","title":"OpenAI","text":"<pre><code>response = openai_client.generate_text(\n    \"Translate the following to French: 'Hello, how are you?'\",\n    model=\"gpt-3.5-turbo\"\n)\n</code></pre>"},{"location":"usage/text_generation/#replicate","title":"Replicate","text":"<pre><code>response = replicate_client.generate_text(\n    \"Generate a haiku about mountains\",\n    model=\"meta/llama-2-70b-chat:latest\"\n)\n</code></pre>"},{"location":"usage/text_generation/#ollama","title":"Ollama","text":"<pre><code>response = ollama_client.generate_text(\n    \"Explain the concept of neural networks\",\n    model=\"llama2\"\n)\n</code></pre>"},{"location":"usage/text_generation/#best-practices","title":"Best Practices","text":"<ol> <li>Prompt Engineering: Craft clear and specific prompts for better results.</li> </ol> <pre><code>good_prompt = \"Write a detailed description of a futuristic city, focusing on transportation and architecture.\"\n</code></pre> <ol> <li> <p>Model Selection: Choose appropriate models based on your task complexity and requirements.</p> </li> <li> <p>Error Handling: Always handle potential errors in text generation:</p> </li> </ol> <pre><code>try:\n    response = client.generate_text(\"Your prompt here\", model=\"gpt-3.5-turbo\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n</code></pre> <ol> <li> <p>Rate Limiting: Be mindful of rate limits imposed by providers. Implement appropriate delays or queuing mechanisms for high-volume applications.</p> </li> <li> <p>Content Filtering: Implement content filtering or moderation for user-facing applications to ensure appropriate outputs.</p> </li> <li> <p>Consistency: For applications requiring consistent outputs, consider using lower temperature values or implementing your own post-processing.</p> </li> </ol> <p>By following these guidelines and exploring the various parameters and features available, you can effectively leverage ClientAI for a wide range of text generation tasks across different AI providers.</p>"}]}